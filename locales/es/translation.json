{
  "Description": "■ Acerca de la aplicación",
  "BasedSettings": "■ Configuración básica",
  "AISettings": "■ Configuración de IA",
  "CharacterSettings": "■ Configuración de personaje",
  "YoutubeSettings": "■ Configuración de YouTube",
  "VoiceSettings": "■ Configuración de voz",
  "SlideSettings": "■ Configuración de diapositivas",
  "LogSettings": "■ Historial de conversaciones",
  "OtherSettings": "■ Otros",
  "ExternalLinkageMode": "Modo de enlace externo (WebSocket)",
  "YoutubeMode": "Modo YouTube",
  "YoutubeInfo": "El primer carácter del comentario es '#' y será ignorado.",
  "YoutubeAPIKey": "Clave API de YouTube",
  "YoutubeLiveID": "ID en directo de YouTube",
  "ConversationContinuityMode": "Modo de continuidad de conversación (Beta)",
  "ConversationContinuityModeInfo": "Cuando no hay comentarios, la IA intenta continuar la conversación. Actualmente solo compatible con OpenAI, Anthropic Claude y Google Gemini.",
  "ConversationContinuityModeInfo2": "Una respuesta llama a LLM varias veces, por lo que el uso de la API puede aumentar. Por favor, tenga esto en cuenta.",
  "ConversationContinuityModeInfo3": "gpt-4, gpt-4-turbo, claude-3-opus, claude-3.5-sonnet funcionan relativamente estables.",
  "MaxPastMessages": "Número de mensajes anteriores a mantener",
  "StatusOn": "Estado: ACTIVADO",
  "StatusOff": "Estado: DESACTIVADO",
  "Select": "Seleccionar",
  "TestVoice": "Probar voz",
  "SelectAIService": "Seleccionar servicio de IA",
  "LocalLLM": "LLM local",
  "SelectModel": "Seleccionar modelo",
  "OpenAIAPIKeyLabel": "Clave API de OpenAI",
  "AnthropicAPIKeyLabel": "Clave API de Anthropic",
  "GoogleAPIKeyLabel": "Clave API de Google Gemini",
  "AzureAPIKeyLabel": "Clave API de Azure OpenAI",
  "AzureAPIURL": "URL API de Azure OpenAI",
  "GroqAPIKeyLabel": "Clave API de Groq",
  "CohereAPIKeyLabel": "Clave API de Cohere",
  "MistralAIAPIKeyLabel": "Clave API de MistralAI",
  "PerplexityAPIKeyLabel": "Clave API de Perplexity",
  "FireworksAPIKeyLabel": "Clave API de Fireworks",
  "DifyAPIKeyLabel": "Clave API de Dify",
  "DeepSeekAPIKeyLabel": "Clave API de DeepSeek",
  "APIKeyInstruction": "Puede obtener la clave API abajo. Ingrese la clave API obtenida en el formulario.",
  "LocalLLMInfo": "El servidor LLM local debe estar en ejecución. La configuración es la siguiente.",
  "LocalLLMInfo2": "Por favor, ingrese la URL del servidor LLM local (incluyendo el número de puerto) y el nombre del modelo.",
  "GroqInfo": "La API de Groq se accede directamente desde el navegador.",
  "DifyInfo": "Dify solo admite tipos de chatbot y agente.",
  "DifyInfo2": "La longitud del historial de conversación depende de las especificaciones de Dify.",
  "DifyInfo3": "Ejemplo: https://api.dify.ai/v1, http://localhost:80/v1",
  "DifyInstruction": "Si usa Dify, no se utilizará el prompt del sistema. Por favor, configure el chatbot de Dify.",
  "EnterURL": "URL",
  "CharacterModelLabel": "Modelo de personaje",
  "CharacterModelInfo": "El modelo puede tardar en cargar cuando se muestra por primera vez.",
  "OpenVRM": "Abrir VRM",
  "BackgroundImage": "Imagen de fondo",
  "ChangeBackgroundImage": "Cambiar imagen de fondo",
  "CharacterSettingsPrompt": "Prompt de personaje",
  "CharacterSettingsInfo": "Este valor se establece como el prompt del sistema.\nPor favor, consulte el prompt inicial y especifique las etiquetas de emoción para controlar las expresiones y movimientos del personaje. Ejemplo: [neutral]¡Buenos días![happy]¡Hoy también es un día difícil!",
  "CharacterSettingsReset": "Restablecer configuración del personaje",
  "SyntheticVoiceEngineChoice": "Elegir motor de síntesis de voz",
  "VoiceAdjustment": "Ajuste de voz",
  "VoiceEngineInstruction": "Seleccione el motor de síntesis de voz que desea usar.",
  "UsingKoeiromap": "Koeiromap",
  "KoeiromapInfo": "Usando API Koeiromap de Koemotion. Solo admite japonés. Para más detalles, consulte el enlace de abajo.",
  "UsingVoiceVox": "VOICEVOX",
  "VoiceVoxInfo": "Usando VOICEVOX. Solo admite japonés. Usa una API local, necesita descargar y ejecutar la aplicación que se adapte a su entorno desde el sitio de abajo.",
  "VoicevoxSpeed": "Velocidad",
  "VoicevoxPitch": "Tono",
  "VoicevoxIntonation": "Entonación",
  "UsingAivisSpeech": "AivisSpeech",
  "AivisSpeechInfo": "Usando AivisSpeech. Solo admite japonés. Usa una API local, necesita descargar y ejecutar la aplicación que se adapte a su entorno desde el sitio de abajo.",
  "AivisSpeechSpeaker": "Hablante",
  "AivisSpeechSpeed": "Velocidad",
  "AivisSpeechPitch": "Tono",
  "AivisSpeechIntonation": "Entonación",
  "AivisSpeechServerUrl": "URL del servidor AivisSpeech",
  "UsingNijiVoice": "NijiVoice",
  "NijiVoiceInfo": "Se usa la API de NijiVoice. Solo admite japonés. La clave API se puede obtener desde la URL de abajo.",
  "NijiVoiceApiKey": "Clave API de NijiVoice",
  "NijiVoiceActorId": "ID del actor",
  "NijiVoiceSpeed": "Velocidad del habla",
  "NijiVoiceEmotionalLevel": "Nivel emocional",
  "NijiVoiceSoundDuration": "Duración del sonido",
  "VoicevoxServerUrl": "URL del servidor VOICEVOX",
  "UpdateSpeakerList": "Actualizar lista de hablantes",
  "UsingGoogleTTS": "Google TTS",
  "UsingStyleBertVITS2": "Style-Bert-VITS2",
  "StyleBertVITS2Info": "Usando Style-Bert-VITS2. Solo admite japonés, inglés y chino. Si usa una API local, necesita descargar y ejecutar la aplicación que se adapte a su entorno desde el sitio de abajo. También configure una clave API si es necesario.",
  "SpeakerSelection": "Selección de hablante",
  "IncludeTimestampInUserMessage": "Incluir marca de tiempo en el mensaje del usuario",
  "IncludeTimestampInUserMessageInfo": "Al incluir marcas de tiempo en los mensajes del usuario, la IA puede generar respuestas considerando el tiempo.\nPor favor, incluya el siguiente texto en su prompt del sistema:\n\n\"La entrada del usuario puede incluir [timestamp]. Esto representa la hora UTC en el momento de la solicitud, así que por favor genere respuestas considerando esta marca de tiempo.\"",
  "GoogleTTSInfo": "Usando Google Cloud Text-to-Speech. Admite múltiples idiomas.",
  "AuthFileInstruction": "Se requiere una clave API o archivo de autenticación. Obténgalo desde la URL de abajo y colóquelo en la carpeta raíz del repositorio si es un archivo JSON.",
  "LanguageModelURL": "Seleccione el modelo de idioma desde la URL de abajo.",
  "LanguageChoice": "Elección de idioma",
  "StyleBeatVITS2ServerURL": "URL del servidor",
  "StyleBeatVITS2ApiKey": "Clave API",
  "StyleBeatVITS2ModelID": "ID del modelo",
  "StyleBeatVITS2Style": "Estilo",
  "StyleBeatVITS2SdpRatio": "Ratio de mezcla SDP/DP",
  "StyleBeatVITS2Length": "Velocidad del habla",
  "ConversationHistory": "Historial de conversación",
  "ConversationHistoryInfo": "Los últimos 10 textos de conversación se almacenan como memorias.",
  "ConversationHistoryReset": "Restablecer historial de conversación",
  "NotConnectedToExternalAssistant": "No conectado a un asistente externo.",
  "APIKeyNotEntered": "No se ha ingresado la clave API.",
  "ChatLog": "Registro de conversación",
  "EnterYourQuestion": "Ingrese su pregunta aquí",
  "AnswerGenerating": "Generando respuesta",
  "AboutThisApplication": "Acerca de esta aplicación",
  "AboutThisApplicationDescription": "Disfrute de conversaciones con un personaje 3D directamente en su navegador web, usando micrófono o entrada de texto y síntesis de voz. También puede cambiar el personaje (VRM), ajustar su personalidad y modificar su voz.<br />La configuración se puede cambiar desde el botón de menú en la parte superior izquierda.",
  "AboutThisApplicationDescription2": "Si desea cambiar el personaje, consulte la pestaña \"Configuración de personaje\".",
  "TechnologyIntroduction": "Introducción a la tecnología",
  "TechnologyIntroductionDescription1": "Esta aplicación fue creada modificando el <b>ChatVRM</b> de pixiv. El código fuente original se puede encontrar",
  "TechnologyIntroductionLink1": "aquí",
  "TechnologyIntroductionDescription2": ".",
  "TechnologyIntroductionDescription3": "Para mostrar y manipular modelos 3D,",
  "TechnologyIntroductionDescription4": "se utiliza. Para generar texto de conversación, se utilizan varios LLM como",
  "TechnologyIntroductionDescription5": "se utilizan. Para la síntesis de voz, se utilizan varios motores TTS como",
  "TechnologyIntroductionDescription6": "se utilizan. Para más detalles, consulte este",
  "TechnologyIntroductionLink2": "artículo explicativo",
  "TechnologyIntroductionDescription7": ".",
  "SourceCodeDescription1": "El código fuente de esta aplicación está disponible públicamente en GitHub. Siéntase libre de modificarlo y adaptarlo como desee.",
  "SourceCodeDescription2": "Para uso comercial, consulte el README del mismo repositorio.",
  "RepositoryURL": "URL del repositorio:",
  "DontShowIntroductionNextTime": "No mostrar este diálogo la próxima vez",
  "Close": "CERRAR",
  "Contact": "Contacto",
  "ContactDescription": "Por favor, contácteme a través del correo electrónico o la cuenta de Twitter de abajo sobre esta aplicación.",
  "Creator": "Creador",
  "CreatorDescription": "Creador: Tegan",
  "Language": "Idioma",
  "UsingGSVITTS": "GSVI TTS",
  "GSVITTSInfo": "Configuración de GSVI TTS",
  "GSVITTSServerUrl": "API endpoint de GSVI TTS",
  "GSVITTSModelID": "ID del modelo GSVI TTS",
  "GSVITTSBatchSize": "Tamaño de lote GSVI TTS (1 ~ 100 Cuanto mayor sea el valor, más rápida será la inferencia, pero puede agotar la memoria si es demasiado grande.)",
  "GSVITTSSpeechRate": "Velocidad del habla (0.5 ~ 2.0 Cuanto mayor sea el valor, más rápido será.)",
  "UsingElevenLabs": "ElevenLabs",
  "ElevenLabsInfo": "Se usa la API de ElevenLabs. Admite múltiples idiomas. La clave API se puede obtener desde la URL de abajo.",
  "ElevenLabsApiKey": "Clave API de ElevenLabs",
  "ElevenLabsVoiceId": "ID de voz de ElevenLabs",
  "ElevenLabsVoiceIdInfo": "El ID de voz se puede seleccionar desde la URL de abajo.",
  "CharacterName": "Nombre del personaje",
  "ShowAssistantText": "Mostrar cuadro de respuesta",
  "ShowCharacterName": "Mostrar nombre del personaje en el cuadro de respuesta",
  "ShowControlPanel": "Mostrar botón de configuración",
  "ShowControlPanelInfo": "La pantalla de configuración se puede mostrar presionando Cmd + . (Mac) / Ctrl + . (Windows).",
  "SlideMode": "Modo presentación",
  "SelectedSlideDocs": "Documentos de presentación seleccionados",
  "SlideModeDescription": "Este es un modo donde la IA presenta diapositivas automáticamente. Solo está disponible cuando el servicio de IA seleccionado es OpenAI, Anthropic Claude o Google Gemini.",
  "PdfConvertLabel": "Conversión de diapositivas PDF",
  "PdfConvertDescription": "Convertir PDF a datos de modo presentación. Solo disponible cuando el servicio de IA seleccionado es OpenAI, Anthropic Claude o Google Gemini.",
  "PdfConvertFileUpload": "Seleccionar archivo PDF",
  "PdfConvertFolderName": "Nombre de la carpeta de guardado",
  "PdfConvertModelSelect": "Seleccionar modelo",
  "PdfConvertButton": "Convertir PDF a diapositivas",
  "PdfConvertLoading": "Convirtiendo...",
  "PdfConvertSuccess": "Conversión completada",
  "PdfConvertError": "Error en la conversión",
  "PdfConvertSubmitError": "Por favor, asegúrese de que el archivo PDF, el nombre de la carpeta y la clave API estén configurados.",
  "LocalStorageReset": "Restablecer configuración",
  "LocalStorageResetInfo": "Las variables de entorno tienen prioridad si están configuradas. La página se recargará.",
  "LocalStorageResetButton": "Restablecer configuración",
  "Errors": {
    "EmptyAPIKey": "La clave API no está configurada",
    "AIInvalidProperty": "La configuración del servicio de IA es incorrecta",
    "AIAPIError": "Ocurrió un error al ejecutar la API de IA",
    "InvalidAIService": "El servicio de IA seleccionado no es válido",
    "MethodNotAllowed": "La solicitud no es apropiada",
    "TTSServiceError": "Ocurrió un error en el servicio TTS {{serviceName}}: {{message}}",
    "UnexpectedError": "Ocurrió un error inesperado",
    "LocalLLMError": "Error de LLM local",
    "LocalLLMStreamError": "Error de stream de LLM local",
    "LocalLLMConnectionError": "Error de conexión al servidor LLM local",
    "LocalLLMNotFound": "Endpoint de LLM local no encontrado",
    "LocalLLMAPIError": "Error de API de LLM local"
  },
  "MessageReceiver": "Recibir instrucciones desde el exterior",
  "MessageReceiverDescription": "Puede usar la API para hacer que los personajes de IA hablen desde el exterior.",
  "ClientID": "ID de cliente",
  "OpenSendMessagePage": "Abrir página de envío de mensaje",
  "RealtimeAPIMode": "Modo API en tiempo real",
  "RealtimeAPIModeContentType": "Tipo de envío",
  "RealtimeAPIModeVoice": "Tipo de voz",
  "AudioMode": "Modo audio (Beta)",
  "InputText": "Texto",
  "InputAudio": "Audio",
  "SearchGrounding": "Usar búsqueda contextual",
  "SearchGroundingDescription": "Al usar la función multimodal, la función de búsqueda se desactiva automáticamente.",
  "UpdateRealtimeAPISettings": "Actualizar configuración de API en tiempo real",
  "UpdateRealtimeAPISettingsInfo": "Al actualizar la clave API, endpoint de Azure, tipo de voz, modelo o prompt del sistema, presione el botón de actualización para iniciar una nueva sesión WebSocket.",
  "AzureEndpoint": "Endpoint de Azure",
  "Toasts": {
    "WebSocketConnectionError": "Error en la conexión WebSocket",
    "WebSocketConnectionClosed": "Conexión WebSocket cerrada",
    "WebSocketConnectionAttempt": "Intentando conexión WebSocket...",
    "WebSocketConnectionSuccess": "Conexión WebSocket exitosa",
    "FunctionExecuting": "Ejecutando {{funcName}}",
    "FunctionExecutionFailed": "La ejecución de {{funcName}} falló",
    "FirefoxNotSupported": "Esta función no está soportada en Firefox",
    "SpeechRecognitionError": "Ocurrió un error de reconocimiento de voz"
  },
  "UsingOpenAITTS": "Usando OpenAI",
  "OpenAITTSInfo": "Usando OpenAI. Admite múltiples idiomas. Si selecciona OpenAI como servicio de IA, no necesita configurar la clave API abajo.",
  "OpenAITTSVoice": "Tipo de voz",
  "OpenAITTSModel": "Modelo",
  "OpenAITTSSpeed": "Velocidad",
  "UsingAzureTTS": "Usando Azure OpenAI",
  "AzureTTSInfo": "Usando Azure OpenAI. Admite múltiples idiomas.",
  "SendMessage": {
    "title": "Adaptador externo AITuberKit",
    "directSendTitle": "Hablar directamente al personaje de IA",
    "directSendDescription": "Puede enviar el mensaje directamente al personaje de IA. Si se envían múltiples mensajes, se procesan en orden. El modelo de voz es el seleccionado en la configuración de AITuberKit.",
    "aiGenerateTitle": "Generar respuesta de IA y luego hablar",
    "aiGenerateDescription": "La IA genera una respuesta del mensaje enviado y luego la pronuncia. Si se envían múltiples mensajes, se procesan en orden. El modelo de IA y el modelo de voz son los seleccionados en la configuración de AITuberKit. El prompt del sistema puede seleccionarse para usar el prompt del sistema de AITuberKit o un prompt del sistema personalizado. Si desea cargar el historial de conversación anterior, incluya la cadena [conversation_history] en el prompt del sistema o mensaje del usuario.",
    "useCurrentSystemPrompt": "Usar prompt del sistema de AITuberKit",
    "userInputTitle": "Enviar entrada de usuario",
    "userInputDescription": "El mensaje enviado se procesa igual que cuando se ingresa desde el formulario de entrada de AITuberKit. Si se envían múltiples mensajes, se procesan en orden. El modelo de IA y el modelo de voz son los seleccionados en la configuración de AITuberKit. El prompt del sistema y el historial de conversación son los valores configurados en AITuberKit."
  },
  "CannotUseVoice": "El modo API en tiempo real o el modo audio está activado, por lo que no se requiere configuración de voz.",
  "Live2D": {
    "FileInfo": "Coloque el modelo Live2D que desea usar en la carpeta public/live2d. El archivo model3.json debe existir en la raíz de esta carpeta.\nSi no se muestra en la selección, por favor recargue la pantalla o verifique si la ruta de la carpeta es correcta.",
    "Info": "Puede especificar emociones y movimientos.\nCada emoción es controlada por el prompt. Para más detalles, consulte \"Configuración de IA => Configuración de personaje\".",
    "Emotions": "Configuración de emociones",
    "EmotionInfo": "Las emociones se pueden especificar en formato separado por comas. Si se especifican múltiples emociones, se seleccionan aleatoriamente.\nEl valor inicial es para el modelo proporcionado por AITuberKit. Si está usando un modelo original, ingrese el valor según su modelo.\nDespués de completar la conversación, se muestra la emoción \"Neutral\".",
    "neutralEmotions": "Neutral",
    "happyEmotions": "Feliz",
    "sadEmotions": "Triste",
    "angryEmotions": "Enojado",
    "relaxedEmotions": "Relajado",
    "MotionGroups": "Configuración de grupos de movimiento",
    "MotionGroupsInfo": "Los grupos de movimiento se seleccionan aleatoriamente del grupo seleccionado.\nIgual que la configuración de emociones, configúrelo según su modelo.\n\"Idle\" es el movimiento mostrado después de completar la conversación.",
    "SelectMotionGroup": "Seleccionar grupo de movimiento",
    "idleMotionGroup": "Inactivo",
    "neutralMotionGroup": "Neutral",
    "happyMotionGroup": "Feliz",
    "sadMotionGroup": "Triste",
    "angryMotionGroup": "Enojado",
    "relaxedMotionGroup": "Relajado"
  },
  "UseVideoAsBackground": "Usar pantalla compartida o webcam como fondo",
  "Temperature": "Temperatura"
}

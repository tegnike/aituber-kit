{
  "Description": "À propos de l'application",
  "BasedSettings": "Paramètres de base",
  "AISettings": "Paramètres d'IA",
  "CharacterSettings": "Paramètres du personnage",
  "YoutubeSettings": "Paramètres YouTube",
  "VoiceSettings": "Paramètres de synthèse vocale",
  "SpeechInputSettings": "Paramètres d'entrée vocale",
  "SlideSettings": "Paramètres des diapositives",
  "LogSettings": "Historique des conversations",
  "OtherSettings": "Autres",
  "ExternalLinkageMode": "Mode de liaison externe (bêta)",
  "YoutubeMode": "Mode YouTube",
  "YoutubeInfo": "Les commentaires commençant par '#' seront ignorés.",
  "YoutubeAPIKey": "Clé API YouTube",
  "YoutubeLiveID": "ID YouTube Live",
  "ConversationContinuityMode": "Mode de continuité de conversation (bêta)",
  "ConversationContinuityModeInfo": "C'est un mode dans lequel l'IA tente de poursuivre la conversation d'elle-même lorsqu'il n'y a pas de commentaires. Il n'est activé que si un modèle compatible multimodal est sélectionné.",
  "ConversationContinuityModeInfo2": "Comme l'IA est appelée plusieurs fois par réponse, les frais d'API peuvent augmenter. Veuillez en tenir compte.",
  "ConversationContinuityModeInfo3": "Selon le modèle sélectionné, il se peut que le fonctionnement ne soit pas stable.",
  "MaxPastMessages": "Nombre de messages passés à conserver",
  "StatusOn": "État: ON",
  "StatusOff": "État: OFF",
  "Select": "Veuillez sélectionner",
  "TestVoice": "Tester la voix",
  "SelectAIService": "Sélectionner le service d'IA",
  "LocalLLM": "LLM local",
  "SelectModel": "Sélectionner le modèle",
  "OpenAIAPIKeyLabel": "Clé API OpenAI",
  "AnthropicAPIKeyLabel": "Clé API Anthropic",
  "GoogleAPIKeyLabel": "Clé API Google Gemini",
  "AzureAPIKeyLabel": "Clé API Azure OpenAI",
  "AzureAPIURL": "URL API Azure OpenAI",
  "GroqAPIKeyLabel": "Clé API Groq",
  "CohereAPIKeyLabel": "Clé API Cohere",
  "MistralAIAPIKeyLabel": "Clé API MistralAI",
  "PerplexityAPIKeyLabel": "Clé API Perplexity",
  "FireworksAPIKeyLabel": "Clé API Fireworks",
  "DifyAPIKeyLabel": "Clé API Dify",
  "DeepSeekAPIKeyLabel": "Clé API DeepSeek",
  "APIKeyInstruction": "Vous pouvez obtenir une clé API via le lien ci-dessous. Veuillez entrer la clé API obtenue dans le formulaire.",
  "LocalLLMInfo": "Vous devez avoir un serveur LLM local en cours d'exécution.",
  "LocalLLMInfo2": "Veuillez entrer l'URL du LLM local (avec le numéro de port) et le nom du modèle.",
  "GroqInfo": "L'API Groq est accessible directement depuis le navigateur.",
  "DifyInfo": "Dify ne prend en charge que les types chatbot ou agent.  \nSi vous n'obtenez pas de réponse satisfaisante, veuillez supprimer l'historique de conversation et poser à nouveau votre question.",
  "DifyInfo2": "La longueur de l'historique des conversations dépend des paramètres du chatbot Dify.",
  "DifyInfo3": "Exemple: https://api.dify.ai/v1, http://localhost:80/v1",
  "DifyInstruction": "Lorsque vous utilisez Dify, ce prompt système n'est pas utilisé. Veuillez le configurer dans votre chatbot Dify.",
  "EnterURL": "Entrer l'URL",
  "CharacterModelLabel": "Modèle de personnage",
  "CharacterModelInfo": "Le chargement initial peut prendre du temps selon le modèle.",
  "OpenVRM": "Ouvrir VRM",
  "BackgroundImage": "Image d'arrière-plan",
  "ChangeBackgroundImage": "Changer l'image d'arrière-plan",
  "BackgroundSettings": "Paramètres d'arrière-plan",
  "BackgroundSettingsDescription": "Vous pouvez télécharger et sélectionner une image d'arrière-plan pour l'application.",
  "UploadBackground": "Télécharger une image d'arrière-plan",
  "DefaultBackground": "Arrière-plan par défaut",
  "CharacterSettingsPrompt": "Prompt du personnage",
  "CharacterSettingsInfo": "Cette valeur sera définie comme prompt système.\nEn vous référant au prompt initial, vous pouvez contrôler les expressions et les mouvements du personnage en spécifiant des balises d'émotion. Exemple: [neutral]Bonjour![happy]Merci pour votre travail aujourd'hui!",
  "CharacterpresetInfo": "La sélection d'un préréglage changera le prompt du personnage.\nVous pouvez utiliser les raccourcis Cmd + Maj + 1~5 (Mac) / Ctrl + Maj + 1~5 (Windows).",
  "Characterpreset1": "Préréglage 1",
  "Characterpreset2": "Préréglage 2",
  "Characterpreset3": "Préréglage 3",
  "Characterpreset4": "Préréglage 4",
  "Characterpreset5": "Préréglage 5",
  "SyntheticVoiceEngineChoice": "Sélection du moteur de synthèse vocale",
  "VoiceAdjustment": "Ajustement de la voix",
  "VoiceEngineInstruction": "Veuillez sélectionner le moteur de synthèse vocale à utiliser.",
  "UsingKoeiromap": "Utiliser Koeiromap",
  "KoeiromapInfo": "Utilise l'API Koeiromap de Koemotion. Prend en charge uniquement le japonais. Pour plus de détails, consultez ci-dessous.",
  "UsingVoiceVox": "Utiliser VOICEVOX",
  "VoiceVoxInfo": "Utilise VOICEVOX. Prend en charge uniquement le japonais. Comme il utilise une API locale, vous devez télécharger et exécuter l'application correspondant à votre environnement depuis le site ci-dessous.",
  "VoicevoxSpeed": "Vitesse de parole",
  "VoicevoxPitch": "Hauteur de voix",
  "VoicevoxIntonation": "Intonation",
  "VoicevoxServerUrl": "URL du serveur VOICEVOX",
  "UsingAivisSpeech": "Utiliser AivisSpeech",
  "AivisSpeechInfo": "Utilise AivisSpeech. Prend en charge uniquement le japonais. Comme il utilise une API locale, vous devez télécharger et exécuter l'application correspondant à votre environnement depuis le site ci-dessous.",
  "AivisSpeechSpeaker": "Locuteur",
  "AivisSpeechSpeed": "Vitesse de parole",
  "AivisSpeechPitch": "Hauteur de voix",
  "AivisSpeechIntonation": "Intonation",
  "AivisSpeechServerUrl": "URL du serveur AivisSpeech",
  "UsingNijiVoice": "Utiliser NijiVoice",
  "NijiVoiceInfo": "Utilise l'API NijiVoice. Prend en charge uniquement le japonais. Veuillez obtenir une clé API à partir de l'URL ci-dessous.",
  "NijiVoiceApiKey": "Clé API NijiVoice",
  "NijiVoiceActorId": "ID du locuteur",
  "NijiVoiceSpeed": "Vitesse de parole",
  "NijiVoiceEmotionalLevel": "Niveau émotionnel",
  "NijiVoiceSoundDuration": "Durée du son",
  "UpdateSpeakerList": "Mettre à jour la liste des locuteurs",
  "UsingGoogleTTS": "Utiliser Google Text-to-Speech",
  "UsingStyleBertVITS2": "Utiliser Style-Bert-VITS2",
  "StyleBertVITS2Info": "Utilise Style-Bert-VITS2. Prend en charge uniquement le japonais, l'anglais et le chinois. Si vous utilisez l'API locale, vous devez télécharger et exécuter l'application correspondant à votre environnement depuis le site ci-dessous. Configurez également une clé API si nécessaire.",
  "SpeakerSelection": "Sélection du type de voix",
  "EnglishToJapanese": "Lire les mots anglais en japonais",
  "IncludeTimestampInUserMessage": "Inclure un horodatage dans les messages de l'utilisateur",
  "IncludeTimestampInUserMessageInfo": "L'inclusion d'un horodatage dans les messages de l'utilisateur permet à l'IA de générer des réponses en tenant compte du temps.\nVeuillez inclure la phrase suivante dans votre prompt système:\n\n\"Lorsque les entrées de l'utilisateur sont accompagnées d'un [timestamp], celui-ci représente l'heure UTC au moment de la requête, veuillez donc tenir compte de cette heure lors de la génération de votre réponse.\"",
  "GoogleTTSInfo": "Utilise Google Cloud Text-to-Speech. Prend en charge plusieurs langues.",
  "AuthFileInstruction": "Une clé API ou un fichier JSON d'authentification est nécessaire. Obtenez-le à partir du lien ci-dessous et, s'il s'agit d'un fichier JSON, placez-le à la racine du dépôt sous le nom credentials.json.",
  "LanguageModelURL": "Veuillez sélectionner le modèle de langue à partir de l'URL ci-dessous.",
  "LanguageChoice": "Sélection de la langue",
  "StyleBeatVITS2ServerURL": "URL du serveur",
  "StyleBeatVITS2ApiKey": "Clé API",
  "StyleBeatVITS2ModelID": "ID du modèle",
  "StyleBeatVITS2Style": "Style",
  "StyleBeatVITS2SdpRatio": "Ratio mixte SDP/DP",
  "StyleBeatVITS2Length": "Vitesse de parole",
  "ConversationHistory": "Historique des conversations",
  "ConversationHistoryInfo": "Les {{count}} dernières conversations sont conservées en mémoire.",
  "ConversationHistoryReset": "Réinitialiser l'historique des conversations",
  "NotConnectedToExternalAssistant": "Non connecté à un assistant externe.",
  "APIKeyNotEntered": "Aucune clé API n'a été saisie.",
  "ChatLog": "Journal de conversation",
  "EnterYourQuestion": "Entrez votre question ici",
  "AnswerGenerating": "Génération de la réponse",
  "AboutThisApplication": "À propos de cette application",
  "AboutThisApplicationDescription": "Interagissez avec un personnage 3D dans votre navigateur web en utilisant un microphone, la saisie de texte et la synthèse vocale. Vous pouvez modifier le personnage (VRM), définir sa personnalité et ajuster la voix.<br />Les paramètres peuvent être modifiés en cliquant sur le bouton de menu en haut à gauche.",
  "AboutThisApplicationDescription2": "Avec AITuberKit, vous pouvez profiter de conversations avec un personnage IA directement dans votre navigateur web. Consultez les sections de paramètres pour modifier le personnage, sa personnalité et les réglages vocaux.",
  "TechnologyIntroduction": "Introduction technologique",
  "TechnologyIntroductionDescription1": "Cette application est basée sur <b>ChatVRM</b> de pixiv, modifié. Le code source original est disponible",
  "TechnologyIntroductionLink1": "ici",
  "TechnologyIntroductionDescription2": ".",
  "TechnologyIntroductionDescription3": "Elle utilise",
  "TechnologyIntroductionDescription4": "pour l'affichage et la manipulation des modèles 3D,",
  "TechnologyIntroductionDescription5": "et divers LLM pour la génération de texte, ainsi que",
  "TechnologyIntroductionDescription6": "et divers TTS pour la synthèse vocale. Pour plus de détails, consultez cet",
  "TechnologyIntroductionLink2": "article explicatif",
  "TechnologyIntroductionDescription7": ".",
  "SourceCodeDescription1": "Le code source de cette application est disponible sur GitHub. Vous êtes libre de le modifier.",
  "SourceCodeDescription2": "Pour l'utilisation commerciale, veuillez consulter le README dans le même dépôt.",
  "RepositoryURL": "URL du dépôt:",
  "DontShowIntroductionNextTime": "Ne plus afficher cette boîte de dialogue",
  "Close": "Fermer",
  "Contact": "Contact",
  "ContactDescription": "Pour toute question concernant cette application, veuillez contacter l'adresse e-mail ou le compte Twitter ci-dessous.",
  "Creator": "Informations sur le créateur",
  "CreatorDescription": "Créateur: Nike",
  "Documentation": "Documentation",
  "DocumentationDescription": "Consultez l'URL ci-dessous pour des tutoriels et des instructions détaillées sur l'utilisation d'AITuberKit.",
  "Language": "Paramètres de langue",
  "UsingGSVITTS": "Utiliser GSVI TTS",
  "GSVITTSInfo": "Paramètres GSVI TTS",
  "GSVITTSServerUrl": "URL du serveur GSVI TTS",
  "GSVITTSModelID": "ID du modèle GSVI TTS",
  "GSVITTSBatchSize": "Taille du lot GSVI TTS (1 ~ 100, plus la valeur est élevée, plus l'inférence est rapide, mais une valeur trop élevée peut épuiser la mémoire)",
  "GSVITTSSpeechRate": "Vitesse de parole (0.5 ~ 2.0, plus la valeur est élevée, plus c'est rapide)",
  "UsingElevenLabs": "Utiliser ElevenLabs",
  "ElevenLabsInfo": "Utilise l'API ElevenLabs. Prend en charge plusieurs langues. Veuillez obtenir une clé API à partir de l'URL ci-dessous.",
  "ElevenLabsApiKey": "Clé API ElevenLabs",
  "ElevenLabsVoiceId": "ID de voix ElevenLabs",
  "ElevenLabsVoiceIdInfo": "Veuillez sélectionner l'ID de voix à partir de l'URL ci-dessous.",
  "CharacterName": "Nom du personnage",
  "ShowAssistantText": "Afficher la zone de réponse",
  "ShowCharacterName": "Afficher le nom du personnage dans la zone de réponse",
  "ShowControlPanel": "Afficher le panneau de contrôle",
  "ShowControlPanelInfo": "L'écran de configuration peut être affiché en appuyant sur Cmd + . (Mac) / Ctrl + . (Windows).\nSi vous utilisez un smartphone, vous pouvez également appuyer longuement (environ 1 seconde) sur le coin supérieur gauche de l'écran.",
  "ShowCharacterPresetMenu": "Afficher le bouton du menu des préréglages de personnages",
  "SlideMode": "Mode diapositives",
  "SelectedSlideDocs": "Diapositives à utiliser",
  "SlideModeDescription": "C'est un mode où l'IA présente automatiquement les diapositives. Il n'est activé que si un modèle multimodal est sélectionné.",
  "PdfConvertLabel": "Conversion de diapositives PDF",
  "PdfConvertDescription": "Convertit le PDF en données pour le mode diaporama. Disponible uniquement si un modèle multimodal est sélectionné.",
  "PdfConvertFileUpload": "Sélectionner un fichier PDF",
  "PdfConvertFolderName": "Nom du dossier de sauvegarde",
  "CustomVoiceTextPlaceholder": "Entrez le texte que vous souhaitez écouter",
  "TestVoiceSettings": "Test vocal",
  "TestSelectedVoice": "Lire",
  "PdfConvertModelSelect": "Sélectionner un modèle",
  "PdfConvertButton": "Convertir le PDF en diapositives",
  "PdfConvertLoading": "Conversion en cours...",
  "PdfConvertSuccess": "Conversion terminée",
  "PdfConvertError": "Échec de la conversion",
  "PdfConvertSubmitError": "Veuillez vérifier que le fichier PDF, le nom du dossier et la clé API sont configurés",
  "LocalStorageReset": "Réinitialiser les paramètres",
  "LocalStorageResetInfo": "Si des variables d'environnement sont définies, leurs valeurs prévaudront. La page sera rechargée.",
  "LocalStorageResetButton": "Réinitialiser les paramètres",
  "InitialSpeechTimeout": "Délai d'attente de reconnaissance vocale",
  "InitialSpeechTimeoutInfo": "Définit le temps d'attente après le début de la reconnaissance vocale jusqu'à ce que la première parole soit détectée. Si aucune parole n'est détectée dans ce délai, la reconnaissance vocale s'arrêtera automatiquement.\nSi défini à 0 secondes, le temps d'attente est illimité.",
  "Milliseconds": "millisecondes",
  "NoSpeechTimeout": "Délai de détection de silence",
  "NoSpeechTimeoutInfo": "Définit le temps après lequel l'entrée se termine automatiquement lorsqu'un silence persiste pendant l'entrée vocale.\nSi défini à 0 secondes, l'envoi automatique par détection de silence est désactivé.",
  "ShowSilenceProgressBar": "Afficher la barre de progression de détection de silence",
  "SpeechRecognitionMode": "Mode de reconnaissance vocale",
  "SpeechRecognitionModeInfo": "Vous pouvez sélectionner le mode de reconnaissance vocale.\n'Standard du navigateur' utilise la reconnaissance vocale intégrée au navigateur. 'OpenAI TTS' utilise l'API Text to Speech d'OpenAI.\nEn général, 'Standard du navigateur' est recommandé car il est plus précis et plus rapide. Cependant, si vous utilisez un navigateur comme Firefox qui ne prend pas en charge l'API WebSpeech, sélectionnez 'OpenAI TTS'.",
  "BrowserSpeechRecognition": "Utiliser la reconnaissance vocale standard du navigateur",
  "WhisperSpeechRecognition": "Utiliser la reconnaissance vocale OpenAI TTS",
  "WhisperTranscriptionModel": "Modèle de transcription",
  "WhisperTranscriptionModelInfo": "Vous pouvez sélectionner le modèle à utiliser pour la reconnaissance vocale. Les modèles plus performants offrent une meilleure précision, mais peuvent entraîner des coûts API plus élevés.",
  "SpeechRecognitionModeDisabledInfo": "Lorsque le mode audio est activé, seule la reconnaissance vocale du navigateur est disponible.\nDe plus, en mode API en temps réel, seule la reconnaissance vocale du navigateur est disponible et la fonction de délai d'attente de reconnaissance vocale est désactivée.",
  "Errors": {
    "EmptyAPIKey": "La clé API n'est pas définie",
    "EmptyLocalLLMURL": "L'URL du LLM local n'est pas définie",
    "AIInvalidProperty": "La valeur du paramètre du service d'IA est incorrecte",
    "AIAPIError": "Une erreur s'est produite lors de l'exécution de l'API IA",
    "InvalidAIService": "Le service d'IA sélectionné est incorrect",
    "MethodNotAllowed": "La requête n'est pas appropriée",
    "TTSServiceError": "Une erreur s'est produite dans le service TTS {{serviceName}}: {{message}}",
    "UnexpectedError": "Une erreur inconnue s'est produite",
    "LocalLLMError": "Une erreur s'est produite dans le LLM local",
    "LocalLLMStreamError": "Une erreur s'est produite dans le traitement du flux du LLM local",
    "LocalLLMConnectionError": "Impossible de se connecter au serveur LLM local",
    "LocalLLMNotFound": "Le point de terminaison du LLM local est introuvable",
    "LocalLLMAPIError": "Une erreur s'est produite dans l'API du LLM local",
    "CustomAPIError": "Une erreur s'est produite dans l'API personnalisée",
    "InvalidJSON": "Le format JSON est incorrect"
  },
  "MessageReceiver": "Accepter les instructions externes",
  "MessageReceiverDescription": "Vous pouvez diriger les déclarations du personnage IA depuis l'extérieur en utilisant l'API.",
  "ClientID": "ID Client",
  "OpenSendMessagePage": "Ouvrir la page d'envoi de messages",
  "RealtimeAPIMode": "Mode API en temps réel",
  "RealtimeAPIModeContentType": "Type d'envoi",
  "RealtimeAPIModeVoice": "Type de voix",
  "AudioMode": "Mode audio",
  "InputText": "Texte",
  "InputAudio": "Audio",
  "SearchGrounding": "Utiliser la fonction de recherche",
  "SearchGroundingDescription": "Lors de l'utilisation de la fonction multimodale, la fonction de recherche est automatiquement désactivée.",
  "UpdateRealtimeAPISettings": "Mettre à jour les paramètres de l'API en temps réel",
  "UpdateRealtimeAPISettingsInfo": "Lorsque vous mettez à jour la clé API, le point de terminaison Azure, le type de voix, le modèle ou le prompt système, appuyez sur le bouton de mise à jour pour démarrer une nouvelle session WebSocket.",
  "AzureEndpoint": "Point de terminaison Azure",
  "Toasts": {
    "WebSocketConnectionError": "Une erreur s'est produite lors de la connexion WebSocket",
    "WebSocketConnectionClosed": "La connexion WebSocket a été fermée",
    "WebSocketConnectionAttempt": "Tentative de connexion WebSocket...",
    "WebSocketConnectionSuccess": "Connexion WebSocket réussie",
    "FunctionExecuting": "Exécution de {{funcName}}",
    "FunctionExecutionFailed": "Échec de l'exécution de {{funcName}}",
    "FirefoxNotSupported": "Cette fonctionnalité n'est pas prise en charge dans Firefox",
    "SpeechRecognitionError": "Une erreur de reconnaissance vocale s'est produite",
    "NoSpeechDetected": "Aucun son détecté.",
    "PresetSwitching": "Passé à {{presetName}}.",
    "WhisperError": "Une erreur s'est produite lors de la reconnaissance vocale avec Whisper",
    "UsingTool": "Utilisation de {{toolName}}",
    "PositionFixed": "La position du personnage a été fixée",
    "PositionUnfixed": "La fixation de la position du personnage a été supprimée",
    "PositionReset": "La position du personnage a été réinitialisée",
    "PositionActionFailed": "Échec de l'opération de positionnement",
    "MicrophonePermissionDenied": "L'accès au microphone a été refusé"
  },
  "ContinuousMic": "Entrée micro permanente",
  "ContinuousMicActive": "Entrée micro permanente active",
  "ContinuousMicModeOn": "Le mode d'entrée micro permanente est activé",
  "ContinuousMicModeOff": "Le mode d'entrée micro permanente est désactivé",
  "ListeningContinuously": "En attente d'entrée vocale...",
  "ContinuousMicInfo": "L'entrée micro redémarre automatiquement lorsque l'IA finit de parler. L'entrée est envoyée automatiquement après le délai de silence configuré.\nSi aucune reconnaissance vocale n'est effectuée dans le délai configuré, l'entrée micro permanente se désactive automatiquement. Si vous souhaitez qu'elle reste toujours active, réglez le délai d'attente de reconnaissance vocale à 0 seconde.",
  "UsingOpenAITTS": "Utiliser OpenAI",
  "OpenAITTSInfo": "Utilise OpenAI. Prend en charge plusieurs langues. Si vous avez sélectionné OpenAI comme service d'IA, il n'est pas nécessaire de configurer la clé API ci-dessous.",
  "OpenAITTSVoice": "Type de voix",
  "OpenAITTSModel": "Modèle",
  "OpenAITTSSpeed": "Vitesse de parole",
  "UsingAzureTTS": "Utiliser Azure OpenAI",
  "AzureTTSInfo": "Utilise Azure OpenAI. Prend en charge plusieurs langues.",
  "SendMessage": {
    "title": "Adaptateur externe AITuberKit",
    "directSendTitle": "Faire parler directement le personnage IA",
    "directSendDescription": "Permet de faire prononcer directement le message envoyé par le personnage IA. Si plusieurs messages sont envoyés, ils seront traités dans l'ordre.\nLe modèle vocal utilisé est celui sélectionné dans les paramètres d'AITuberKit.",
    "aiGenerateTitle": "Générer une réponse avec l'IA puis la faire prononcer",
    "aiGenerateDescription": "L'IA génère une réponse à partir du message envoyé, puis le personnage IA prononce cette réponse. Si plusieurs messages sont envoyés, ils seront traités dans l'ordre.\nLes modèles d'IA et de voix utilisés sont ceux sélectionnés dans les paramètres d'AITuberKit.\nVous pouvez choisir d'utiliser le prompt système d'AITuberKit ou un prompt système personnalisé.\nPour inclure l'historique des conversations précédentes, incluez la chaîne [conversation_history] n'importe où dans le prompt système ou le message utilisateur.",
    "useCurrentSystemPrompt": "Utiliser le prompt système d'AITuberKit",
    "userInputTitle": "Envoyer une entrée utilisateur",
    "userInputDescription": "Le message envoyé sera traité comme s'il avait été saisi dans le formulaire d'entrée d'AITuberKit. Si plusieurs messages sont envoyés, ils seront traités dans l'ordre.\nLes modèles d'IA et de voix utilisés sont ceux sélectionnés dans les paramètres d'AITuberKit.\nLe prompt système et l'historique des conversations d'AITuberKit seront utilisés."
  },
  "CannotUseVoice": "Lorsque le mode API en temps réel ou le mode audio est activé,\nles paramètres de synthèse vocale ne sont pas nécessaires.",
  "Live2D": {
    "FileInfo": "Placez le dossier du modèle Live2D que vous souhaitez utiliser dans public/live2d. Ce dossier doit contenir un fichier model3.json directement à sa racine.\nSi l'option n'apparaît pas dans la liste, rechargez la page ou vérifiez que le chemin du dossier est correct.",
    "Info": "Vous pouvez spécifier les émotions et les mouvements.\nChaque émotion est contrôlée par le prompt. Pour plus de détails, consultez 'Paramètres d'IA => Paramètres du personnage'.",
    "Emotions": "Paramètres d'expressions",
    "EmotionInfo": "Les émotions peuvent être spécifiées avec plusieurs valeurs séparées par des virgules. Si plusieurs sont spécifiées, une sera choisie aléatoirement.\nLes valeurs par défaut correspondent aux modèles fournis avec AITuberKit. Si vous utilisez votre propre modèle, entrez des valeurs adaptées à celui-ci.\nAprès la fin de la conversation, l'expression 'normale' sera affichée.",
    "neutralEmotions": "Normal",
    "happyEmotions": "Heureux",
    "sadEmotions": "Triste",
    "angryEmotions": "En colère",
    "relaxedEmotions": "Détendu",
    "surprisedEmotions": "Surpris",
    "MotionGroups": "Paramètres des groupes de mouvements",
    "MotionGroupsInfo": "Les mouvements sont sélectionnés aléatoirement dans le groupe choisi.\nComme pour les paramètres d'expressions, configurez-les en fonction de votre modèle.\n'Au repos' correspond aux mouvements affichés après la fin d'une conversation.",
    "SelectMotionGroup": "Sélectionner un groupe de mouvements",
    "idleMotionGroup": "Au repos",
    "neutralMotionGroup": "Normal",
    "happyMotionGroup": "Heureux",
    "sadMotionGroup": "Triste",
    "angryMotionGroup": "En colère",
    "relaxedMotionGroup": "Détendu",
    "surprisedMotionGroup": "Surpris"
  },
  "UseVideoAsBackground": "Utiliser un écran partagé ou une webcam comme arrière-plan",
  "Temperature": "Température",
  "MaxTokens": "Nombre maximum de tokens",
  "MaxTokensInfo": "Le nombre maximum de tokens varie selon le modèle d'IA utilisé. Veuillez consulter les spécifications de chaque modèle.",
  "CannotUseParameters": "Lorsque le mode API en temps réel ou le mode audio est activé, les paramètres Temperature et Max Tokens ne peuvent pas être spécifiés.",
  "PresetQuestions": "Questions prédéfinies",
  "PresetQuestionsInfo": "Vous pouvez créer et enregistrer plusieurs modèles de questions à l'avance. Les questions enregistrées apparaîtront sous forme de boutons dans l'interface utilisateur et seront insérées dans la zone de saisie du chat lorsque vous cliquerez dessus.",
  "EnterPresetQuestion": "Veuillez saisir une question",
  "DragToReorder": "Glisser pour réorganiser",
  "CustomAPIEndpoint": "Point de terminaison API personnalisé",
  "CustomAPIEndpointInfo": "Veuillez saisir l'URL du point de terminaison API auquel envoyer des requêtes POST.",
  "CustomAPIStream": "Mode streaming",
  "CustomAPIStreamForced": "Actuellement, le mode streaming est toujours activé.",
  "IncludeSystemMessages": "Inclure les messages du système",
  "CustomAPIHeaders": "En-têtes personnalisés",
  "CustomAPIHeadersInfo": "Veuillez saisir les informations d'en-tête à inclure dans la requête API au format JSON.",
  "CustomAPIBody": "Corps personnalisé",
  "CustomAPIBodyInfo": "Veuillez saisir les informations du corps à inclure dans la requête API au format JSON. Les messages sont inclus automatiquement.",
  "CustomAPIDescription": "Remarque : Les messages sont automatiquement inclus dans le corps de la requête. En mode streaming, le serveur doit renvoyer text/event-stream.",
  "EditSlideScripts": "Édition des répliques",
  "PleaseSelectSlide": "Veuillez sélectionner une diapositive",
  "XAIAPIKeyLabel": "Clé API xAI",
  "DynamicRetrievalDescription": "Définit le seuil de moment où le modèle exécute la recherche. 0 signifie que la recherche est toujours exécutée, 1 signifie qu'elle ne l'est jamais.",
  "DynamicRetrieval": "Recherche dynamique",
  "OpenRouterModelNameInstruction": "Veuillez saisir l'identifiant du modèle depuis OpenRouter (exemple : \"openai/gpt-4o\", \"mistralai/mistral-large-latest\"). Vous pouvez vérifier l'identifiant du modèle sur la page des modèles OpenRouter.",
  "FixPosition": "Fixer la position",
  "ResetPosition": "Réinitialiser la position",
  "CharacterPosition": "Position du personnage",
  "DynamicRetrievalThreshold": "Seuil dynamique",
  "CurrentStatus": "État actuel",
  "PositionNotFixed": "Non fixé",
  "PositionFixed": "Fixé",
  "OpenRouterAPIKeyLabel": "Clé API OpenRouter",
  "CharacterPositionInfo": "Vous pouvez fixer la position et l'orientation du personnage. La position de la caméra est enregistrée pour VRM, et la position du modèle pour Live2D.",
  "UnfixPosition": "Désactiver la fixation"
}

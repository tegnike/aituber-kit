{
  "Description": "Über die App",
  "BasedSettings": "Grundeinstellungen",
  "AISettings": "KI-Einstellungen",
  "CharacterSettings": "Charaktereinstellungen",
  "YoutubeSettings": "YouTube-Einstellungen",
  "VoiceSettings": "Einstellungen für synthetische Sprache",
  "SlideSettings": "Slide-Einstellungen",
  "LogSettings": "Gesprächsverlauf",
  "OtherSettings": "Sonstiges",
  "ExternalLinkageMode": "Externer Verknüpfungsmodus (Beta-Version)",
  "YoutubeMode": "YouTube-Modus",
  "YoutubeInfo": "Das erste Zeichen des Kommentars ist '#' und wird ignoriert.",
  "YoutubeAPIKey": "YouTube API-Schlüssel",
  "YoutubeLiveID": "YouTube Live-ID",
  "ConversationContinuityMode": "Gesprächskontinuitätsmodus (Beta)",
  "ConversationContinuityModeInfo": "Wenn keine Kommentare vorhanden sind, versucht die KI das Gespräch fortzusetzen. Derzeit nur von OpenAI, Anthropic Claude, Google Gemini unterstützt.",
  "ConversationContinuityModeInfo2": "Eine Antwort erfordert mehrere LLM-Aufrufe, daher kann die API-Nutzung steigen. Bitte beachten Sie dies.",
  "ConversationContinuityModeInfo3": "gpt-4, gpt-4-turbo, claude-3-opus, claude-3.5-sonnet funktionieren relativ stabil.",
  "MaxPastMessages": "Anzahl der zu behaltenden vergangenen Nachrichten",
  "StatusOn": "Status: EIN",
  "StatusOff": "Status: AUS",
  "Select": "Auswählen",
  "TestVoice": "Sprachtest",
  "SelectAIService": "KI-Dienst auswählen",
  "LocalLLM": "Lokales LLM",
  "SelectModel": "Modell auswählen",
  "OpenAIAPIKeyLabel": "OpenAI API-Schlüssel",
  "AnthropicAPIKeyLabel": "Anthropic API-Schlüssel",
  "GoogleAPIKeyLabel": "Google Gemini API-Schlüssel",
  "AzureAPIKeyLabel": "Azure OpenAI API-Schlüssel",
  "AzureAPIURL": "Azure OpenAI API-URL",
  "GroqAPIKeyLabel": "Groq API-Schlüssel",
  "CohereAPIKeyLabel": "Cohere API-Schlüssel",
  "MistralAIAPIKeyLabel": "MistralAI API-Schlüssel",
  "PerplexityAPIKeyLabel": "Perplexity API-Schlüssel",
  "FireworksAPIKeyLabel": "Fireworks API-Schlüssel",
  "DifyAPIKeyLabel": "Dify API-Schlüssel",
  "DeepSeekAPIKeyLabel": "DeepSeek API-Schlüssel",
  "APIKeyInstruction": "Sie können den API-Schlüssel unten erhalten. Geben Sie den erhaltenen API-Schlüssel in das Formular ein.",
  "LocalLLMInfo": "Der lokale LLM-Server muss ausgeführt werden. Die Konfiguration ist wie folgt.",
  "LocalLLMInfo2": "Geben Sie die lokale LLM-Server-URL (einschließlich Portnummer) und den Modellnamen ein.",
  "GroqInfo": "Die Groq-API wird direkt vom Browser aus zugegriffen.",
  "DifyInfo": "Dify unterstützt nur Chatbot- und Agent-Typen.",
  "DifyInfo2": "Die Länge des Gesprächsverlaufs hängt von den Dify-Spezifikationen ab.",
  "DifyInfo3": "Beispiel: https://api.dify.ai/v1, http://localhost:80/v1",
  "DifyInstruction": "Wenn Sie Dify verwenden, wird der Systemprompt nicht verwendet. Bitte konfigurieren Sie den Dify-Chatbot.",
  "EnterURL": "URL",
  "CharacterModelLabel": "Charaktermodell",
  "CharacterModelInfo": "Das Modell kann beim ersten Anzeigen längere Ladezeiten haben.",
  "OpenVRM": "VRM öffnen",
  "BackgroundImage": "Hintergrundbild",
  "ChangeBackgroundImage": "Hintergrundbild ändern",
  "CharacterSettingsPrompt": "Charakter-Prompt",
  "CharacterSettingsInfo": "Dieser Wert wird als Systemprompt festgelegt.\nBeziehen Sie sich auf den ursprünglichen Prompt und geben Sie Emotions-Tags an, um die Ausdrücke und Bewegungen des Charakters zu steuern. Beispiel: [neutral]Guten Morgen![happy]Auch heute wird ein geschäftiger Tag!",
  "characterpresetInfo": "Durch Auswahl einer Voreinstellung wird die Zeichenaufforderung geändert.\nCmd + Shift + 1~5 (Mac) / Ctrl + Shift + 1~5 (Windows) für Tastaturkürzel.\nWenn Sie eine Voreinstellung auswählen, während Sie die Umschalttaste gedrückt halten, wird die aktuelle Zeichenaufforderung in der Voreinstellung gespeichert.",
  "Characterpreset1": "Preset 1",
  "Characterpreset2": "Preset 2",
  "Characterpreset3": "Preset 3",
  "Characterpreset4": "Preset 4",
  "Characterpreset5": "Preset 5",
  "SyntheticVoiceEngineChoice": "Sprachsynthese-Engine auswählen",
  "VoiceAdjustment": "Sprachanpassung",
  "VoiceEngineInstruction": "Wählen Sie die Sprachsynthese-Engine aus, die Sie verwenden möchten.",
  "UsingKoeiromap": "Koeiromap",
  "KoeiromapInfo": "Verwendet die Koeiromap-API von Koemotion. Unterstützt nur Japanisch. Siehe Link unten für Details.",
  "UsingVoiceVox": "VOICEVOX",
  "VoiceVoxInfo": "Verwendet VOICEVOX. Unterstützt nur Japanisch. Verwendet lokale API, Sie müssen die für Ihr System geeignete Anwendung herunterladen und ausführen.",
  "VoicevoxSpeed": "Geschwindigkeit",
  "VoicevoxPitch": "Tonhöhe",
  "VoicevoxIntonation": "Intonation",
  "UsingAivisSpeech": "AivisSpeech",
  "AivisSpeechInfo": "Verwendet AivisSpeech. Unterstützt nur Japanisch. Verwendet lokale API, Sie müssen die für Ihr System geeignete Anwendung herunterladen und ausführen.",
  "AivisSpeechSpeaker": "Sprecher",
  "AivisSpeechSpeed": "Geschwindigkeit",
  "AivisSpeechPitch": "Tonhöhe",
  "AivisSpeechIntonation": "Intonation",
  "AivisSpeechServerUrl": "AivisSpeech Server-URL",
  "UsingNijiVoice": "NijiVoice",
  "NijiVoiceInfo": "Verwendet NijiVoice-API. Unterstützt nur Japanisch. API-Schlüssel kann von der URL unten erhalten werden.",
  "NijiVoiceApiKey": "NijiVoice API-Schlüssel",
  "NijiVoiceActorId": "Sprecher-ID",
  "NijiVoiceSpeed": "Sprechgeschwindigkeit",
  "NijiVoiceEmotionalLevel": "Emotionslevel",
  "NijiVoiceSoundDuration": "Tondauer",
  "VoicevoxServerUrl": "VOICEVOX Server-URL",
  "UpdateSpeakerList": "Sprecherliste aktualisieren",
  "UsingGoogleTTS": "Google Text-to-Speech verwenden",
  "UsingStyleBertVITS2": "Style-Bert-VITS2",
  "StyleBertVITS2Info": "Verwendet Style-Bert-VITS2. Unterstützt nur Japanisch, Englisch und Chinesisch. Bei lokaler API müssen Sie die passende Anwendung herunterladen und ausführen. Bei Bedarf richten Sie bitte einen API-Schlüssel ein.",
  "SpeakerSelection": "Sprecherauswahl",
  "IncludeTimestampInUserMessage": "Zeitstempel in Benutzernachrichten einschließen",
  "IncludeTimestampInUserMessageInfo": "Das Einschließen des Zeitstempels hilft der KI, Antworten unter Berücksichtigung der Sendezeit zu generieren.\nBitte fügen Sie die folgende Zeichenfolge in den Systemprompt ein:\n\n\"Die Benutzereingabe kann [timestamp] enthalten. Dies ist die UTC-Zeit zum Zeitpunkt der Anfrage, bitte generieren Sie Antworten mit dieser Information.\"",
  "GoogleTTSInfo": "Verwendet Google Cloud Text-to-Speech. Unterstützt mehrere Sprachen.",
  "AuthFileInstruction": "API-Schlüssel oder Authentifizierungsdatei erforderlich. Von der URL unten erhalten und bei JSON-Datei im Stammverzeichnis platzieren.",
  "LanguageModelURL": "Sprachmodell von der URL unten auswählen.",
  "LanguageChoice": "Sprachauswahl",
  "StyleBeatVITS2ServerURL": "Server-URL",
  "StyleBeatVITS2ApiKey": "API-Schlüssel",
  "StyleBeatVITS2ModelID": "Modell-ID",
  "StyleBeatVITS2Style": "Stil",
  "StyleBeatVITS2SdpRatio": "SDP/DP-Mischratio",
  "StyleBeatVITS2Length": "Sprechgeschwindigkeit",
  "ConversationHistory": "Gesprächsverlauf",
  "ConversationHistoryInfo": "Die letzten {{count}} Konversationen werden als Gedächtnis gespeichert.",
  "ConversationHistoryReset": "Gesprächsverlauf zurücksetzen",
  "NotConnectedToExternalAssistant": "Nicht mit externem Assistenten verbunden.",
  "APIKeyNotEntered": "API-Schlüssel nicht eingegeben.",
  "ChatLog": "Chat-Protokoll",
  "EnterYourQuestion": "Geben Sie hier Ihre Frage ein",
  "AnswerGenerating": "Antwort wird generiert",
  "AboutThisApplication": "Über diese Anwendung",
  "AboutThisApplicationDescription": "Erleben Sie Gespräche mit 3D-Charakteren direkt im Browser durch Mikrofon oder Text und Sprachsynthese. Sie können den Charakter (VRM) wechseln, die Persönlichkeit einstellen und die Stimme anpassen.\nEinstellungen können über die Menü-Schaltfläche oben links geändert werden.",
  "AboutThisApplicationDescription2": "Wenn Sie den Charakter ändern möchten, sehen Sie bitte den Tab \"Charaktereinstellungen\".",
  "TechnologyIntroduction": "Technologie-Einführung",
  "TechnologyIntroductionDescription1": "Diese Anwendung basiert auf dem <b>ChatVRM</b>-Projekt von pixiv. Der ursprüngliche Quellcode ist",
  "TechnologyIntroductionLink1": "hier",
  "TechnologyIntroductionDescription2": "verfügbar.",
  "TechnologyIntroductionDescription3": "Für die Anzeige und Manipulation von 3D-Modellen wird",
  "TechnologyIntroductionDescription4": "verwendet. Für die Gesprächsgenerierung werden verschiedene LLMs wie",
  "TechnologyIntroductionDescription5": "eingesetzt. Für die Sprachsynthese werden verschiedene TTS-Engines wie",
  "TechnologyIntroductionDescription6": "verwendet. Weitere Details finden Sie in diesem",
  "TechnologyIntroductionLink2": "Erklärungsartikel",
  "TechnologyIntroductionDescription7": ".",
  "SourceCodeDescription1": "Der Quellcode dieser Anwendung wird auf GitHub geteilt. Sie können ihn frei modifizieren.",
  "SourceCodeDescription2": "Für kommerzielle Nutzung beachten Sie bitte die README des Repositories.",
  "RepositoryURL": "Repository-URL:",
  "DontShowIntroductionNextTime": "Diesen Dialog beim nächsten Mal nicht anzeigen",
  "Close": "SCHLIEßEN",
  "Contact": "Kontakt",
  "ContactDescription": "Bei Fragen zu dieser Anwendung kontaktieren Sie uns bitte über die unten stehende E-Mail-Adresse oder Twitter-Account.",
  "Creator": "Ersteller",
  "CreatorDescription": "Ersteller: Tegan",
  "Language": "Sprache",
  "UsingGSVITTS": "GSVI TTS",
  "GSVITTSInfo": "GSVI TTS-Einstellungen",
  "GSVITTSServerUrl": "GSVI TTS API-Endpunkt",
  "GSVITTSModelID": "GSVI TTS Modell-ID",
  "GSVITTSBatchSize": "GSVI TTS Batch-Größe (1 ~ 100 Je höher der Wert, desto schneller die Inferenz, aber zu groß kann Speicher erschöpfen)",
  "GSVITTSSpeechRate": "Sprechgeschwindigkeit (0.5 ~ 2.0 Je höher der Wert, desto schneller)",
  "UsingElevenLabs": "ElevenLabs",
  "ElevenLabsInfo": "Verwendet ElevenLabs API. Unterstützt mehrere Sprachen. API-Schlüssel kann von der URL unten erhalten werden.",
  "ElevenLabsApiKey": "ElevenLabs API-Schlüssel",
  "ElevenLabsVoiceId": "ElevenLabs Stimmen-ID",
  "ElevenLabsVoiceIdInfo": "Stimmen-ID kann von der URL unten ausgewählt werden.",
  "CharacterName": "Charaktername",
  "ShowAssistantText": "Antwortbox anzeigen",
  "ShowCharacterName": "Charakternamen in Antwortbox anzeigen",
  "ShowControlPanel": "Einstellungsschaltfläche anzeigen",
  "ShowControlPanelInfo": "Das Einstellungsfenster kann mit Cmd + . (Mac) / Ctrl + . (Windows) angezeigt werden.\nWenn Sie ein Smartphone verwenden, können Sie auch die linke obere Ecke des Bildschirms etwa 1 Sekunde lang gedrückt halten.",
  "SlideMode": "Präsentationsmodus",
  "SelectedSlideDocs": "Ausgewählte Präsentationsdokumente",
  "SlideModeDescription": "Dies ist ein Modus, in dem die KI automatisch Folien präsentiert. Nur verfügbar, wenn der ausgewählte KI-Dienst OpenAI, Anthropic Claude oder Google Gemini ist.",
  "PdfConvertLabel": "PDF-Präsentationskonvertierung",
  "PdfConvertDescription": "Konvertiert PDF in Präsentationsmodus-Daten. Nur verfügbar, wenn der ausgewählte KI-Dienst OpenAI, Anthropic Claude oder Google Gemini ist.",
  "PdfConvertFileUpload": "PDF-Datei auswählen",
  "PdfConvertFolderName": "Speicherordnername",
  "PdfConvertModelSelect": "Modell auswählen",
  "PdfConvertButton": "PDF in Präsentation konvertieren",
  "PdfConvertLoading": "Konvertierung läuft...",
  "PdfConvertSuccess": "Konvertierung abgeschlossen",
  "PdfConvertError": "Konvertierung fehlgeschlagen",
  "PdfConvertSubmitError": "Bitte stellen Sie sicher, dass PDF-Datei, Ordnername und API-Schlüssel konfiguriert sind.",
  "LocalStorageReset": "Einstellungen zurücksetzen",
  "LocalStorageResetInfo": "Umgebungsvariablen haben Vorrang, wenn sie gesetzt sind. Die Seite wird neu geladen.",
  "LocalStorageResetButton": "Einstellungen zurücksetzen",
  "Errors": {
    "EmptyAPIKey": "API-Schlüssel ist nicht konfiguriert",
    "AIInvalidProperty": "KI-Dienstkonfiguration ist nicht korrekt",
    "AIAPIError": "Fehler bei der Ausführung der KI-API",
    "InvalidAIService": "Ausgewählter KI-Dienst ist ungültig",
    "MethodNotAllowed": "Anfrage ist nicht angemessen",
    "TTSServiceError": "Fehler im TTS-Dienst {{serviceName}}: {{message}}",
    "UnexpectedError": "Ein unerwarteter Fehler ist aufgetreten",
    "LocalLLMError": "Lokaler LLM-Fehler",
    "LocalLLMStreamError": "Lokaler LLM-Stream-Fehler",
    "LocalLLMConnectionError": "Verbindungsfehler zum lokalen LLM-Server",
    "LocalLLMNotFound": "Lokaler LLM-Endpunkt nicht gefunden",
    "LocalLLMAPIError": "Lokaler LLM-API-Fehler",
    "EmptyLocalLLMURL": "Die URL des lokalen LLM ist nicht festgelegt",
    "CustomAPIError": "Ein Fehler ist bei der benutzerdefinierten API aufgetreten",
    "InvalidJSON": "Das JSON-Format ist ungültig"
  },
  "MessageReceiver": "Anweisungen von außen empfangen",
  "MessageReceiverDescription": "Sie können die API verwenden, um KI-Charaktere von außen sprechen zu lassen.",
  "ClientID": "Client-ID",
  "OpenSendMessagePage": "Nachrichtenseite öffnen",
  "RealtimeAPIMode": "Echtzeit-API-Modus",
  "RealtimeAPIModeContentType": "Sendungstyp",
  "RealtimeAPIModeVoice": "Stimmtyp",
  "AudioMode": "Audiomodus",
  "InputText": "Text",
  "InputAudio": "Audio",
  "SearchGrounding": "Kontextuelle Suche verwenden",
  "SearchGroundingDescription": "Bei Verwendung der multimodalen Funktion wird die Suchfunktion automatisch deaktiviert.",
  "UpdateRealtimeAPISettings": "Echtzeit-API-Einstellungen aktualisieren",
  "UpdateRealtimeAPISettingsInfo": "Wenn Sie API-Schlüssel, Azure-Endpunkt, Stimmtyp, Modell oder Systemprompt aktualisieren, drücken Sie die Aktualisierungsschaltfläche, um eine neue WebSocket-Sitzung zu starten.",
  "AzureEndpoint": "Azure-Endpunkt",
  "Toasts": {
    "WebSocketConnectionError": "WebSocket-Verbindungsfehler",
    "WebSocketConnectionClosed": "WebSocket-Verbindung geschlossen",
    "WebSocketConnectionAttempt": "WebSocket-Verbindungsversuch...",
    "WebSocketConnectionSuccess": "WebSocket-Verbindung erfolgreich",
    "FunctionExecuting": "Führe {{funcName}} aus",
    "FunctionExecutionFailed": "Ausführung von {{funcName}} fehlgeschlagen",
    "FirefoxNotSupported": "Diese Funktion wird in Firefox nicht unterstützt",
    "SpeechRecognitionError": "Ein Spracherkennungsfehler ist aufgetreten",
    "PresetSwitching": "Wechsel zu {{presetName}}.",
    "WhisperError": "Ein Fehler ist bei der Spracherkennung durch Whisper aufgetreten"
  },
  "UsingOpenAITTS": "OpenAI verwenden",
  "OpenAITTSInfo": "OpenAI verwenden. Unterstützt mehrere Sprachen. Wenn Sie OpenAI als KI-Dienst auswählen, müssen Sie den API-Schlüssel unten nicht konfigurieren.",
  "OpenAITTSVoice": "Stimmtyp",
  "OpenAITTSModel": "Modell",
  "OpenAITTSSpeed": "Geschwindigkeit",
  "UsingAzureTTS": "Azure OpenAI verwenden",
  "AzureTTSInfo": "Azure OpenAI verwenden. Unterstützt mehrere Sprachen.",
  "SendMessage": {
    "title": "AITuberKit Externer Adapter",
    "directSendTitle": "Direkt zum KI-Charakter sprechen",
    "directSendDescription": "Sie können die Nachricht direkt an den KI-Charakter senden. Wenn mehrere Nachrichten gesendet werden, werden sie der Reihe nach verarbeitet. Das Sprachmodell ist das in den AITuberKit-Einstellungen ausgewählte.",
    "aiGenerateTitle": "KI-Antwort generieren und dann sprechen",
    "aiGenerateDescription": "Die KI generiert eine Antwort aus der gesendeten Nachricht und spricht sie dann. Wenn mehrere Nachrichten gesendet werden, werden sie der Reihe nach verarbeitet. Das KI-Modell und das Sprachmodell sind die in den AITuberKit-Einstellungen ausgewählten. Der Systemprompt kann ausgewählt werden, um entweder den AITuberKit-Systemprompt oder einen benutzerdefinierten Systemprompt zu verwenden. Wenn Sie den vorherigen Gesprächsverlauf laden möchten, fügen Sie die Zeichenfolge [conversation_history] in den Systemprompt oder die Benutzernachricht ein.",
    "useCurrentSystemPrompt": "AITuberKit-Systemprompt verwenden",
    "userInputTitle": "Benutzereingabe senden",
    "userInputDescription": "Die gesendete Nachricht wird wie bei der Eingabe über das AITuberKit-Eingabeformular verarbeitet. Wenn mehrere Nachrichten gesendet werden, werden sie der Reihe nach verarbeitet. Das KI-Modell und das Sprachmodell sind die in den AITuberKit-Einstellungen ausgewählten. Der Systemprompt und der Gesprächsverlauf sind die in AITuberKit konfigurierten Werte."
  },
  "CannotUseVoice": "Wenn der Echtzeit-API-Modus oder der Audiomodus aktiviert ist,\n sind die Einstellungen für synthetische Sprache nicht erforderlich.",
  "Live2D": {
    "FileInfo": "Platzieren Sie das Live2D-Modell, das Sie verwenden möchten, im Ordner public/live2d. Die Datei model3.json muss im Stammverzeichnis dieses Ordners existieren.\nWenn es nicht in der Auswahl angezeigt wird, laden Sie den Bildschirm neu oder überprüfen Sie, ob der Ordnerpfad korrekt ist.",
    "Info": "Sie können Emotionen und Bewegungen angeben.\nJede Emotion wird durch den Prompt gesteuert. Weitere Details finden Sie unter \"KI-Einstellungen => Charaktereinstellungen\".",
    "Emotions": "Emotionseinstellungen",
    "EmotionInfo": "Emotionen können im Komma-getrennten Format angegeben werden. Wenn mehrere Emotionen angegeben werden, werden sie zufällig ausgewählt.\nDer Anfangswert ist für das von AITuberKit bereitgestellte Modell. Wenn Sie ein eigenes Modell verwenden, geben Sie den Wert entsprechend Ihrem Modell ein.\nNach Abschluss des Gesprächs wird die Emotion \"Neutral\" angezeigt.",
    "neutralEmotions": "Neutral",
    "happyEmotions": "Glücklich",
    "sadEmotions": "Traurig",
    "angryEmotions": "Wütend",
    "relaxedEmotions": "Entspannt",
    "MotionGroups": "Bewegungsgruppen-Einstellungen",
    "MotionGroupsInfo": "Bewegungsgruppen werden zufällig aus der ausgewählten Gruppe ausgewählt.\nWie bei den Emotionseinstellungen konfigurieren Sie es entsprechend Ihrem Modell.\n\"Idle\" ist die Bewegung, die nach Abschluss des Gesprächs angezeigt wird.",
    "SelectMotionGroup": "Bewegungsgruppe auswählen",
    "idleMotionGroup": "Inaktiv",
    "neutralMotionGroup": "Neutral",
    "happyMotionGroup": "Glücklich",
    "sadMotionGroup": "Traurig",
    "angryMotionGroup": "Wütend",
    "relaxedMotionGroup": "Entspannt",
    "surprisedEmotions": "Überraschung",
    "surprisedMotionGroup": "Überraschung"
  },
  "UseVideoAsBackground": "Geteilten Bildschirm oder Webcam als Hintergrund verwenden",
  "Temperature": "Temperatur",
  "MaxTokens": "Maximale Tokenanzahl",
  "MaxTokensInfo": "Die maximale Tokenanzahl variiert je nach verwendetem KI-Modell. Bitte überprüfen Sie die Spezifikationen jedes Modells.",
  "CannotUseParameters": "Wenn der Echtzeit-API-Modus oder der Audiomodus aktiviert ist, können die Parameter Temperature und Max Tokens nicht angegeben werden.",
  "DocumentationDescription": "Detaillierte Anleitungen und Tutorials zur Verwendung von AITuberKit finden Sie unter der folgenden URL.",
  "PresetQuestions": "Voreingestellte Fragen",
  "PresetQuestionsInfo": "Sie können mehrere Frage-Muster im Voraus erstellen und registrieren. Die registrierten Fragen werden in Form von Schaltflächen in der Benutzeroberfläche angezeigt und beim Klicken in das Chat-Eingabefeld gesetzt.",
  "EnterPresetQuestion": "Bitte Frage eingeben",
  "DragToReorder": "Ziehen, um die Reihenfolge zu ändern",
  "ShowSilenceProgressBar": "Fortschrittsanzeige für Stille anzeigen",
  "CharacterpresetInfo": "Wenn Sie ein Preset auswählen, ändert sich der Charakter-Prompt.\nCmd + Shift + 1~5 (Mac) / Ctrl + Shift + 1~5 (Windows) ermöglicht Shortcuts.",
  "SpeechInputSettings": "Sprachinput-Einstellungen",
  "SpeechRecognitionMode": "Spracherkennungsmodus",
  "SpeechRecognitionModeInfo": "Sie können den Spracherkennungsmodus auswählen.\n„Browser Standard“ verwendet die im Browser integrierte Spracherkennung. „OpenAI TTS“ verwendet die Text-to-Speech-API von OpenAI.\nIm Allgemeinen wird „Browser Standard“ empfohlen, da es eine höhere Genauigkeit und schnellere Erkennungszeiten bietet. Wenn Sie jedoch einen Browser verwenden, der die WebSpeech-API nicht unterstützt, wie Firefox, wählen Sie bitte „OpenAI TTS“.",
  "BrowserSpeechRecognition": "Browser Standard Spracherkennung verwenden",
  "WhisperSpeechRecognition": "OpenAI TTS Spracherkennung verwenden",
  "WhisperAPIKeyInfo": "Im Whisper-Modus ist ein OpenAI-API-Schlüssel erforderlich. Bitte geben Sie den OpenAI-API-Schlüssel in den KI-Einstellungen ein.",
  "WhisperTranscriptionModel": "Transkriptionsmodell",
  "WhisperTranscriptionModelInfo": "Sie können das Modell auswählen, das für die Spracherkennung verwendet werden soll. Hochleistungsmodelle können genauer erkannt werden, können jedoch höhere API-Kosten verursachen.",
  "InitialSpeechTimeout": "Spracherkennung Timeout",
  "InitialSpeechTimeoutInfo": "Stellen Sie die Wartezeit ein, bis die erste Äußerung nach Beginn der Spracherkennung erkannt wird. Wenn innerhalb dieser Zeit keine Äußerung erkannt wird, wird die Spracherkennung automatisch gestoppt.\nWenn Sie 0 Sekunden einstellen, wird die Wartezeit unbegrenzt.",
  "Milliseconds": "Millisekunden",
  "ContinuousMic": "Ständige Mikrofoneingabe",
  "ContinuousMicActive": "Ständige Mikrofoneingabe aktiv",
  "ContinuousMicModeOn": "Der Modus für ständige Mikrofoneingabe ist aktiviert",
  "ContinuousMicModeOff": "Der Modus für ständige Mikrofoneingabe ist deaktiviert",
  "ListeningContinuously": "Warte auf Sprachinput...",
  "ContinuousMicInfo": "Das Mikrofon wird automatisch wieder aktiviert, wenn die Äußerung der KI beendet ist. Nach Ablauf der festgelegten Stillezeit wird automatisch gesendet.\nWenn die festgelegte Zeit überschritten wird, ohne dass eine Spracherkennung erfolgt, wird die ständige Mikrofoneingabe automatisch deaktiviert. Wenn Sie möchten, dass sie immer aktiviert bleibt, stellen Sie das Spracherkennung-Timeout auf 0 Sekunden ein.",
  "CustomAPIEndpoint": "Benutzerdefinierter API-Endpunkt",
  "CustomAPIEndpointInfo": "Bitte geben Sie die URL des API-Endpunkts ein, an den die POST-Anfrage gesendet werden soll.",
  "CustomAPIStream": "Streaming-Modus",
  "CustomAPIStreamForced": "Der Streaming-Modus ist derzeit immer aktiviert.",
  "CustomAPIHeaders": "Benutzerdefinierte Header",
  "CustomAPIHeadersInfo": "Bitte geben Sie die Header-Informationen im JSON-Format ein, die in die API-Anfrage aufgenommen werden sollen.",
  "CustomAPIBody": "Benutzerdefinierter Body",
  "CustomAPIBodyInfo": "Bitte geben Sie die Body-Informationen im JSON-Format ein, die in die API-Anfrage aufgenommen werden sollen. messages werden automatisch hinzugefügt.",
  "CustomAPIDescription": "Hinweis: Nachrichten werden automatisch in den Anfrage-Body aufgenommen. Im Streaming-Modus muss der Server text/event-stream zurückgeben.",
  "ShowCharacterPresetMenu": "Charakter-Preset-Menü-Button anzeigen",
  "SpeechRecognitionModeDisabledInfo": "Wenn der Audiomodus aktiviert ist, ist nur die Browser-Spracherkennung verfügbar.\nAußerdem ist im Echtzeit-API-Modus nur die Browser-Spracherkennung verfügbar, und die Funktion für das Spracherkennung-Timeout wird deaktiviert."
}

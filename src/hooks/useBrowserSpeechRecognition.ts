import { useState, useEffect, useCallback, useRef, useMemo } from 'react'
import { getVoiceLanguageCode } from '@/utils/voiceLanguage'
import settingsStore from '@/features/stores/settings'
import toastStore from '@/features/stores/toast'
import homeStore from '@/features/stores/home'
import { useTranslation } from 'react-i18next'
import { useSilenceDetection } from './useSilenceDetection'
import { SpeakQueue } from '@/features/messages/speakQueue'

/**
 * ãƒ–ãƒ©ã‚¦ã‚¶ã®éŸ³å£°èªè­˜APIã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ•ãƒƒã‚¯
 */
export function useBrowserSpeechRecognition(
  onChatProcessStart: (text: string) => void
) {
  const { t } = useTranslation()
  const selectLanguage = settingsStore((s) => s.selectLanguage)
  const initialSpeechTimeout = settingsStore((s) => s.initialSpeechTimeout)

  // ----- çŠ¶æ…‹ç®¡ç† -----
  const [userMessage, setUserMessage] = useState('')
  const [isListening, setIsListening] = useState(false)
  const isListeningRef = useRef(false)

  // ----- éŸ³å£°èªè­˜é–¢é€£ -----
  const [recognition, setRecognition] = useState<SpeechRecognition | null>(null)
  const transcriptRef = useRef('')
  const speechDetectedRef = useRef<boolean>(false)
  const recognitionStartTimeRef = useRef<number>(0)
  const initialSpeechCheckTimerRef = useRef<NodeJS.Timeout | null>(null)
  // ----- ç«¶åˆçŠ¶æ…‹é˜²æ­¢: å†èµ·å‹•ã‚¿ã‚¤ãƒãƒ¼ã®è¿½è·¡ -----
  const restartTimeoutRef = useRef<NodeJS.Timeout | null>(null)
  // ----- éŸ³å£°èªè­˜ãŒå®Ÿéš›ã«å‹•ä½œä¸­ã‹ã©ã†ã‹ã‚’è¿½è·¡ -----
  // true: onstartç™ºç«æ¸ˆã¿ï¼ˆå‹•ä½œä¸­ï¼‰, false: onendç™ºç«æ¸ˆã¿ï¼ˆåœæ­¢ä¸­ï¼‰
  const recognitionActiveRef = useRef<boolean>(false)
  // ----- é–‹å§‹å‡¦ç†ä¸­ã‹ã©ã†ã‹ã‚’è¿½è·¡ï¼ˆæ’ä»–åˆ¶å¾¡ç”¨ï¼‰ -----
  const isStartingRef = useRef<boolean>(false)

  // ----- ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ãƒˆãƒªã‚¬ãƒ¼é–¢é€£ -----
  const keyPressStartTime = useRef<number | null>(null)
  const isKeyboardTriggered = useRef(false)

  // ----- ç„¡éŸ³æ¤œå‡ºãƒ•ãƒƒã‚¯ã‚’ä½¿ç”¨ -----
  const {
    silenceTimeoutRemaining,
    clearSilenceDetection,
    startSilenceDetection,
    updateSpeechTimestamp,
    isSpeechEnded,
  } = useSilenceDetection({
    onTextDetected: onChatProcessStart,
    transcriptRef,
    setUserMessage,
    speechDetectedRef,
  })

  // ----- åˆæœŸéŸ³å£°æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹é–¢æ•° -----
  const clearInitialSpeechCheckTimer = useCallback(() => {
    if (initialSpeechCheckTimerRef.current) {
      clearTimeout(initialSpeechCheckTimerRef.current)
      initialSpeechCheckTimerRef.current = null
    }
  }, [])

  // ----- éŸ³å£°æœªæ¤œå‡ºæ™‚ã®åœæ­¢å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹å…±é€šé–¢æ•° (Requirement 5.1) -----
  const handleNoSpeechTimeout = useCallback(
    async (stopListeningFn: () => Promise<void>) => {
      console.log(
        `â±ï¸ ${initialSpeechTimeout}ç§’é–“éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚éŸ³å£°èªè­˜ã‚’åœæ­¢ã—ã¾ã™ã€‚`
      )
      await stopListeningFn()

      // å¸¸æ™‚ãƒã‚¤ã‚¯å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚ªãƒ•ã«è¨­å®š
      if (settingsStore.getState().continuousMicListeningMode) {
        console.log(
          'ğŸ”‡ éŸ³å£°æœªæ¤œå‡ºã«ã‚ˆã‚Šå¸¸æ™‚ãƒã‚¤ã‚¯å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰ã‚’OFFã«è¨­å®šã—ã¾ã™ã€‚'
        )
        settingsStore.setState({ continuousMicListeningMode: false })
      }

      toastStore.getState().addToast({
        message: t('Toasts.NoSpeechDetected'),
        type: 'info',
        tag: 'no-speech-detected',
      })
    },
    [initialSpeechTimeout, t]
  )

  // ----- åˆæœŸéŸ³å£°æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹å…±é€šé–¢æ•° (Requirement 5.1) -----
  const setupInitialSpeechTimer = useCallback(
    (stopListeningFn: () => Promise<void>) => {
      // æ—¢å­˜ã®ã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢ã—ã¦ã‹ã‚‰æ–°ã—ã„ã‚¿ã‚¤ãƒãƒ¼ã‚’è¨­å®š (Requirement 5.2)
      clearInitialSpeechCheckTimer()

      if (initialSpeechTimeout > 0) {
        initialSpeechCheckTimerRef.current = setTimeout(() => {
          if (!speechDetectedRef.current && isListeningRef.current) {
            handleNoSpeechTimeout(stopListeningFn)
          }
        }, initialSpeechTimeout * 1000)
      }
    },
    [initialSpeechTimeout, clearInitialSpeechCheckTimer, handleNoSpeechTimeout]
  )

  // ----- éŸ³å£°èªè­˜åœæ­¢å‡¦ç† -----
  const stopListening = useCallback(async () => {
    // ä¿ç•™ä¸­ã®å†èµ·å‹•ã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ« (ç«¶åˆçŠ¶æ…‹é˜²æ­¢)
    if (restartTimeoutRef.current) {
      clearTimeout(restartTimeoutRef.current)
      restartTimeoutRef.current = null
    }

    // å„ç¨®ã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢
    clearSilenceDetection()
    clearInitialSpeechCheckTimer()

    // ãƒªã‚¹ãƒ‹ãƒ³ã‚°çŠ¶æ…‹ã‚’æ›´æ–°
    isListeningRef.current = false
    setIsListening(false)

    if (!recognition) return

    // éŸ³å£°èªè­˜ã‚’åœæ­¢
    try {
      recognition.stop()
    } catch (error) {
      console.error('Error stopping recognition:', error)
    }

    // ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ãƒˆãƒªã‚¬ãƒ¼ã®å ´åˆã®å‡¦ç†
    const trimmedTranscriptRef = transcriptRef.current.trim()
    if (isKeyboardTriggered.current) {
      const pressDuration = Date.now() - (keyPressStartTime.current || 0)
      // æŠ¼ã—ã¦ã‹ã‚‰1ç§’ä»¥ä¸Š ã‹ã¤ æ–‡å­—ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿é€ä¿¡
      // ç„¡éŸ³æ¤œå‡ºã«ã‚ˆã‚‹è‡ªå‹•é€ä¿¡ãŒæ—¢ã«è¡Œã‚ã‚Œã¦ã„ãªã„å ´åˆã®ã¿é€ä¿¡ã™ã‚‹
      if (pressDuration >= 1000 && trimmedTranscriptRef && !isSpeechEnded()) {
        onChatProcessStart(trimmedTranscriptRef)
        setUserMessage('')
      }
      isKeyboardTriggered.current = false
    }
  }, [
    clearSilenceDetection,
    clearInitialSpeechCheckTimer,
    recognition,
    isSpeechEnded,
    onChatProcessStart,
  ])

  // ----- ãƒã‚¤ã‚¯æ¨©é™ç¢ºèª -----
  const checkMicrophonePermission = useCallback(async (): Promise<boolean> => {
    // Firefoxã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã—ã¦çµ‚äº†
    if (navigator.userAgent.toLowerCase().includes('firefox')) {
      toastStore.getState().addToast({
        message: t('Toasts.FirefoxNotSupported'),
        type: 'error',
        tag: 'microphone-permission-error-firefox',
      })
      return false
    }

    try {
      // getUserMediaã‚’ç›´æ¥å‘¼ã³å‡ºã—ã€ãƒ–ãƒ©ã‚¦ã‚¶ã®ãƒã‚¤ãƒ†ã‚£ãƒ–è¨±å¯ãƒ¢ãƒ¼ãƒ€ãƒ«ã‚’è¡¨ç¤º
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      stream.getTracks().forEach((track) => track.stop())
      return true
    } catch (error) {
      // ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ˜ç¤ºçš„ã«æ‹’å¦ã—ãŸå ´åˆã‚„ã€ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã®å ´åˆ
      console.error('Microphone permission error:', error)
      toastStore.getState().addToast({
        message: t('Toasts.MicrophonePermissionDenied'),
        type: 'error',
        tag: 'microphone-permission-error',
      })
      return false
    }
  }, [t])

  // ----- éŸ³å£°èªè­˜é–‹å§‹å‡¦ç† -----
  const startListening = useCallback(async () => {
    // æ’ä»–åˆ¶å¾¡: æ—¢ã«é–‹å§‹å‡¦ç†ä¸­ã®å ´åˆã¯ä½•ã‚‚ã—ãªã„
    if (isStartingRef.current) {
      console.log('Recognition start already in progress, skipping')
      return
    }
    isStartingRef.current = true

    // ä¿ç•™ä¸­ã®å†èµ·å‹•ã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ« (onendãƒãƒ³ãƒ‰ãƒ©ã¨ã®ç«¶åˆçŠ¶æ…‹é˜²æ­¢)
    if (restartTimeoutRef.current) {
      clearTimeout(restartTimeoutRef.current)
      restartTimeoutRef.current = null
    }

    try {
      const hasPermission = await checkMicrophonePermission()
      if (!hasPermission) {
        isStartingRef.current = false
        return
      }

      if (!recognition) {
        isStartingRef.current = false
        return
      }

      // æ—¢ã«èªè­˜ãŒé–‹å§‹ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€onendã‚¤ãƒ™ãƒ³ãƒˆã‚’å¾…ã£ã¦åœæ­¢ã‚’ç¢ºèª
      if (isListeningRef.current) {
        await new Promise<void>((resolve) => {
          let timeoutId: NodeJS.Timeout

          const onEndHandler = () => {
            clearTimeout(timeoutId)
            recognition.removeEventListener('end', onEndHandler)
            resolve()
          }

          timeoutId = setTimeout(() => {
            recognition.removeEventListener('end', onEndHandler)
            console.log('Recognition stop timeout, forcing resolve')
            resolve()
          }, 500) // 500ms ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ

          recognition.addEventListener('end', onEndHandler)
          try {
            recognition.stop()
          } catch (err) {
            clearTimeout(timeoutId)
            recognition.removeEventListener('end', onEndHandler)
            resolve()
          }
        })
        // çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
        isListeningRef.current = false
        setIsListening(false)
      }

      // ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
      transcriptRef.current = ''
      setUserMessage('')

      try {
        // éŸ³å£°èªè­˜ãŒã¾ã å‹•ä½œä¸­ã®å ´åˆã¯ã€onendã‚’å¾…ã¤
        if (recognitionActiveRef.current) {
          console.log('Recognition still active, waiting for onend...')
          await new Promise<void>((resolve) => {
            let timeoutId: NodeJS.Timeout
            const onEndHandler = () => {
              clearTimeout(timeoutId)
              recognition.removeEventListener('end', onEndHandler)
              resolve()
            }
            timeoutId = setTimeout(() => {
              recognition.removeEventListener('end', onEndHandler)
              console.log('Recognition active wait timeout, forcing resolve')
              resolve()
            }, 500)
            recognition.addEventListener('end', onEndHandler)
          })
        }

        recognition.start()
        console.log('Recognition started successfully')
        // ãƒªã‚¹ãƒ‹ãƒ³ã‚°çŠ¶æ…‹ã‚’æ›´æ–°
        isListeningRef.current = true
        setIsListening(true)
      } catch (error) {
        console.error('Error starting recognition:', error)

        // InvalidStateErrorã®å ´åˆã¯ã€æ—¢ã«é–‹å§‹ã•ã‚Œã¦ã„ã‚‹ã¨ã¿ãªã™
        if (
          error instanceof DOMException &&
          error.name === 'InvalidStateError'
        ) {
          console.log('Recognition is already running, skipping retry')
          // æ—¢ã«å®Ÿè¡Œä¸­ãªã®ã§ã€ãƒªã‚¹ãƒ‹ãƒ³ã‚°çŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹
          isListeningRef.current = true
          setIsListening(true)

          // onstart ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ã¨åŒæ§˜ã®å‡¦ç†ã‚’æ‰‹å‹•ã§å®Ÿè¡Œ
          console.log('Speech recognition started (manually triggered)')
          recognitionStartTimeRef.current = Date.now()
          speechDetectedRef.current = false

          // åˆæœŸéŸ³å£°æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼è¨­å®š (Requirement 5.2: å…±é€šé–¢æ•°ã‚’ä½¿ç”¨)
          setupInitialSpeechTimer(stopListening)

          // ç„¡éŸ³æ¤œå‡ºé–‹å§‹
          startSilenceDetection(stopListening)
        } else {
          // ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã®å ´åˆã®ã¿å†è©¦è¡Œ
          setTimeout(() => {
            try {
              if (recognition) {
                // ä¸€åº¦ç¢ºå®Ÿã«åœæ­¢ã‚’è©¦ã¿ã‚‹
                try {
                  recognition.stop()
                  // åœæ­¢å¾Œã«çŸ­ã„é…å»¶
                  setTimeout(() => {
                    recognition.start()
                    console.log('Recognition started on retry')
                    isListeningRef.current = true
                    setIsListening(true)
                  }, 100)
                } catch (stopError) {
                  // åœæ­¢ã§ããªã‹ã£ãŸå ´åˆã¯ç›´æ¥ã‚¹ã‚¿ãƒ¼ãƒˆ
                  try {
                    recognition.start()
                    console.log('Recognition started on retry without stopping')
                    isListeningRef.current = true
                    setIsListening(true)
                  } catch (startError) {
                    console.error(
                      'Failed to start recognition on retry:',
                      startError
                    )
                    isListeningRef.current = false
                    setIsListening(false)
                  }
                }
              }
            } catch (retryError) {
              console.error('Failed to start recognition on retry:', retryError)
              isListeningRef.current = false
              setIsListening(false)
              return
            }
          }, 300)
        }
      }
    } finally {
      // æ’ä»–åˆ¶å¾¡ã‚’è§£é™¤
      isStartingRef.current = false
    }
  }, [recognition, checkMicrophonePermission])

  // ----- éŸ³å£°èªè­˜ãƒˆã‚°ãƒ«å‡¦ç† -----
  const toggleListening = useCallback(() => {
    if (isListeningRef.current) {
      stopListening()
    } else {
      keyPressStartTime.current = Date.now()
      isKeyboardTriggered.current = true
      startListening()
      // AIã®ç™ºè©±ã‚’åœæ­¢
      homeStore.setState({ isSpeaking: false })
    }
  }, [startListening, stopListening])

  // ----- ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ -----
  const handleSendMessage = useCallback(async () => {
    const trimmedMessage = userMessage.trim()
    if (trimmedMessage) {
      // AIã®ç™ºè©±ã‚’åœæ­¢
      homeStore.setState({ isSpeaking: false })
      SpeakQueue.stopAll()

      // ãƒã‚¤ã‚¯å…¥åŠ›ã‚’åœæ­¢ï¼ˆå¸¸æ™‚éŸ³å£°å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰æ™‚ã‚‚è‡ªå‹•é€ä¿¡ã¨åŒæ§˜ã«åœæ­¢ï¼‰
      await stopListening()

      onChatProcessStart(trimmedMessage)
      setUserMessage('')
    }
  }, [userMessage, onChatProcessStart, stopListening])

  // ----- ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¥åŠ› -----
  const handleInputChange = useCallback(
    (e: React.ChangeEvent<HTMLInputElement | HTMLTextAreaElement>) => {
      setUserMessage(e.target.value)
    },
    []
  )

  // ----- éŸ³å£°èªè­˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®åˆæœŸåŒ–ã¨ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©è¨­å®š -----
  useEffect(() => {
    const SpeechRecognition =
      window.SpeechRecognition || window.webkitSpeechRecognition

    if (!SpeechRecognition) {
      console.error('Speech Recognition API is not supported in this browser')
      toastStore.getState().addToast({
        message: t('Toasts.SpeechRecognitionNotSupported'),
        type: 'error',
        tag: 'speech-recognition-not-supported',
      })
      return
    }

    const newRecognition = new SpeechRecognition()
    newRecognition.lang = getVoiceLanguageCode(selectLanguage)
    newRecognition.continuous = true
    newRecognition.interimResults = true

    // ----- ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ã®è¨­å®š -----

    // éŸ³å£°èªè­˜é–‹å§‹æ™‚
    newRecognition.onstart = () => {
      console.log('Speech recognition started')
      recognitionStartTimeRef.current = Date.now()
      speechDetectedRef.current = false
      // éŸ³å£°èªè­˜ãŒå®Ÿéš›ã«å‹•ä½œä¸­ã§ã‚ã‚‹ã“ã¨ã‚’è¨˜éŒ²
      recognitionActiveRef.current = true

      // åˆæœŸéŸ³å£°æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼è¨­å®š (Requirement 5.2: å…±é€šé–¢æ•°ã‚’ä½¿ç”¨)
      setupInitialSpeechTimer(stopListening)

      // ç„¡éŸ³æ¤œå‡ºé–‹å§‹
      startSilenceDetection(stopListening)
    }

    // éŸ³å£°å…¥åŠ›æ¤œå‡ºæ™‚
    newRecognition.onspeechstart = () => {
      console.log('ğŸ—£ï¸ éŸ³å£°å…¥åŠ›ã‚’æ¤œå‡ºã—ã¾ã—ãŸï¼ˆonspeechstartï¼‰')
      // ã“ã“ã§ã¯ã‚¿ã‚¤ãƒãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹ã ã‘ã§ã€speechDetectedRefã¯è¨­å®šã—ãªã„
      updateSpeechTimestamp()
    }

    // éŸ³é‡ãƒ¬ãƒ™ãƒ«è¿½è·¡ç”¨å¤‰æ•°
    let lastTranscriptLength = 0

    // éŸ³å£°èªè­˜çµæœãŒå¾—ã‚‰ã‚ŒãŸã¨ã
    newRecognition.onresult = (event) => {
      if (!isListeningRef.current) return

      const transcript = Array.from(event.results)
        .map((result) => result[0].transcript)
        .join('')

      // æœ‰æ„ãªå¤‰åŒ–ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
      const isSignificantChange =
        transcript.trim().length > lastTranscriptLength
      lastTranscriptLength = transcript.trim().length

      if (isSignificantChange) {
        console.log('ğŸ¤ æœ‰æ„ãªéŸ³å£°ã‚’æ¤œå‡ºã—ã¾ã—ãŸï¼ˆãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆå¤‰æ›´ã‚ã‚Šï¼‰')
        updateSpeechTimestamp()
        speechDetectedRef.current = true
      } else {
        console.log(
          'ğŸ”‡ ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒã‚¤ã‚ºã‚’ç„¡è¦–ã—ã¾ã™ï¼ˆãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆå¤‰æ›´ãªã—ï¼‰'
        )
      }

      transcriptRef.current = transcript
      setUserMessage(transcript)
    }

    // éŸ³å£°å…¥åŠ›çµ‚äº†æ™‚
    newRecognition.onspeechend = () => {
      console.log(
        'ğŸ›‘ éŸ³å£°å…¥åŠ›ãŒçµ‚äº†ã—ã¾ã—ãŸï¼ˆonspeechendï¼‰ã€‚ç„¡éŸ³æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼ãŒå‹•ä½œä¸­ã§ã™ã€‚'
      )
    }

    // éŸ³å£°èªè­˜çµ‚äº†æ™‚
    newRecognition.onend = () => {
      console.log('Recognition ended')
      // éŸ³å£°èªè­˜ãŒåœæ­¢ã—ãŸã“ã¨ã‚’è¨˜éŒ²
      recognitionActiveRef.current = false
      clearSilenceDetection()
      clearInitialSpeechCheckTimer()

      // isListeningRef.currentãŒtrueã®å ´åˆã¯å†é–‹
      if (isListeningRef.current) {
        console.log('Restarting speech recognition...')
        // å†èµ·å‹•ã‚¿ã‚¤ãƒãƒ¼ã‚’refã«ä¿å­˜ã—ã¦è¿½è·¡ (ç«¶åˆçŠ¶æ…‹é˜²æ­¢)
        restartTimeoutRef.current = setTimeout(() => {
          // setTimeoutå®Ÿè¡Œæ™‚ã«å†åº¦çŠ¶æ…‹ã‚’ç¢ºèª (ç«¶åˆçŠ¶æ…‹é˜²æ­¢)
          if (isListeningRef.current) {
            startListening()
          }
          restartTimeoutRef.current = null
        }, 1000)
      }
    }

    // éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼æ™‚
    newRecognition.onerror = (event) => {
      console.error('Speech recognition error:', event.error)

      // no-speechã‚¨ãƒ©ãƒ¼ã®å ´åˆ
      if (event.error === 'no-speech' && isListeningRef.current) {
        // åˆå›éŸ³å£°æ¤œå‡ºã•ã‚Œã¦ã„ãªã„å ´åˆã®ã¿ã€ç´¯ç©æ™‚é–“ã‚’ãƒã‚§ãƒƒã‚¯
        if (!speechDetectedRef.current && initialSpeechTimeout > 0) {
          // èªè­˜é–‹å§‹ã‹ã‚‰ã®çµŒéæ™‚é–“ã‚’è¨ˆç®—
          const elapsedTime =
            (Date.now() - recognitionStartTimeRef.current) / 1000
          console.log(
            `éŸ³å£°æœªæ¤œå‡ºã®ç´¯ç©æ™‚é–“: ${elapsedTime.toFixed(1)}ç§’ / è¨­å®š: ${initialSpeechTimeout}ç§’`
          )

          // è¨­å®šã•ã‚ŒãŸåˆæœŸéŸ³å£°ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¶…ãˆãŸå ´åˆã¯ã€å†èµ·å‹•ã›ãšã«çµ‚äº†
          if (elapsedTime >= initialSpeechTimeout) {
            clearSilenceDetection()
            clearInitialSpeechCheckTimer()
            // å…±é€šé–¢æ•°ã‚’ä½¿ç”¨ (Requirement 5.3)
            handleNoSpeechTimeout(stopListening)
            return
          }
        }

        // éŸ³å£°ãŒæ—¢ã«æ¤œå‡ºã•ã‚Œã¦ã„ã‚‹å ´åˆã€ã¾ãŸã¯åˆæœŸã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã«é”ã—ã¦ã„ãªã„å ´åˆã¯
        // onendãƒãƒ³ãƒ‰ãƒ©ã«å†èµ·å‹•ã‚’å§”ã­ã‚‹ï¼ˆç›´æ¥start()ã‚’å‘¼ã¶ã¨ç«¶åˆçŠ¶æ…‹ãŒç™ºç”Ÿã™ã‚‹ãŸã‚ï¼‰
        if (
          isListeningRef.current &&
          !homeStore.getState().chatProcessing &&
          (settingsStore.getState().continuousMicListeningMode ||
            isKeyboardTriggered.current)
        ) {
          console.log('No speech detected, will restart via onend handler...')
          // onendãƒãƒ³ãƒ‰ãƒ©ãŒè‡ªå‹•çš„ã«å†èµ·å‹•ã™ã‚‹
        } else {
          console.log(
            'éŸ³å£°èªè­˜ã®å†èµ·å‹•ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ï¼ˆå¸¸æ™‚ãƒã‚¤ã‚¯ãƒ¢ãƒ¼ãƒ‰ãŒã‚ªãƒ•ã¾ãŸã¯ä»–ã®æ¡ä»¶ã‚’æº€ãŸã•ãªã„ï¼‰'
          )
          isListeningRef.current = false
          setIsListening(false)
        }
      } else {
        // ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯é€šå¸¸ã®çµ‚äº†å‡¦ç†
        clearSilenceDetection()
        clearInitialSpeechCheckTimer()
        stopListening()
      }
    }

    setRecognition(newRecognition)

    // ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–¢æ•°
    return () => {
      // ä¿ç•™ä¸­ã®å†èµ·å‹•ã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢
      if (restartTimeoutRef.current) {
        clearTimeout(restartTimeoutRef.current)
        restartTimeoutRef.current = null
      }
      try {
        if (newRecognition) {
          newRecognition.onstart = null
          newRecognition.onspeechstart = null
          newRecognition.onresult = null
          newRecognition.onspeechend = null
          newRecognition.onend = null
          newRecognition.onerror = null
          newRecognition.abort()
        }
      } catch (error) {
        console.error('Error cleaning up speech recognition:', error)
      }
      clearSilenceDetection()
      clearInitialSpeechCheckTimer()
    }
  }, [
    selectLanguage,
    initialSpeechTimeout,
    t,
    // stopListening,
    clearSilenceDetection,
    clearInitialSpeechCheckTimer,
    startSilenceDetection,
    updateSpeechTimestamp,
    setupInitialSpeechTimer,
    handleNoSpeechTimeout,
  ])

  // ----- éŸ³å£°èªè­˜ãŒå®Ÿéš›ã«ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹é–¢æ•° -----
  const checkRecognitionActive = useCallback(() => {
    // onstartç™ºç«æ¸ˆã¿ã§onendæœªç™ºç«ãªã‚‰å‹•ä½œä¸­
    return recognitionActiveRef.current
  }, [])

  // æˆ»ã‚Šå€¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ãƒ¡ãƒ¢åŒ–ï¼ˆRequirement 1.1, 1.4ï¼‰
  const returnValue = useMemo(
    () => ({
      userMessage,
      isListening,
      silenceTimeoutRemaining,
      handleInputChange,
      handleSendMessage,
      toggleListening,
      startListening,
      stopListening,
      checkRecognitionActive,
    }),
    [
      userMessage,
      isListening,
      silenceTimeoutRemaining,
      handleInputChange,
      handleSendMessage,
      toggleListening,
      startListening,
      stopListening,
      checkRecognitionActive,
    ]
  )

  return returnValue
}

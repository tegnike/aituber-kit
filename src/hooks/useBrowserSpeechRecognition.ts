import { useState, useEffect, useCallback, useRef } from 'react'
import { getVoiceLanguageCode } from '@/utils/voiceLanguage'
import settingsStore from '@/features/stores/settings'
import toastStore from '@/features/stores/toast'
import homeStore from '@/features/stores/home'
import { useTranslation } from 'react-i18next'
import { useSilenceDetection } from './useSilenceDetection'
import { SpeakQueue } from '@/features/messages/speakQueue'

/**
 * ãƒ–ãƒ©ã‚¦ã‚¶ã®éŸ³å£°èªè­˜APIã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ•ãƒƒã‚¯
 */
export const useBrowserSpeechRecognition = (
  onChatProcessStart: (text: string) => void
) => {
  const { t } = useTranslation()
  const selectLanguage = settingsStore((s) => s.selectLanguage)
  const initialSpeechTimeout = settingsStore((s) => s.initialSpeechTimeout)

  // ----- çŠ¶æ…‹ç®¡ç† -----
  const [userMessage, setUserMessage] = useState('')
  const [isListening, setIsListening] = useState(false)
  const isListeningRef = useRef(false)

  // ----- éŸ³å£°èªè­˜é–¢é€£ -----
  const [recognition, setRecognition] = useState<SpeechRecognition | null>(null)
  const transcriptRef = useRef('')
  const speechDetectedRef = useRef<boolean>(false)
  const recognitionStartTimeRef = useRef<number>(0)
  const initialSpeechCheckTimerRef = useRef<NodeJS.Timeout | null>(null)

  // ----- ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ãƒˆãƒªã‚¬ãƒ¼é–¢é€£ -----
  const keyPressStartTime = useRef<number | null>(null)
  const isKeyboardTriggered = useRef(false)

  // ----- ç„¡éŸ³æ¤œå‡ºãƒ•ãƒƒã‚¯ã‚’ä½¿ç”¨ -----
  const {
    silenceTimeoutRemaining,
    clearSilenceDetection,
    startSilenceDetection,
    updateSpeechTimestamp,
    isSpeechEnded,
  } = useSilenceDetection({
    onTextDetected: onChatProcessStart,
    transcriptRef,
    setUserMessage,
    speechDetectedRef,
  })

  // ----- åˆæœŸéŸ³å£°æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹é–¢æ•° -----
  const clearInitialSpeechCheckTimer = useCallback(() => {
    if (initialSpeechCheckTimerRef.current) {
      clearTimeout(initialSpeechCheckTimerRef.current)
      initialSpeechCheckTimerRef.current = null
    }
  }, [])

  // ----- éŸ³å£°èªè­˜åœæ­¢å‡¦ç† -----
  const stopListening = useCallback(async () => {
    // å„ç¨®ã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢
    clearSilenceDetection()
    clearInitialSpeechCheckTimer()

    // ãƒªã‚¹ãƒ‹ãƒ³ã‚°çŠ¶æ…‹ã‚’æ›´æ–°
    isListeningRef.current = false
    setIsListening(false)

    if (!recognition) return

    // éŸ³å£°èªè­˜ã‚’åœæ­¢
    try {
      recognition.stop()
    } catch (error) {
      console.error('Error stopping recognition:', error)
    }

    // ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ãƒˆãƒªã‚¬ãƒ¼ã®å ´åˆã®å‡¦ç†
    const trimmedTranscriptRef = transcriptRef.current.trim()
    if (isKeyboardTriggered.current) {
      const pressDuration = Date.now() - (keyPressStartTime.current || 0)
      // æŠ¼ã—ã¦ã‹ã‚‰1ç§’ä»¥ä¸Š ã‹ã¤ æ–‡å­—ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿é€ä¿¡
      // ç„¡éŸ³æ¤œå‡ºã«ã‚ˆã‚‹è‡ªå‹•é€ä¿¡ãŒæ—¢ã«è¡Œã‚ã‚Œã¦ã„ãªã„å ´åˆã®ã¿é€ä¿¡ã™ã‚‹
      if (pressDuration >= 1000 && trimmedTranscriptRef && !isSpeechEnded()) {
        onChatProcessStart(trimmedTranscriptRef)
        setUserMessage('')
      }
      isKeyboardTriggered.current = false
    }
  }, [
    clearSilenceDetection,
    clearInitialSpeechCheckTimer,
    recognition,
    isSpeechEnded,
    onChatProcessStart,
  ])

  // ----- ãƒã‚¤ã‚¯æ¨©é™ç¢ºèª -----
  const checkMicrophonePermission = useCallback(async (): Promise<boolean> => {
    // Firefoxã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã—ã¦çµ‚äº†
    if (navigator.userAgent.toLowerCase().includes('firefox')) {
      toastStore.getState().addToast({
        message: t('Toasts.FirefoxNotSupported'),
        type: 'error',
        tag: 'microphone-permission-error-firefox',
      })
      return false
    }

    try {
      // getUserMediaã‚’ç›´æ¥å‘¼ã³å‡ºã—ã€ãƒ–ãƒ©ã‚¦ã‚¶ã®ãƒã‚¤ãƒ†ã‚£ãƒ–è¨±å¯ãƒ¢ãƒ¼ãƒ€ãƒ«ã‚’è¡¨ç¤º
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      stream.getTracks().forEach((track) => track.stop())
      return true
    } catch (error) {
      // ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ˜ç¤ºçš„ã«æ‹’å¦ã—ãŸå ´åˆã‚„ã€ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã®å ´åˆ
      console.error('Microphone permission error:', error)
      toastStore.getState().addToast({
        message: t('Toasts.MicrophonePermissionDenied'),
        type: 'error',
        tag: 'microphone-permission-error',
      })
      return false
    }
  }, [t])

  // ----- éŸ³å£°èªè­˜é–‹å§‹å‡¦ç† -----
  const startListening = useCallback(async () => {
    const hasPermission = await checkMicrophonePermission()
    if (!hasPermission) return

    if (!recognition) return

    // æ—¢ã«èªè­˜ãŒé–‹å§‹ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ä¸€åº¦åœæ­¢ã—ã¦ã‹ã‚‰å†é–‹ã™ã‚‹
    if (isListeningRef.current) {
      try {
        recognition.stop()
        // åœæ­¢å®Œäº†ã‚’å¾…ã¤ãŸã‚ã®çŸ­ã„é…å»¶
        await new Promise((resolve) => setTimeout(resolve, 100))
      } catch (err) {
        console.log('Recognition was not running, proceeding to start', err)
      }
    }

    // ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
    transcriptRef.current = ''
    setUserMessage('')

    try {
      recognition.start()
      console.log('Recognition started successfully')
      // ãƒªã‚¹ãƒ‹ãƒ³ã‚°çŠ¶æ…‹ã‚’æ›´æ–°
      isListeningRef.current = true
      setIsListening(true)
    } catch (error) {
      console.error('Error starting recognition:', error)

      // InvalidStateErrorã®å ´åˆã¯ã€æ—¢ã«é–‹å§‹ã•ã‚Œã¦ã„ã‚‹ã¨ã¿ãªã™
      if (error instanceof DOMException && error.name === 'InvalidStateError') {
        console.log('Recognition is already running, skipping retry')
        // æ—¢ã«å®Ÿè¡Œä¸­ãªã®ã§ã€ãƒªã‚¹ãƒ‹ãƒ³ã‚°çŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹
        isListeningRef.current = true
        setIsListening(true)

        // onstart ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ã¨åŒæ§˜ã®å‡¦ç†ã‚’æ‰‹å‹•ã§å®Ÿè¡Œ
        console.log('Speech recognition started (manually triggered)')
        recognitionStartTimeRef.current = Date.now()
        speechDetectedRef.current = false

        // åˆæœŸéŸ³å£°æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼è¨­å®š
        if (initialSpeechTimeout > 0) {
          initialSpeechCheckTimerRef.current = setTimeout(() => {
            if (!speechDetectedRef.current && isListeningRef.current) {
              console.log(
                `â±ï¸ ${initialSpeechTimeout}ç§’é–“éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚éŸ³å£°èªè­˜ã‚’åœæ­¢ã—ã¾ã™ã€‚`
              )
              stopListening()

              // å¸¸æ™‚ãƒã‚¤ã‚¯å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚ªãƒ•ã«è¨­å®š
              if (settingsStore.getState().continuousMicListeningMode) {
                console.log(
                  'ğŸ”‡ éŸ³å£°æœªæ¤œå‡ºã«ã‚ˆã‚Šå¸¸æ™‚ãƒã‚¤ã‚¯å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰ã‚’OFFã«è¨­å®šã—ã¾ã™ã€‚'
                )
                settingsStore.setState({ continuousMicListeningMode: false })
              }

              toastStore.getState().addToast({
                message: t('Toasts.NoSpeechDetected'),
                type: 'info',
                tag: 'no-speech-detected',
              })
            }
          }, initialSpeechTimeout * 1000)
        }

        // ç„¡éŸ³æ¤œå‡ºé–‹å§‹
        startSilenceDetection(stopListening)
      } else {
        // ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã®å ´åˆã®ã¿å†è©¦è¡Œ
        setTimeout(() => {
          try {
            if (recognition) {
              // ä¸€åº¦ç¢ºå®Ÿã«åœæ­¢ã‚’è©¦ã¿ã‚‹
              try {
                recognition.stop()
                // åœæ­¢å¾Œã«çŸ­ã„é…å»¶
                setTimeout(() => {
                  recognition.start()
                  console.log('Recognition started on retry')
                  isListeningRef.current = true
                  setIsListening(true)
                }, 100)
              } catch (stopError) {
                // åœæ­¢ã§ããªã‹ã£ãŸå ´åˆã¯ç›´æ¥ã‚¹ã‚¿ãƒ¼ãƒˆ
                try {
                  recognition.start()
                  console.log('Recognition started on retry without stopping')
                  isListeningRef.current = true
                  setIsListening(true)
                } catch (startError) {
                  console.error(
                    'Failed to start recognition on retry:',
                    startError
                  )
                  isListeningRef.current = false
                  setIsListening(false)
                }
              }
            }
          } catch (retryError) {
            console.error('Failed to start recognition on retry:', retryError)
            isListeningRef.current = false
            setIsListening(false)
            return
          }
        }, 300)
      }
    }
  }, [recognition, checkMicrophonePermission])

  // ----- éŸ³å£°èªè­˜ãƒˆã‚°ãƒ«å‡¦ç† -----
  const toggleListening = useCallback(() => {
    if (isListeningRef.current) {
      stopListening()
    } else {
      keyPressStartTime.current = Date.now()
      isKeyboardTriggered.current = true
      startListening()
      // AIã®ç™ºè©±ã‚’åœæ­¢
      homeStore.setState({ isSpeaking: false })
    }
  }, [startListening, stopListening])

  // ----- ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ -----
  const handleSendMessage = useCallback(() => {
    if (userMessage.trim()) {
      // AIã®ç™ºè©±ã‚’åœæ­¢
      homeStore.setState({ isSpeaking: false })
      SpeakQueue.stopAll()
      onChatProcessStart(userMessage)
      setUserMessage('')
    }
  }, [userMessage, onChatProcessStart])

  // ----- ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¥åŠ› -----
  const handleInputChange = useCallback(
    (e: React.ChangeEvent<HTMLInputElement | HTMLTextAreaElement>) => {
      setUserMessage(e.target.value)
    },
    []
  )

  // ----- éŸ³å£°èªè­˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®åˆæœŸåŒ–ã¨ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©è¨­å®š -----
  useEffect(() => {
    const SpeechRecognition =
      window.SpeechRecognition || window.webkitSpeechRecognition

    if (!SpeechRecognition) {
      console.error('Speech Recognition API is not supported in this browser')
      toastStore.getState().addToast({
        message: t('Toasts.SpeechRecognitionNotSupported'),
        type: 'error',
        tag: 'speech-recognition-not-supported',
      })
      return
    }

    const newRecognition = new SpeechRecognition()
    newRecognition.lang = getVoiceLanguageCode(selectLanguage)
    newRecognition.continuous = true
    newRecognition.interimResults = true

    // ----- ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ã®è¨­å®š -----

    // éŸ³å£°èªè­˜é–‹å§‹æ™‚
    newRecognition.onstart = () => {
      console.log('Speech recognition started')
      recognitionStartTimeRef.current = Date.now()
      speechDetectedRef.current = false

      // åˆæœŸéŸ³å£°æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼è¨­å®š
      if (initialSpeechTimeout > 0) {
        initialSpeechCheckTimerRef.current = setTimeout(() => {
          if (!speechDetectedRef.current && isListeningRef.current) {
            console.log(
              `â±ï¸ ${initialSpeechTimeout}ç§’é–“éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚éŸ³å£°èªè­˜ã‚’åœæ­¢ã—ã¾ã™ã€‚`
            )
            stopListening()

            // å¸¸æ™‚ãƒã‚¤ã‚¯å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚ªãƒ•ã«è¨­å®š
            if (settingsStore.getState().continuousMicListeningMode) {
              console.log(
                'ğŸ”‡ éŸ³å£°æœªæ¤œå‡ºã«ã‚ˆã‚Šå¸¸æ™‚ãƒã‚¤ã‚¯å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰ã‚’OFFã«è¨­å®šã—ã¾ã™ã€‚'
              )
              settingsStore.setState({ continuousMicListeningMode: false })
            }

            toastStore.getState().addToast({
              message: t('Toasts.NoSpeechDetected'),
              type: 'info',
              tag: 'no-speech-detected',
            })
          }
        }, initialSpeechTimeout * 1000)
      }

      // ç„¡éŸ³æ¤œå‡ºé–‹å§‹
      startSilenceDetection(stopListening)
    }

    // éŸ³å£°å…¥åŠ›æ¤œå‡ºæ™‚
    newRecognition.onspeechstart = () => {
      console.log('ğŸ—£ï¸ éŸ³å£°å…¥åŠ›ã‚’æ¤œå‡ºã—ã¾ã—ãŸï¼ˆonspeechstartï¼‰')
      // ã“ã“ã§ã¯ã‚¿ã‚¤ãƒãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹ã ã‘ã§ã€speechDetectedRefã¯è¨­å®šã—ãªã„
      updateSpeechTimestamp()
    }

    // éŸ³é‡ãƒ¬ãƒ™ãƒ«è¿½è·¡ç”¨å¤‰æ•°
    let lastTranscriptLength = 0

    // éŸ³å£°èªè­˜çµæœãŒå¾—ã‚‰ã‚ŒãŸã¨ã
    newRecognition.onresult = (event) => {
      if (!isListeningRef.current) return

      const transcript = Array.from(event.results)
        .map((result) => result[0].transcript)
        .join('')

      // æœ‰æ„ãªå¤‰åŒ–ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
      const isSignificantChange =
        transcript.trim().length > lastTranscriptLength
      lastTranscriptLength = transcript.trim().length

      if (isSignificantChange) {
        console.log('ğŸ¤ æœ‰æ„ãªéŸ³å£°ã‚’æ¤œå‡ºã—ã¾ã—ãŸï¼ˆãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆå¤‰æ›´ã‚ã‚Šï¼‰')
        updateSpeechTimestamp()
        speechDetectedRef.current = true
      } else {
        console.log(
          'ğŸ”‡ ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒã‚¤ã‚ºã‚’ç„¡è¦–ã—ã¾ã™ï¼ˆãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆå¤‰æ›´ãªã—ï¼‰'
        )
      }

      transcriptRef.current = transcript
      setUserMessage(transcript)
    }

    // éŸ³å£°å…¥åŠ›çµ‚äº†æ™‚
    newRecognition.onspeechend = () => {
      console.log(
        'ğŸ›‘ éŸ³å£°å…¥åŠ›ãŒçµ‚äº†ã—ã¾ã—ãŸï¼ˆonspeechendï¼‰ã€‚ç„¡éŸ³æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼ãŒå‹•ä½œä¸­ã§ã™ã€‚'
      )
    }

    // éŸ³å£°èªè­˜çµ‚äº†æ™‚
    newRecognition.onend = () => {
      console.log('Recognition ended')
      clearSilenceDetection()
      clearInitialSpeechCheckTimer()

      // isListeningRef.currentãŒtrueã®å ´åˆã¯å†é–‹
      if (isListeningRef.current) {
        console.log('Restarting speech recognition...')
        setTimeout(() => {
          startListening()
        }, 1000)
      }
    }

    // éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼æ™‚
    newRecognition.onerror = (event) => {
      console.error('Speech recognition error:', event.error)

      // no-speechã‚¨ãƒ©ãƒ¼ã®å ´åˆ
      if (event.error === 'no-speech' && isListeningRef.current) {
        // åˆå›éŸ³å£°æ¤œå‡ºã•ã‚Œã¦ã„ãªã„å ´åˆã®ã¿ã€ç´¯ç©æ™‚é–“ã‚’ãƒã‚§ãƒƒã‚¯
        if (!speechDetectedRef.current && initialSpeechTimeout > 0) {
          // èªè­˜é–‹å§‹ã‹ã‚‰ã®çµŒéæ™‚é–“ã‚’è¨ˆç®—
          const elapsedTime =
            (Date.now() - recognitionStartTimeRef.current) / 1000
          console.log(
            `éŸ³å£°æœªæ¤œå‡ºã®ç´¯ç©æ™‚é–“: ${elapsedTime.toFixed(1)}ç§’ / è¨­å®š: ${initialSpeechTimeout}ç§’`
          )

          // è¨­å®šã•ã‚ŒãŸåˆæœŸéŸ³å£°ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¶…ãˆãŸå ´åˆã¯ã€å†èµ·å‹•ã›ãšã«çµ‚äº†
          if (elapsedTime >= initialSpeechTimeout) {
            console.log(
              `â±ï¸ ${initialSpeechTimeout}ç§’é–“éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚éŸ³å£°èªè­˜ã‚’åœæ­¢ã—ã¾ã™ã€‚`
            )
            clearSilenceDetection()
            clearInitialSpeechCheckTimer()
            stopListening()

            // å¸¸æ™‚ãƒã‚¤ã‚¯å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚ªãƒ•ã«è¨­å®š
            if (settingsStore.getState().continuousMicListeningMode) {
              console.log(
                'ğŸ”‡ éŸ³å£°æœªæ¤œå‡ºã«ã‚ˆã‚Šå¸¸æ™‚ãƒã‚¤ã‚¯å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰ã‚’OFFã«è¨­å®šã—ã¾ã™ã€‚'
              )
              settingsStore.setState({ continuousMicListeningMode: false })
            }

            toastStore.getState().addToast({
              message: t('Toasts.NoSpeechDetected'),
              type: 'info',
              tag: 'no-speech-detected',
            })
            return
          }
        }

        // éŸ³å£°ãŒæ—¢ã«æ¤œå‡ºã•ã‚Œã¦ã„ã‚‹å ´åˆã€ã¾ãŸã¯åˆæœŸã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã«é”ã—ã¦ã„ãªã„å ´åˆã¯å†èµ·å‹•
        console.log(
          'No speech detected, automatically restarting recognition...'
        )

        // å°‘ã—é…å»¶ã‚’å…¥ã‚Œã¦ã‹ã‚‰å†èµ·å‹•
        setTimeout(() => {
          if (
            isListeningRef.current &&
            !homeStore.getState().chatProcessing &&
            // å¸¸æ™‚ãƒã‚¤ã‚¯ãƒ¢ãƒ¼ãƒ‰ãŒã‚ªãƒ³ã®å ´åˆã®ã¿å†èµ·å‹•
            (settingsStore.getState().continuousMicListeningMode ||
              isKeyboardTriggered.current)
          ) {
            try {
              // æ˜ç¤ºçš„ã«åœæ­¢ã—ã¦ã‹ã‚‰å†é–‹
              try {
                newRecognition.stop()
                // å°‘ã—å¾…ã£ã¦ã‹ã‚‰å†é–‹
                setTimeout(() => {
                  newRecognition.start()
                  console.log(
                    'Recognition automatically restarted after no-speech timeout'
                  )
                }, 100)
              } catch (stopError) {
                // stop()ã§ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸå ´åˆã¯ç›´æ¥start()ã‚’è©¦ã¿ã‚‹
                newRecognition.start()
                console.log(
                  'Recognition automatically restarted without stopping'
                )
              }
            } catch (restartError) {
              console.error(
                'Failed to restart recognition after no-speech:',
                restartError
              )
              isListeningRef.current = false
              setIsListening(false)
            }
          } else {
            console.log(
              'éŸ³å£°èªè­˜ã®å†èµ·å‹•ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ï¼ˆå¸¸æ™‚ãƒã‚¤ã‚¯ãƒ¢ãƒ¼ãƒ‰ãŒã‚ªãƒ•ã¾ãŸã¯ä»–ã®æ¡ä»¶ã‚’æº€ãŸã•ãªã„ï¼‰'
            )
            console.log('isListeningRef.current', isListeningRef.current)
            console.log(
              '!homeStore.getState().isSpeaking',
              !homeStore.getState().isSpeaking
            )
            console.log(
              '!homeStore.getState().chatProcessing',
              !homeStore.getState().chatProcessing
            )
          }
        }, 2000)
      } else {
        // ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯é€šå¸¸ã®çµ‚äº†å‡¦ç†
        clearSilenceDetection()
        clearInitialSpeechCheckTimer()
        stopListening()
      }
    }

    setRecognition(newRecognition)

    // ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–¢æ•°
    return () => {
      try {
        if (newRecognition) {
          newRecognition.onstart = null
          newRecognition.onspeechstart = null
          newRecognition.onresult = null
          newRecognition.onspeechend = null
          newRecognition.onend = null
          newRecognition.onerror = null
          newRecognition.abort()
        }
      } catch (error) {
        console.error('Error cleaning up speech recognition:', error)
      }
      clearSilenceDetection()
      clearInitialSpeechCheckTimer()
    }
  }, [
    selectLanguage,
    initialSpeechTimeout,
    t,
    // stopListening,
    clearSilenceDetection,
    clearInitialSpeechCheckTimer,
    startSilenceDetection,
    updateSpeechTimestamp,
  ])

  return {
    userMessage,
    isListening,
    silenceTimeoutRemaining,
    handleInputChange,
    handleSendMessage,
    toggleListening,
    startListening,
    stopListening,
  }
}

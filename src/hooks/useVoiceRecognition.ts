import { useState, useEffect, useCallback, useRef } from 'react'
import settingsStore from '@/features/stores/settings'
import webSocketStore from '@/features/stores/websocketStore'
import toastStore from '@/features/stores/toast'
import { useTranslation } from 'react-i18next'
import homeStore from '@/features/stores/home'
import { getVoiceLanguageCode } from '@/utils/voiceLanguage'
import { processAudio, base64EncodeAudio } from '@/utils/audioProcessing'
import { useSilenceDetection } from './useSilenceDetection'
import { SpeakQueue } from '@/features/messages/speakQueue'

// AudioContext ã®å‹å®šç¾©ã‚’æ‹¡å¼µ
type AudioContextType = typeof AudioContext

type UseVoiceRecognitionProps = {
  onChatProcessStart: (text: string) => void
}

export const useVoiceRecognition = ({
  onChatProcessStart,
}: UseVoiceRecognitionProps) => {
  const { t } = useTranslation()
  const selectLanguage = settingsStore((s) => s.selectLanguage)
  const realtimeAPIMode = settingsStore((s) => s.realtimeAPIMode)
  const speechRecognitionMode = settingsStore((s) => s.speechRecognitionMode)
  const continuousMicListeningMode = settingsStore(
    (s) => s.continuousMicListeningMode
  )
  const initialSpeechTimeout = settingsStore((s) => s.initialSpeechTimeout)

  // ----- çŠ¶æ…‹ç®¡ç† -----
  const [userMessage, setUserMessage] = useState('')
  const [isListening, setIsListening] = useState(false)
  const isListeningRef = useRef(false)
  const [isProcessing, setIsProcessing] = useState(false)

  // ----- éŸ³å£°èªè­˜é–¢é€£ -----
  const [recognition, setRecognition] = useState<SpeechRecognition | null>(null)
  const transcriptRef = useRef('')
  const speechDetectedRef = useRef<boolean>(false)
  const recognitionStartTimeRef = useRef<number>(0)
  const initialSpeechCheckTimerRef = useRef<NodeJS.Timeout | null>(null)

  // ----- éŸ³å£°éŒ²éŸ³é–¢é€£ (ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ APIã¨Whisperç”¨) -----
  const [audioContext, setAudioContext] = useState<AudioContext | null>(null)
  const [mediaRecorder, setMediaRecorder] = useState<MediaRecorder | null>(null)
  const audioBufferRef = useRef<Float32Array | null>(null)
  const audioChunksRef = useRef<Blob[]>([])

  // ----- ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ãƒˆãƒªã‚¬ãƒ¼é–¢é€£ -----
  const keyPressStartTime = useRef<number | null>(null)
  const isKeyboardTriggered = useRef(false)

  // ----- å‚ç…§ä¿æŒç”¨ -----
  const stopListeningRef = useRef<(() => Promise<void>) | null>(null)
  const sendAudioBufferRef = useRef<(() => void) | null>(null)

  // ----- ç„¡éŸ³æ¤œå‡ºãƒ•ãƒƒã‚¯ã‚’ä½¿ç”¨ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ãƒ¢ãƒ¼ãƒ‰ã®ã¿ï¼‰ -----
  const {
    silenceTimeoutRemaining,
    clearSilenceDetection,
    startSilenceDetection,
    updateSpeechTimestamp,
    isSpeechEnded,
  } = useSilenceDetection({
    onTextDetected: onChatProcessStart,
    transcriptRef,
    setUserMessage,
    speechDetectedRef,
  })

  // ----- éŸ³å£°åœæ­¢ -----
  const handleStopSpeaking = useCallback(() => {
    homeStore.setState({ isSpeaking: false })
  }, [])

  // ----- åˆæœŸéŸ³å£°æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹é–¢æ•° -----
  const clearInitialSpeechCheckTimer = useCallback(() => {
    if (initialSpeechCheckTimerRef.current) {
      clearTimeout(initialSpeechCheckTimerRef.current)
      initialSpeechCheckTimerRef.current = null
    }
  }, [])

  // ----- ãƒã‚¤ã‚¯æ¨©é™ç¢ºèª -----
  const checkMicrophonePermission = async (): Promise<boolean> => {
    // Firefoxã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã—ã¦çµ‚äº†
    if (
      navigator.userAgent.toLowerCase().includes('firefox') &&
      speechRecognitionMode === 'browser'
    ) {
      toastStore.getState().addToast({
        message: t('Toasts.FirefoxNotSupported'),
        type: 'error',
        tag: 'microphone-permission-error-firefox',
      })
      return false
    }

    try {
      // getUserMediaã‚’ç›´æ¥å‘¼ã³å‡ºã—ã€ãƒ–ãƒ©ã‚¦ã‚¶ã®ãƒã‚¤ãƒ†ã‚£ãƒ–è¨±å¯ãƒ¢ãƒ¼ãƒ€ãƒ«ã‚’è¡¨ç¤º
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      stream.getTracks().forEach((track) => track.stop())
      return true
    } catch (error) {
      // ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ˜ç¤ºçš„ã«æ‹’å¦ã—ãŸå ´åˆã‚„ã€ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã®å ´åˆ
      console.error('Microphone permission error:', error)
      return false
    }
  }

  // ----- Whisper APIã«éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’é€ä¿¡ã—ã¦æ–‡å­—èµ·ã“ã— -----
  const processWhisperRecognition = async (
    audioBlob: Blob
  ): Promise<string> => {
    setIsProcessing(true)

    try {
      // é©åˆ‡ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã«æ–°ã—ã„Blobã‚’ä½œæˆ
      // OpenAI Whisper APIã¯ç‰¹å®šã®å½¢å¼ã®ã¿ã‚’ã‚µãƒãƒ¼ãƒˆ
      const formData = new FormData()

      // ãƒ•ã‚¡ã‚¤ãƒ«åã¨MIMEã‚¿ã‚¤ãƒ—ã‚’æ±ºå®š
      let fileExtension = 'webm'
      let mimeType = audioBlob.type

      // MIMEã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ã¦æ‹¡å¼µå­ã‚’è¨­å®š
      if (mimeType.includes('mp3')) {
        fileExtension = 'mp3'
      } else if (mimeType.includes('ogg')) {
        fileExtension = 'ogg'
      } else if (mimeType.includes('wav')) {
        fileExtension = 'wav'
      } else if (mimeType.includes('mp4')) {
        fileExtension = 'mp4'
      }

      // ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
      const fileName = `audio.${fileExtension}`

      // FormDataã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ 
      formData.append('file', audioBlob, fileName)

      // è¨€èªè¨­å®šã®è¿½åŠ 
      if (selectLanguage) {
        formData.append('language', selectLanguage)
      }

      // OpenAI APIã‚­ãƒ¼ã‚’è¿½åŠ 
      const openaiKey = settingsStore.getState().openaiKey
      if (openaiKey) {
        formData.append('openaiKey', openaiKey)
      }

      // Whisperãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ 
      const whisperModel = settingsStore.getState().whisperTranscriptionModel
      formData.append('model', whisperModel)

      console.log(
        `Sending audio to Whisper API - size: ${audioBlob.size} bytes, type: ${mimeType}, filename: ${fileName}, model: ${whisperModel}`
      )

      // APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
      const response = await fetch('/api/whisper', {
        method: 'POST',
        body: formData,
      })

      if (!response.ok) {
        const errorData = await response.json()
        throw new Error(
          `Whisper API error: ${response.status} - ${errorData.details || errorData.error || 'Unknown error'}`
        )
      }

      const result = await response.json()
      return result.text || ''
    } catch (error) {
      console.error('Whisper transcription error:', error)
      toastStore.getState().addToast({
        message: t('Toasts.WhisperError'),
        type: 'error',
        tag: 'whisper-error',
      })
      return ''
    } finally {
      setIsProcessing(false)
    }
  }

  // ----- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ APIç”¨ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿é€ä¿¡ -----
  const sendAudioBuffer = useCallback(() => {
    if (!audioBufferRef.current || audioBufferRef.current.length === 0) {
      console.error('éŸ³å£°ãƒãƒƒãƒ•ã‚¡ãŒç©ºã§ã™')
      return
    }

    const base64Chunk = base64EncodeAudio(audioBufferRef.current)
    const ss = settingsStore.getState()
    const wsManager = webSocketStore.getState().wsManager

    if (wsManager?.websocket?.readyState !== WebSocket.OPEN) {
      return
    }

    let sendContent: { type: string; text?: string; audio?: string }[] = []

    if (ss.realtimeAPIModeContentType === 'input_audio') {
      console.log('Sending buffer. Length:', audioBufferRef.current.length)
      sendContent = [{ type: 'input_audio', audio: base64Chunk }]
    } else {
      const currentText = transcriptRef.current.trim()
      if (currentText) {
        console.log('Sending text. userMessage:', currentText)
        sendContent = [{ type: 'input_text', text: currentText }]
      }
    }

    if (sendContent.length > 0) {
      wsManager.websocket.send(
        JSON.stringify({
          type: 'conversation.item.create',
          item: {
            type: 'message',
            role: 'user',
            content: sendContent,
          },
        })
      )
      wsManager.websocket.send(
        JSON.stringify({
          type: 'response.create',
        })
      )
    }

    audioBufferRef.current = null // é€ä¿¡å¾Œã«ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢
  }, [])

  // ----- éŸ³å£°èªè­˜åœæ­¢å‡¦ç† -----
  const stopListeningImpl = useCallback(async () => {
    // å„ç¨®ã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ãƒ¢ãƒ¼ãƒ‰ã®ã¿ï¼‰
    if (speechRecognitionMode === 'browser') {
      clearSilenceDetection()
      clearInitialSpeechCheckTimer()
    }

    // ãƒªã‚¹ãƒ‹ãƒ³ã‚°çŠ¶æ…‹ã‚’æ›´æ–°
    isListeningRef.current = false
    setIsListening(false)

    if (speechRecognitionMode === 'browser') {
      // ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ
      if (!recognition) return

      // éŸ³å£°èªè­˜ã‚’åœæ­¢
      recognition.stop()

      // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ APIãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã®å‡¦ç†
      if (realtimeAPIMode && mediaRecorder) {
        mediaRecorder.stop()
        mediaRecorder.ondataavailable = null

        // éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã¨é€ä¿¡
        await new Promise<void>((resolve) => {
          mediaRecorder.onstop = async () => {
            console.log('stop MediaRecorder')
            if (audioChunksRef.current.length > 0) {
              const audioBlob = new Blob(audioChunksRef.current, {
                type: 'audio/webm',
              })
              const arrayBuffer = await audioBlob.arrayBuffer()

              if (audioContext) {
                const audioBuffer =
                  await audioContext.decodeAudioData(arrayBuffer)
                const processedData = processAudio(audioBuffer)
                audioBufferRef.current = processedData
              }
              resolve()
            } else {
              console.error('éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ãŒç©ºã§ã™')
              resolve()
            }
          }
        })

        // éŸ³å£°ãƒ‡ãƒ¼ã‚¿é€ä¿¡
        if (sendAudioBufferRef.current) {
          sendAudioBufferRef.current()
        }
      }

      // ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ãƒˆãƒªã‚¬ãƒ¼ã®å ´åˆã®å‡¦ç†
      const trimmedTranscriptRef = transcriptRef.current.trim()
      if (isKeyboardTriggered.current) {
        const pressDuration = Date.now() - (keyPressStartTime.current || 0)
        // æŠ¼ã—ã¦ã‹ã‚‰1ç§’ä»¥ä¸Š ã‹ã¤ æ–‡å­—ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿é€ä¿¡
        // ç„¡éŸ³æ¤œå‡ºã«ã‚ˆã‚‹è‡ªå‹•é€ä¿¡ãŒæ—¢ã«è¡Œã‚ã‚Œã¦ã„ãªã„å ´åˆã®ã¿é€ä¿¡ã™ã‚‹
        if (pressDuration >= 1000 && trimmedTranscriptRef && !isSpeechEnded()) {
          onChatProcessStart(trimmedTranscriptRef)
          setUserMessage('')
        }
        isKeyboardTriggered.current = false
      }
    } else {
      // Whisperãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ
      if (!mediaRecorder) return

      // éŒ²éŸ³åœæ­¢
      mediaRecorder.stop()

      try {
        // éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¦æ–‡å­—èµ·ã“ã—
        if (audioChunksRef.current.length > 0) {
          console.log(
            `Processing ${audioChunksRef.current.length} audio chunks for Whisper`
          )

          // ä¿å­˜ã•ã‚ŒãŸMIMEã‚¿ã‚¤ãƒ—ã‚’å–å¾—
          let blobType = 'audio/webm'
          if (mediaRecorder.mimeType && mediaRecorder.mimeType !== '') {
            blobType = mediaRecorder.mimeType
          }

          console.log(`Creating blob with MIME type: ${blobType}`)

          // éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒãƒ¼ã‚¸ã—ã¦Blobã«å¤‰æ›
          const audioBlob = new Blob(audioChunksRef.current, {
            type: blobType, // MediaRecorderã§ä½¿ç”¨ã•ã‚ŒãŸMIMEã‚¿ã‚¤ãƒ—ã‚’ä½¿ç”¨
          })

          console.log(
            'Created audio blob:',
            audioBlob.size,
            'bytes,',
            audioBlob.type
          )

          // Whisper APIã«é€ä¿¡
          const transcript = await processWhisperRecognition(audioBlob)

          if (transcript.trim()) {
            console.log('Whisper transcription result:', transcript)

            // æ–‡å­—èµ·ã“ã—çµæœã‚’ã‚»ãƒƒãƒˆ
            transcriptRef.current = transcript

            // LLMã«é€ä¿¡
            onChatProcessStart(transcript)
          } else {
            console.log('Whisper returned empty transcription')
            toastStore.getState().addToast({
              message: t('Toasts.NoSpeechDetected'),
              type: 'info',
              tag: 'no-speech-detected',
            })
          }
        } else {
          console.warn('No audio chunks recorded')
          toastStore.getState().addToast({
            message: t('Toasts.NoSpeechDetected'),
            type: 'info',
            tag: 'no-speech-detected',
          })
        }
      } catch (error) {
        console.error('Error processing Whisper audio:', error)
        toastStore.getState().addToast({
          message: t('Toasts.WhisperError'),
          type: 'error',
          tag: 'whisper-error',
        })
      } finally {
        // ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾
        if (mediaRecorder.stream) {
          mediaRecorder.stream.getTracks().forEach((track) => track.stop())
        }
        setMediaRecorder(null)
        audioChunksRef.current = []
      }
    }
  }, [
    speechRecognitionMode,
    recognition,
    realtimeAPIMode,
    mediaRecorder,
    audioContext,
    onChatProcessStart,
    clearInitialSpeechCheckTimer,
    clearSilenceDetection,
    isSpeechEnded,
    processWhisperRecognition,
  ])

  // ----- éŸ³å£°èªè­˜é–‹å§‹å‡¦ç† -----
  const startListening = useCallback(async () => {
    const hasPermission = await checkMicrophonePermission()
    if (!hasPermission) return

    if (speechRecognitionMode === 'browser') {
      // ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ
      if (!recognition || isListeningRef.current || !audioContext) return

      // ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
      transcriptRef.current = ''
      setUserMessage('')

      try {
        recognition.start()
      } catch (error) {
        console.error('Error starting recognition:', error)
        return
      }

      // ãƒªã‚¹ãƒ‹ãƒ³ã‚°çŠ¶æ…‹ã‚’æ›´æ–°
      isListeningRef.current = true
      setIsListening(true)

      // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ APIãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã®éŒ²éŸ³é–‹å§‹
      if (realtimeAPIMode) {
        audioChunksRef.current = [] // éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒªã‚»ãƒƒãƒˆ

        try {
          const stream = await navigator.mediaDevices.getUserMedia({
            audio: true,
          })
          const recorder = new MediaRecorder(stream, { mimeType: 'audio/webm' })
          setMediaRecorder(recorder)

          recorder.ondataavailable = (event) => {
            if (event.data.size > 0) {
              if (!isListeningRef.current) {
                recognition.stop()
                recorder.stop()
                recorder.ondataavailable = null
                return
              }
              audioChunksRef.current.push(event.data)
              console.log('add audio chunk:', audioChunksRef.current.length)
            }
          }

          recorder.start(100) // ã‚ˆã‚Šå°ã•ãªé–“éš”ã§ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
        } catch (error) {
          console.error('Error starting media recorder:', error)
          // éŒ²éŸ³ã«å¤±æ•—ã—ã¦ã‚‚éŸ³å£°èªè­˜ã¯ç¶šè¡Œã™ã‚‹
        }
      }
    } else {
      // Whisperãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ
      if (isListeningRef.current) return

      // ãƒªã‚¹ãƒ‹ãƒ³ã‚°çŠ¶æ…‹ã‚’æ›´æ–°
      isListeningRef.current = true
      setIsListening(true)

      // ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
      transcriptRef.current = ''
      setUserMessage('')

      // éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒªã‚»ãƒƒãƒˆ
      audioChunksRef.current = []

      try {
        // MediaRecorderã§ã®éŒ²éŸ³ã‚’é–‹å§‹
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            channelCount: 1,
            sampleRate: 16000, // Whisperã«æœ€é©åŒ–
            echoCancellation: true,
            noiseSuppression: true,
          },
        })

        // MediaRecorderã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹mimeTypeã‚’ç¢ºèª
        const mimeTypes = [
          'audio/mp3',
          'audio/mp4',
          'audio/mpeg',
          'audio/ogg',
          'audio/wav',
          'audio/webm',
          'audio/webm;codecs=opus',
        ]

        let selectedMimeType = 'audio/webm'
        for (const type of mimeTypes) {
          if (MediaRecorder.isTypeSupported(type)) {
            selectedMimeType = type
            // mp3ã¨oggã‚’å„ªå…ˆ
            if (type === 'audio/mp3' || type === 'audio/ogg') {
              break
            }
          }
        }

        console.log(
          `Using MIME type: ${selectedMimeType} for Whisper recording`
        )

        // OpenAI WhisperãŒå¯¾å¿œã—ã¦ã„ã‚‹å½¢å¼ã§éŒ²éŸ³
        const recorder = new MediaRecorder(stream, {
          mimeType: selectedMimeType,
          audioBitsPerSecond: 128000, // éŸ³è³ªè¨­å®š
        })

        recorder.ondataavailable = (event) => {
          if (event.data.size > 0) {
            audioChunksRef.current.push(event.data)
            console.log(
              'Whisper recording: added chunk, size:',
              event.data.size,
              'type:',
              event.data.type
            )
          }
        }

        setMediaRecorder(recorder)
        recorder.start(100) // 100msã”ã¨ã«ãƒ‡ãƒ¼ã‚¿åé›†
      } catch (error) {
        console.error('Error starting Whisper recording:', error)
        isListeningRef.current = false
        setIsListening(false)

        toastStore.getState().addToast({
          message: t('Toasts.SpeechRecognitionError'),
          type: 'error',
          tag: 'speech-recognition-error',
        })
      }
    }
  }, [
    speechRecognitionMode,
    recognition,
    audioContext,
    realtimeAPIMode,
    checkMicrophonePermission,
  ])

  // AIã®ç™ºè©±å®Œäº†å¾Œã«éŸ³å£°èªè­˜ã‚’è‡ªå‹•çš„ã«å†é–‹ã™ã‚‹å‡¦ç†
  const handleSpeakCompletion = useCallback(() => {
    // å¸¸æ™‚ãƒã‚¤ã‚¯å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰ãŒONã§ã€ç¾åœ¨ãƒã‚¤ã‚¯å…¥åŠ›ãŒè¡Œã‚ã‚Œã¦ã„ãªã„å ´åˆã®ã¿å®Ÿè¡Œ
    if (
      continuousMicListeningMode &&
      !isListeningRef.current &&
      speechRecognitionMode === 'browser' &&
      !homeStore.getState().chatProcessing
    ) {
      console.log('ğŸ”„ AIã®ç™ºè©±ãŒå®Œäº†ã—ã¾ã—ãŸã€‚éŸ³å£°èªè­˜ã‚’è‡ªå‹•çš„ã«å†é–‹ã—ã¾ã™ã€‚')
      setTimeout(() => {
        startListening()
      }, 300) // ãƒã‚¤ã‚¯èµ·å‹•ã¾ã§ã«å°‘ã—é…å»¶ã‚’å…¥ã‚Œã‚‹
    }
  }, [continuousMicListeningMode, speechRecognitionMode, startListening])

  // å¸¸æ™‚ãƒã‚¤ã‚¯å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰ã®å¤‰æ›´ã‚’ç›£è¦–
  useEffect(() => {
    if (
      continuousMicListeningMode &&
      !isListeningRef.current &&
      speechRecognitionMode === 'browser' &&
      !homeStore.getState().isSpeaking &&
      !homeStore.getState().chatProcessing
    ) {
      // å¸¸æ™‚ãƒã‚¤ã‚¯å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰ãŒONã«ãªã£ãŸå ´åˆã€è‡ªå‹•çš„ã«ãƒã‚¤ã‚¯å…¥åŠ›ã‚’é–‹å§‹
      console.log(
        'ğŸ¤ å¸¸æ™‚ãƒã‚¤ã‚¯å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰ãŒONã«ãªã‚Šã¾ã—ãŸã€‚éŸ³å£°èªè­˜ã‚’é–‹å§‹ã—ã¾ã™ã€‚'
      )
      startListening()
    }
  }, [continuousMicListeningMode, speechRecognitionMode, startListening])

  // ç™ºè©±å®Œäº†æ™‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ç™»éŒ²
  useEffect(() => {
    // ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¢ãƒ¼ãƒ‰ã§ã®ã¿ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ç™»éŒ²
    if (speechRecognitionMode === 'browser') {
      SpeakQueue.onSpeakCompletion(handleSpeakCompletion)

      return () => {
        // ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚¢ãƒ³ãƒã‚¦ãƒ³ãƒˆæ™‚ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å‰Šé™¤
        SpeakQueue.removeSpeakCompletionCallback(handleSpeakCompletion)
      }
    }
  }, [speechRecognitionMode, handleSpeakCompletion])

  // ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ãƒã‚¦ãƒ³ãƒˆæ™‚ã«å¸¸æ™‚ãƒã‚¤ã‚¯å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰ãŒONã®å ´åˆã¯è‡ªå‹•çš„ã«ãƒã‚¤ã‚¯å…¥åŠ›ã‚’é–‹å§‹
  useEffect(() => {
    if (
      continuousMicListeningMode &&
      speechRecognitionMode === 'browser' &&
      !isListeningRef.current &&
      !homeStore.getState().isSpeaking &&
      !homeStore.getState().chatProcessing
    ) {
      const delayedStart = async () => {
        console.log('ğŸ¤ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒã‚¦ãƒ³ãƒˆæ™‚ã«éŸ³å£°èªè­˜ã‚’è‡ªå‹•çš„ã«é–‹å§‹ã—ã¾ã™')
        // ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒã‚¦ãƒ³ãƒˆæ™‚ã«å°‘ã—é…å»¶ã•ã›ã¦ã‹ã‚‰é–‹å§‹
        await new Promise((resolve) => setTimeout(resolve, 1000))
        if (
          continuousMicListeningMode &&
          !isListeningRef.current &&
          !homeStore.getState().isSpeaking &&
          !homeStore.getState().chatProcessing
        ) {
          startListening()
        }
      }

      delayedStart()
    }

    return () => {
      // ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚¢ãƒ³ãƒã‚¦ãƒ³ãƒˆæ™‚ã«ãƒã‚¤ã‚¯å…¥åŠ›ã‚’åœæ­¢
      if (isListeningRef.current) {
        stopListeningRef.current?.()
      }
    }
  }, []) // ãƒã‚¦ãƒ³ãƒˆæ™‚ã®ã¿å®Ÿè¡Œ

  // ----- éŸ³å£°èªè­˜ãƒˆã‚°ãƒ«å‡¦ç† -----
  const toggleListening = useCallback(() => {
    if (isListeningRef.current) {
      stopListeningRef.current?.()
    } else {
      keyPressStartTime.current = Date.now()
      isKeyboardTriggered.current = true
      startListening()
      handleStopSpeaking()
    }
  }, [startListening, handleStopSpeaking])

  // ----- ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ -----
  const handleSendMessage = useCallback(() => {
    if (userMessage.trim()) {
      handleStopSpeaking()
      onChatProcessStart(userMessage)
      setUserMessage('')
    }
  }, [userMessage, onChatProcessStart, handleStopSpeaking])

  // ----- ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¥åŠ› -----
  const handleInputChange = useCallback(
    (e: React.ChangeEvent<HTMLInputElement | HTMLTextAreaElement>) => {
      setUserMessage(e.target.value)
    },
    []
  )

  // ----- ç°¡æ˜“ç‰ˆã®åœæ­¢é–¢æ•° -----
  const stopListening = useCallback(async () => {
    if (stopListeningRef.current) {
      await stopListeningRef.current()
    }
  }, [])

  // ----- å‰¯ä½œç”¨å‡¦ç† -----

  // stopListeningRefã®æ›´æ–°
  useEffect(() => {
    stopListeningRef.current = stopListeningImpl
  }, [stopListeningImpl])

  // sendAudioBufferRefã®æ›´æ–°
  useEffect(() => {
    sendAudioBufferRef.current = sendAudioBuffer
  }, [sendAudioBuffer])

  // AudioContextã®åˆæœŸåŒ–
  useEffect(() => {
    const AudioContextClass = (window.AudioContext ||
      (window as any).webkitAudioContext) as AudioContextType
    const context = new AudioContextClass()
    setAudioContext(context)
  }, [])

  // éŸ³å£°èªè­˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®åˆæœŸåŒ–ã¨ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©è¨­å®šï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ãƒ¢ãƒ¼ãƒ‰ã®ã¿ï¼‰
  useEffect(() => {
    // Whisperãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯åˆæœŸåŒ–ã—ãªã„
    if (speechRecognitionMode !== 'browser') return

    const SpeechRecognition =
      window.SpeechRecognition || window.webkitSpeechRecognition

    if (!SpeechRecognition) return

    const newRecognition = new SpeechRecognition()
    newRecognition.lang = getVoiceLanguageCode(selectLanguage)
    newRecognition.continuous = true
    newRecognition.interimResults = true

    // ----- ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ã®è¨­å®š -----

    // éŸ³å£°èªè­˜é–‹å§‹æ™‚
    newRecognition.onstart = () => {
      console.log('Speech recognition started')
      recognitionStartTimeRef.current = Date.now()
      speechDetectedRef.current = false

      // åˆæœŸéŸ³å£°æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼è¨­å®š
      if (initialSpeechTimeout > 0) {
        initialSpeechCheckTimerRef.current = setTimeout(() => {
          if (!speechDetectedRef.current && isListeningRef.current) {
            console.log(
              `â±ï¸ ${initialSpeechTimeout}ç§’é–“éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚éŸ³å£°èªè­˜ã‚’åœæ­¢ã—ã¾ã™ã€‚`
            )
            stopListening()

            toastStore.getState().addToast({
              message: t('Toasts.NoSpeechDetected'),
              type: 'info',
              tag: 'no-speech-detected',
            })
          }
        }, initialSpeechTimeout * 1000)
      }

      // ç„¡éŸ³æ¤œå‡ºé–‹å§‹
      if (stopListeningRef.current) {
        startSilenceDetection(stopListeningRef.current)
      }
    }

    // éŸ³å£°å…¥åŠ›æ¤œå‡ºæ™‚
    newRecognition.onspeechstart = () => {
      console.log('ğŸ—£ï¸ éŸ³å£°å…¥åŠ›ã‚’æ¤œå‡ºã—ã¾ã—ãŸï¼ˆonspeechstartï¼‰')
      speechDetectedRef.current = true
      updateSpeechTimestamp()
    }

    // éŸ³é‡ãƒ¬ãƒ™ãƒ«è¿½è·¡ç”¨å¤‰æ•°
    let lastTranscriptLength = 0

    // éŸ³å£°èªè­˜çµæœãŒå¾—ã‚‰ã‚ŒãŸã¨ã
    newRecognition.onresult = (event) => {
      if (!isListeningRef.current) return

      const transcript = Array.from(event.results)
        .map((result) => result[0].transcript)
        .join('')

      // æœ‰æ„ãªå¤‰åŒ–ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
      const isSignificantChange =
        transcript.trim().length > lastTranscriptLength
      lastTranscriptLength = transcript.trim().length

      if (isSignificantChange) {
        console.log('ğŸ“¢ æœ‰æ„ãªéŸ³å£°ã‚’æ¤œå‡ºã—ã¾ã—ãŸï¼ˆãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆå¤‰æ›´ã‚ã‚Šï¼‰')
        updateSpeechTimestamp()
        speechDetectedRef.current = true
      } else {
        console.log(
          'ğŸ”‡ ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒã‚¤ã‚ºã‚’ç„¡è¦–ã—ã¾ã™ï¼ˆãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆå¤‰æ›´ãªã—ï¼‰'
        )
      }

      transcriptRef.current = transcript
      setUserMessage(transcript)
    }

    // éŸ³å£°å…¥åŠ›çµ‚äº†æ™‚
    newRecognition.onspeechend = () => {
      console.log(
        'ğŸ›‘ éŸ³å£°å…¥åŠ›ãŒçµ‚äº†ã—ã¾ã—ãŸï¼ˆonspeechendï¼‰ã€‚ç„¡éŸ³æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼ãŒå‹•ä½œä¸­ã§ã™ã€‚'
      )
    }

    // éŸ³å£°èªè­˜çµ‚äº†æ™‚
    newRecognition.onend = () => {
      console.log('Recognition ended')
      clearSilenceDetection()
      clearInitialSpeechCheckTimer()
    }

    // éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼æ™‚
    newRecognition.onerror = (event) => {
      console.error('Speech recognition error:', event.error)
      clearSilenceDetection()
      clearInitialSpeechCheckTimer()
      stopListening()
    }

    setRecognition(newRecognition)
  }, [
    speechRecognitionMode,
    selectLanguage,
    t,
    stopListening,
    clearInitialSpeechCheckTimer,
    clearSilenceDetection,
    startSilenceDetection,
    updateSpeechTimestamp,
  ])

  // ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã®è¨­å®š
  useEffect(() => {
    const handleKeyDown = async (e: KeyboardEvent) => {
      if (e.key === 'Alt' && !isListeningRef.current) {
        // Alt ã‚­ãƒ¼ã‚’æŠ¼ã—ãŸæ™‚ã®å‡¦ç†
        keyPressStartTime.current = Date.now()
        isKeyboardTriggered.current = true
        handleStopSpeaking()
        await startListening()
      }
    }

    const handleKeyUp = (e: KeyboardEvent) => {
      if (e.key === 'Alt' && isListeningRef.current) {
        // Alt ã‚­ãƒ¼ã‚’é›¢ã—ãŸæ™‚ã®å‡¦ç†
        stopListening()
      }
    }

    window.addEventListener('keydown', handleKeyDown)
    window.addEventListener('keyup', handleKeyUp)

    return () => {
      window.removeEventListener('keydown', handleKeyDown)
      window.removeEventListener('keyup', handleKeyUp)
    }
  }, [startListening, stopListening, handleStopSpeaking])

  // å…¬é–‹ã™ã‚‹API
  return {
    userMessage,
    isListening,
    isProcessing,
    silenceTimeoutRemaining:
      speechRecognitionMode === 'browser' ? silenceTimeoutRemaining : null,
    handleInputChange,
    handleSendMessage,
    toggleListening,
    handleStopSpeaking,
    startListening,
    stopListening,
  }
}

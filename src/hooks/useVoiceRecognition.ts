import { useState, useEffect, useCallback, useRef } from 'react'
import settingsStore from '@/features/stores/settings'
import webSocketStore from '@/features/stores/websocketStore'
import toastStore from '@/features/stores/toast'
import { useTranslation } from 'react-i18next'
import homeStore from '@/features/stores/home'
import { getVoiceLanguageCode } from '@/utils/voiceLanguage'
import { processAudio, base64EncodeAudio } from '@/utils/audioProcessing'
import { useSilenceDetection } from './useSilenceDetection'

// AudioContext ã®å‹å®šç¾©ã‚’æ‹¡å¼µ
type AudioContextType = typeof AudioContext

// éŸ³å£°èªè­˜é–‹å§‹å¾Œã€éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œãªã„ã¾ã¾çµŒéã—ãŸå ´åˆã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ5ç§’ï¼‰
const INITIAL_SPEECH_TIMEOUT = 5000

type UseVoiceRecognitionProps = {
  onChatProcessStart: (text: string) => void
}

export const useVoiceRecognition = ({
  onChatProcessStart,
}: UseVoiceRecognitionProps) => {
  const realtimeAPIMode = settingsStore((s) => s.realtimeAPIMode)
  const [userMessage, setUserMessage] = useState('')
  const [recognition, setRecognition] = useState<SpeechRecognition | null>(null)
  const [audioContext, setAudioContext] = useState<AudioContext | null>(null)
  const [mediaRecorder, setMediaRecorder] = useState<MediaRecorder | null>(null)
  const keyPressStartTime = useRef<number | null>(null)
  const transcriptRef = useRef('')
  const isKeyboardTriggered = useRef(false)
  const audioBufferRef = useRef<Float32Array | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const isListeningRef = useRef(false)
  const [isListening, setIsListening] = useState(false)
  // éŸ³å£°èªè­˜é–‹å§‹æ™‚åˆ»ã‚’ä¿æŒã™ã‚‹å¤‰æ•°ã‚’è¿½åŠ 
  const recognitionStartTimeRef = useRef<number>(0)
  // éŸ³å£°ãŒæ¤œå‡ºã•ã‚ŒãŸã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°
  const speechDetectedRef = useRef<boolean>(false)
  // åˆæœŸéŸ³å£°æ¤œå‡ºç”¨ã®ã‚¿ã‚¤ãƒãƒ¼
  const initialSpeechCheckTimerRef = useRef<NodeJS.Timeout | null>(null)
  const selectLanguage = settingsStore((s) => s.selectLanguage)

  const { t } = useTranslation()

  const stopListeningRef = useRef<(() => Promise<void>) | null>(null)
  const sendAudioBufferRef = useRef<(() => void) | null>(null)

  // ç„¡éŸ³æ¤œå‡ºãƒ•ãƒƒã‚¯ã‚’ä½¿ç”¨
  const {
    silenceTimeoutRemaining,
    clearSilenceDetection,
    startSilenceDetection,
    updateSpeechTimestamp,
    isSpeechEnded,
  } = useSilenceDetection({
    onTextDetected: onChatProcessStart,
    transcriptRef,
    setUserMessage,
    speechDetectedRef,
  })

  // éŸ³å£°åœæ­¢
  const handleStopSpeaking = useCallback(() => {
    homeStore.setState({ isSpeaking: false })
  }, [])

  // åˆæœŸéŸ³å£°æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹é–¢æ•°
  const clearInitialSpeechCheckTimer = useCallback(() => {
    if (initialSpeechCheckTimerRef.current) {
      clearTimeout(initialSpeechCheckTimerRef.current)
      initialSpeechCheckTimerRef.current = null
    }
  }, [])

  const checkMicrophonePermission = async (): Promise<boolean> => {
    // Firefoxã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã—ã¦çµ‚äº†
    if (navigator.userAgent.toLowerCase().includes('firefox')) {
      toastStore.getState().addToast({
        message: t('Toasts.FirefoxNotSupported'),
        type: 'error',
        tag: 'microphone-permission-error-firefox',
      })
      return false
    }

    try {
      // getUserMediaã‚’ç›´æ¥å‘¼ã³å‡ºã—ã€ãƒ–ãƒ©ã‚¦ã‚¶ã®ãƒã‚¤ãƒ†ã‚£ãƒ–è¨±å¯ãƒ¢ãƒ¼ãƒ€ãƒ«ã‚’è¡¨ç¤º
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      stream.getTracks().forEach((track) => track.stop())
      return true
    } catch (error) {
      // ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ˜ç¤ºçš„ã«æ‹’å¦ã—ãŸå ´åˆã‚„ã€ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã®å ´åˆ
      console.error('Microphone permission error:', error)
      return false
    }
  }

  // stopListeningé–¢æ•°ã®å…ˆè¡Œå®£è¨€ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã¯ä¸‹éƒ¨ã§è¡Œã†ï¼‰
  const stopListening = useCallback(async () => {
    if (stopListeningRef.current) {
      await stopListeningRef.current()
    }
  }, [])

  // sendAudioBufferé–¢æ•°ã‚’ã“ã“ã«ç§»å‹•
  const sendAudioBuffer = useCallback(() => {
    if (audioBufferRef.current && audioBufferRef.current.length > 0) {
      const base64Chunk = base64EncodeAudio(audioBufferRef.current)
      const ss = settingsStore.getState()
      const wsManager = webSocketStore.getState().wsManager
      if (wsManager?.websocket?.readyState === WebSocket.OPEN) {
        let sendContent: { type: string; text?: string; audio?: string }[] = []

        if (ss.realtimeAPIModeContentType === 'input_audio') {
          console.log('Sending buffer. Length:', audioBufferRef.current.length)
          sendContent = [
            {
              type: 'input_audio',
              audio: base64Chunk,
            },
          ]
        } else {
          const currentText = transcriptRef.current.trim()
          console.log('Sending text. userMessage:', currentText)
          if (currentText) {
            sendContent = [
              {
                type: 'input_text',
                text: currentText,
              },
            ]
          }
        }

        if (sendContent.length > 0) {
          wsManager.websocket.send(
            JSON.stringify({
              type: 'conversation.item.create',
              item: {
                type: 'message',
                role: 'user',
                content: sendContent,
              },
            })
          )
          wsManager.websocket.send(
            JSON.stringify({
              type: 'response.create',
            })
          )
        }
      }
      audioBufferRef.current = null // é€ä¿¡å¾Œã«ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢
    } else {
      console.error('éŸ³å£°ãƒãƒƒãƒ•ã‚¡ãŒç©ºã§ã™')
    }
  }, [])

  // sendAudioBufferã‚’Refã«ä¿å­˜
  useEffect(() => {
    sendAudioBufferRef.current = sendAudioBuffer
  }, [sendAudioBuffer])

  // ã“ã“ã§æœ€çµ‚çš„ãªstopListeningå®Ÿè£…ã‚’è¡Œã†
  const stopListeningImpl = useCallback(async () => {
    // ç„¡éŸ³æ¤œå‡ºã‚’ã‚¯ãƒªã‚¢
    clearSilenceDetection()

    // åˆæœŸéŸ³å£°æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢
    clearInitialSpeechCheckTimer()

    isListeningRef.current = false
    setIsListening(false)
    if (recognition) {
      recognition.stop()

      if (realtimeAPIMode) {
        if (mediaRecorder) {
          mediaRecorder.stop()
          mediaRecorder.ondataavailable = null
          await new Promise<void>((resolve) => {
            mediaRecorder.onstop = async () => {
              console.log('stop MediaRecorder')
              if (audioChunksRef.current.length > 0) {
                const audioBlob = new Blob(audioChunksRef.current, {
                  type: 'audio/webm',
                })
                const arrayBuffer = await audioBlob.arrayBuffer()
                const audioBuffer =
                  await audioContext!.decodeAudioData(arrayBuffer)
                const processedData = processAudio(audioBuffer)

                audioBufferRef.current = processedData
                resolve()
              } else {
                console.error('éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ãŒç©ºã§ã™')
                resolve()
              }
            }
          })
        }
        // sendAudioBufferã®ä»£ã‚ã‚Šã«sendAudioBufferRef.currentã‚’ä½¿ç”¨
        if (sendAudioBufferRef.current) {
          sendAudioBufferRef.current()
        }
      }

      const trimmedTranscriptRef = transcriptRef.current.trim()
      if (isKeyboardTriggered.current) {
        const pressDuration = Date.now() - (keyPressStartTime.current || 0)
        // æŠ¼ã—ã¦ã‹ã‚‰1ç§’ä»¥ä¸Š ã‹ã¤ æ–‡å­—ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿é€ä¿¡
        // ç„¡éŸ³æ¤œå‡ºã«ã‚ˆã‚‹è‡ªå‹•é€ä¿¡ãŒæ—¢ã«è¡Œã‚ã‚Œã¦ã„ãªã„å ´åˆã®ã¿é€ä¿¡ã™ã‚‹
        if (pressDuration >= 1000 && trimmedTranscriptRef && !isSpeechEnded()) {
          onChatProcessStart(trimmedTranscriptRef)
          setUserMessage('')
        }
        isKeyboardTriggered.current = false
      }
    }
  }, [
    recognition,
    realtimeAPIMode,
    mediaRecorder,
    audioContext,
    onChatProcessStart,
    clearInitialSpeechCheckTimer,
    clearSilenceDetection,
    isSpeechEnded,
  ])

  // stopListeningã®å®Ÿè£…ã‚’ä¸Šæ›¸ã
  useEffect(() => {
    stopListeningRef.current = stopListeningImpl
  }, [stopListeningImpl])

  useEffect(() => {
    const SpeechRecognition =
      window.SpeechRecognition || window.webkitSpeechRecognition
    if (SpeechRecognition) {
      const newRecognition = new SpeechRecognition()
      newRecognition.lang = getVoiceLanguageCode(selectLanguage)
      newRecognition.continuous = true
      newRecognition.interimResults = true

      // éŸ³å£°èªè­˜é–‹å§‹æ™‚ã®ãƒãƒ³ãƒ‰ãƒ©ã‚’è¿½åŠ 
      newRecognition.onstart = () => {
        console.log('Speech recognition started')
        // éŸ³å£°èªè­˜é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²
        recognitionStartTimeRef.current = Date.now()
        // éŸ³å£°æ¤œå‡ºãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
        speechDetectedRef.current = false

        // 5ç§’å¾Œã«éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã‚¿ã‚¤ãƒãƒ¼ã‚’è¨­å®š
        initialSpeechCheckTimerRef.current = setTimeout(() => {
          // éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œã¦ã„ãªã„å ´åˆã¯éŸ³å£°èªè­˜ã‚’åœæ­¢
          if (!speechDetectedRef.current && isListeningRef.current) {
            console.log(
              'â±ï¸ 5ç§’é–“éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚éŸ³å£°èªè­˜ã‚’åœæ­¢ã—ã¾ã™ã€‚'
            )
            stopListening()

            // å¿…è¦ã«å¿œã˜ã¦ãƒˆãƒ¼ã‚¹ãƒˆé€šçŸ¥ã‚’è¡¨ç¤º
            toastStore.getState().addToast({
              message: t('Toasts.NoSpeechDetected'),
              type: 'info',
              tag: 'no-speech-detected',
            })
          }
        }, INITIAL_SPEECH_TIMEOUT)

        // ç„¡éŸ³æ¤œå‡ºã‚’é–‹å§‹
        if (stopListeningRef.current) {
          startSilenceDetection(stopListeningRef.current)
        }
      }

      // éŸ³å£°å…¥åŠ›æ¤œå‡ºæ™‚ã®ãƒãƒ³ãƒ‰ãƒ©ã‚’è¿½åŠ 
      newRecognition.onspeechstart = () => {
        console.log('ğŸ—£ï¸ éŸ³å£°å…¥åŠ›ã‚’æ¤œå‡ºã—ã¾ã—ãŸï¼ˆonspeechstartï¼‰')
        // éŸ³å£°æ¤œå‡ºãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹
        speechDetectedRef.current = true
        // éŸ³å£°æ¤œå‡ºæ™‚åˆ»ã‚’æ›´æ–°
        updateSpeechTimestamp()
      }

      // éŸ³é‡ãƒ¬ãƒ™ãƒ«ã‚’è¿½è·¡ã™ã‚‹ãŸã‚ã®å¤‰æ•°ã‚’è¿½åŠ 
      let significantSpeechDetected = false
      let lastTranscriptLength = 0

      // çµæœãŒè¿”ã£ã¦ããŸæ™‚ã®ãƒãƒ³ãƒ‰ãƒ©ï¼ˆéŸ³å£°æ¤œå‡ºä¸­ï¼‰
      newRecognition.onresult = (event) => {
        if (!isListeningRef.current) return

        const transcript = Array.from(event.results)
          .map((result) => result[0].transcript)
          .join('')

        // ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒå¤‰åŒ–ã—ãŸå ´åˆã®ã¿æ„å‘³ã®ã‚ã‚‹éŸ³å£°ã¨ã¿ãªã™
        const isSignificantChange =
          transcript.trim().length > lastTranscriptLength
        lastTranscriptLength = transcript.trim().length

        // å®Ÿéš›ã«èªè­˜å¯èƒ½ãªéŸ³å£°ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã®ã¿ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æ›´æ–°
        if (isSignificantChange) {
          console.log('ğŸ“¢ æœ‰æ„ãªéŸ³å£°ã‚’æ¤œå‡ºã—ã¾ã—ãŸï¼ˆãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆå¤‰æ›´ã‚ã‚Šï¼‰')
          significantSpeechDetected = true
          // æ„å‘³ã®ã‚ã‚‹éŸ³å£°ã‚’æ¤œå‡ºã—ãŸã®ã§ã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æ›´æ–°
          updateSpeechTimestamp()
          // éŸ³å£°æ¤œå‡ºãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹
          speechDetectedRef.current = true
        } else {
          console.log(
            'ğŸ”‡ ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒã‚¤ã‚ºã‚’ç„¡è¦–ã—ã¾ã™ï¼ˆãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆå¤‰æ›´ãªã—ï¼‰'
          )
        }

        transcriptRef.current = transcript
        setUserMessage(transcript)
      }

      // éŸ³å£°å…¥åŠ›çµ‚äº†æ™‚ã®ãƒãƒ³ãƒ‰ãƒ©
      newRecognition.onspeechend = () => {
        console.log(
          'ğŸ›‘ éŸ³å£°å…¥åŠ›ãŒçµ‚äº†ã—ã¾ã—ãŸï¼ˆonspeechendï¼‰ã€‚ç„¡éŸ³æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼ãŒå‹•ä½œä¸­ã§ã™ã€‚'
        )
        // éŸ³å£°å…¥åŠ›ãŒçµ‚ã‚ã£ãŸãŒã€ç„¡éŸ³æ¤œå‡ºã¯ãã®ã¾ã¾ç¶™ç¶šã™ã‚‹
        // ã‚¿ã‚¤ãƒãƒ¼ãŒè‡ªå‹•çš„ã«å‡¦ç†ã™ã‚‹
      }

      // éŸ³å£°èªè­˜çµ‚äº†æ™‚ã®ãƒãƒ³ãƒ‰ãƒ©
      newRecognition.onend = () => {
        console.log('Recognition ended')
        // ç„¡éŸ³æ¤œå‡ºã‚’ã‚¯ãƒªã‚¢
        clearSilenceDetection()
        // åˆæœŸéŸ³å£°æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢
        clearInitialSpeechCheckTimer()
      }

      newRecognition.onerror = (event) => {
        console.error('Speech recognition error:', event.error)
        clearSilenceDetection()
        // åˆæœŸéŸ³å£°æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢
        clearInitialSpeechCheckTimer()
        stopListening()
      }

      setRecognition(newRecognition)
    }
  }, [
    stopListening,
    clearInitialSpeechCheckTimer,
    selectLanguage,
    t,
    clearSilenceDetection,
    startSilenceDetection,
    updateSpeechTimestamp,
  ])

  useEffect(() => {
    const AudioContextClass = (window.AudioContext ||
      (window as any).webkitAudioContext) as AudioContextType
    const context = new AudioContextClass()
    setAudioContext(context)
  }, [])

  const startListening = useCallback(async () => {
    const hasPermission = await checkMicrophonePermission()
    if (!hasPermission) return

    if (recognition && !isListeningRef.current && audioContext) {
      transcriptRef.current = ''
      setUserMessage('')
      try {
        recognition.start()
      } catch (error) {
        console.error('Error starting recognition:', error)
      }
      isListeningRef.current = true
      setIsListening(true)

      if (realtimeAPIMode) {
        audioChunksRef.current = [] // éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒªã‚»ãƒƒãƒˆ

        navigator.mediaDevices.getUserMedia({ audio: true }).then((stream) => {
          const recorder = new MediaRecorder(stream, { mimeType: 'audio/webm' })
          setMediaRecorder(recorder)

          recorder.ondataavailable = (event) => {
            if (event.data.size > 0) {
              if (!isListeningRef.current) {
                recognition.stop()
                recorder.stop()
                recorder.ondataavailable = null
                return
              }
              audioChunksRef.current.push(event.data)
              console.log('add audio chunk:', audioChunksRef.current.length)
            }
          }

          recorder.start(100) // ã‚ˆã‚Šå°ã•ãªé–“éš”ã§ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
        })
      }
    }
  }, [recognition, audioContext, realtimeAPIMode])

  const toggleListening = useCallback(() => {
    if (isListeningRef.current) {
      stopListening()
    } else {
      keyPressStartTime.current = Date.now()
      isKeyboardTriggered.current = true
      startListening()
      handleStopSpeaking()
    }
  }, [startListening, stopListening, handleStopSpeaking])

  // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡
  const handleSendMessage = useCallback(() => {
    if (userMessage.trim()) {
      handleStopSpeaking()
      onChatProcessStart(userMessage)
      setUserMessage('')
    }
  }, [userMessage, onChatProcessStart, handleStopSpeaking])

  // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¥åŠ›
  const handleInputChange = useCallback(
    (e: React.ChangeEvent<HTMLInputElement | HTMLTextAreaElement>) => {
      setUserMessage(e.target.value)
    },
    []
  )

  return {
    userMessage,
    isListening,
    silenceTimeoutRemaining,
    handleInputChange,
    handleSendMessage,
    toggleListening,
    handleStopSpeaking,
    startListening,
    stopListening,
  }
}

import { useState, useEffect, useCallback, useRef } from 'react'
import { useTranslation } from 'react-i18next'
import settingsStore from '@/features/stores/settings'
import webSocketStore from '@/features/stores/websocketStore'
import toastStore from '@/features/stores/toast'
import homeStore from '@/features/stores/home'
import { useSilenceDetection } from './useSilenceDetection'
import { processAudio, base64EncodeAudio } from '@/utils/audioProcessing'
import { useAudioProcessing } from './useAudioProcessing'

/**
 * ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ APIã‚’ä½¿ç”¨ã—ãŸéŸ³å£°èªè­˜ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ•ãƒƒã‚¯
 */
export const useRealtimeVoiceAPI = (
  onChatProcessStart: (text: string) => void
) => {
  const { t } = useTranslation()
  const selectLanguage = settingsStore((s) => s.selectLanguage)
  const realtimeAPIMode = settingsStore((s) => s.realtimeAPIMode)
  const initialSpeechTimeout = settingsStore((s) => s.initialSpeechTimeout)

  // ----- çŠ¶æ…‹ç®¡ç† -----
  const [userMessage, setUserMessage] = useState('')
  const [isListening, setIsListening] = useState(false)
  const isListeningRef = useRef(false)

  // ----- éŸ³å£°èªè­˜é–¢é€£ -----
  const [recognition, setRecognition] = useState<SpeechRecognition | null>(null)
  const transcriptRef = useRef('')
  const speechDetectedRef = useRef<boolean>(false)
  const initialSpeechCheckTimerRef = useRef<NodeJS.Timeout | null>(null)

  // ----- ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªå‡¦ç†ãƒ•ãƒƒã‚¯ã‚’ä½¿ç”¨ -----
  const {
    audioContext,
    mediaRecorder,
    checkMicrophonePermission,
    startRecording,
    stopRecording,
    audioChunksRef,
  } = useAudioProcessing()

  // ----- ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒãƒƒãƒ•ã‚¡ç”¨ -----
  const audioBufferRef = useRef<Float32Array | null>(null)

  // ----- ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ãƒˆãƒªã‚¬ãƒ¼é–¢é€£ -----
  const keyPressStartTime = useRef<number | null>(null)
  const isKeyboardTriggered = useRef(false)

  // ----- ç„¡éŸ³æ¤œå‡ºãƒ•ãƒƒã‚¯ã‚’ä½¿ç”¨ -----
  const {
    silenceTimeoutRemaining,
    clearSilenceDetection,
    startSilenceDetection,
    updateSpeechTimestamp,
    isSpeechEnded,
  } = useSilenceDetection({
    onTextDetected: (text: string) => {
      // æ¤œå‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’å…ƒã® onChatProcessStart ã«æ¸¡ã™å‰ã«ã€WebSocketã§é€ä¿¡ã™ã‚‹å‡¦ç†ã‚’è¿½åŠ 
      sendTextToWebSocket(text)
      // å…ƒã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚å‘¼ã³å‡ºã™
      onChatProcessStart(text)
    },
    transcriptRef,
    setUserMessage,
    speechDetectedRef,
  })

  // ----- ãƒ†ã‚­ã‚¹ãƒˆã‚’WebSocketã§é€ä¿¡ã™ã‚‹é–¢æ•° -----
  const sendTextToWebSocket = useCallback((text: string) => {
    const wsManager = webSocketStore.getState().wsManager
    const ss = settingsStore.getState()

    if (wsManager?.websocket?.readyState !== WebSocket.OPEN) {
      console.error('WebSocket is not open')
      return
    }

    let sendContent: { type: string; text?: string; audio?: string }[] = []

    if (ss.realtimeAPIModeContentType === 'input_text' || text.trim()) {
      console.log(
        'Sending text through WebSocket from silence detection:',
        text
      )
      sendContent = [{ type: 'input_text', text: text.trim() }]
    }

    if (sendContent.length > 0) {
      wsManager.websocket.send(
        JSON.stringify({
          type: 'conversation.item.create',
          item: {
            type: 'message',
            role: 'user',
            content: sendContent,
          },
        })
      )
      wsManager.websocket.send(
        JSON.stringify({
          type: 'response.create',
        })
      )
    }
  }, [])

  // ----- åˆæœŸéŸ³å£°æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹é–¢æ•° -----
  const clearInitialSpeechCheckTimer = useCallback(() => {
    if (initialSpeechCheckTimerRef.current) {
      clearTimeout(initialSpeechCheckTimerRef.current)
      initialSpeechCheckTimerRef.current = null
    }
  }, [])

  // ----- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ APIç”¨ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿é€ä¿¡ -----
  const sendAudioBuffer = useCallback(() => {
    console.log('sendAudioBuffer')
    if (!audioBufferRef.current || audioBufferRef.current.length === 0) {
      console.error('éŸ³å£°ãƒãƒƒãƒ•ã‚¡ãŒç©ºã§ã™')
      return
    }

    const base64Chunk = base64EncodeAudio(audioBufferRef.current)
    const ss = settingsStore.getState()
    const wsManager = webSocketStore.getState().wsManager

    if (wsManager?.websocket?.readyState !== WebSocket.OPEN) {
      return
    }

    let sendContent: { type: string; text?: string; audio?: string }[] = []

    if (ss.realtimeAPIModeContentType === 'input_audio') {
      console.log('Sending buffer. Length:', audioBufferRef.current.length)
      sendContent = [{ type: 'input_audio', audio: base64Chunk }]
    } else {
      const currentText = transcriptRef.current.trim()
      if (currentText) {
        console.log('Sending text. userMessage:', currentText)
        sendContent = [{ type: 'input_text', text: currentText }]
      }
    }

    if (sendContent.length > 0) {
      wsManager.websocket.send(
        JSON.stringify({
          type: 'conversation.item.create',
          item: {
            type: 'message',
            role: 'user',
            content: sendContent,
          },
        })
      )
      wsManager.websocket.send(
        JSON.stringify({
          type: 'response.create',
        })
      )
    }

    audioBufferRef.current = null // é€ä¿¡å¾Œã«ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢
  }, [])

  // ----- éŸ³å£°èªè­˜åœæ­¢å‡¦ç† -----
  const stopListening = useCallback(async () => {
    console.log('ğŸ›‘ useRealtimeVoiceAPI: stopListening å‘¼ã³å‡ºã—é–‹å§‹')

    // ãƒªã‚¹ãƒ‹ãƒ³ã‚°çŠ¶æ…‹ã‚’å…ˆã«æ›´æ–°ã—ã¦ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿åé›†ã‚’é˜²æ­¢
    isListeningRef.current = false
    setIsListening(false)

    // å„ç¨®ã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢
    clearSilenceDetection()
    clearInitialSpeechCheckTimer()

    // éŸ³å£°èªè­˜ã‚’åœæ­¢ - ã¾ãšrecognitionã‹ã‚‰
    if (recognition) {
      try {
        console.log('ğŸ™ï¸ éŸ³å£°èªè­˜ã‚’åœæ­¢ã—ã¾ã™')
        recognition.stop()
        console.log('ğŸ™ï¸ éŸ³å£°èªè­˜åœæ­¢æˆåŠŸ')
      } catch (error) {
        console.error('ğŸ”´ éŸ³å£°èªè­˜åœæ­¢ã‚¨ãƒ©ãƒ¼:', error)
      }
    }

    // éŒ²éŸ³ã‚’åœæ­¢ã—ã¦éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    console.log('ğŸ¤ MediaRecorderã‚’åœæ­¢ã—ã¾ã™')
    const audioBlob = await stopRecording()
    console.log(
      'ğŸ¤ MediaRecorderåœæ­¢å®Œäº†',
      audioBlob ? `ã‚µã‚¤ã‚º: ${audioBlob.size}` : 'ãƒ‡ãƒ¼ã‚¿ãªã—'
    )

    // éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å‡¦ç†
    if (audioBlob && audioContext) {
      try {
        console.log('ğŸ”Š éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¾ã™')
        const arrayBuffer = await audioBlob.arrayBuffer()
        const audioBuffer = await audioContext.decodeAudioData(arrayBuffer)
        const processedData = processAudio(audioBuffer)
        audioBufferRef.current = processedData

        // éŸ³å£°ãƒ‡ãƒ¼ã‚¿é€ä¿¡
        sendAudioBuffer()
      } catch (error) {
        console.error('ğŸ”´ éŸ³å£°ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼:', error)
      }
    } else {
      console.log('âš ï¸ æœ‰åŠ¹ãªéŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ')
    }

    // ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ãƒˆãƒªã‚¬ãƒ¼ã®å ´åˆã®å‡¦ç†
    const trimmedTranscriptRef = transcriptRef.current.trim()
    if (isKeyboardTriggered.current) {
      const pressDuration = Date.now() - (keyPressStartTime.current || 0)
      // æŠ¼ã—ã¦ã‹ã‚‰1ç§’ä»¥ä¸Š ã‹ã¤ æ–‡å­—ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿é€ä¿¡
      // ç„¡éŸ³æ¤œå‡ºã«ã‚ˆã‚‹è‡ªå‹•é€ä¿¡ãŒæ—¢ã«è¡Œã‚ã‚Œã¦ã„ãªã„å ´åˆã®ã¿é€ä¿¡ã™ã‚‹
      if (pressDuration >= 1000 && trimmedTranscriptRef && !isSpeechEnded()) {
        console.log('âŒ¨ï¸ ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ãƒˆãƒªã‚¬ãƒ¼ã«ã‚ˆã‚‹é€ä¿¡:', trimmedTranscriptRef)
        onChatProcessStart(trimmedTranscriptRef)
        setUserMessage('')
      }
      isKeyboardTriggered.current = false
    }

    console.log('ğŸ useRealtimeVoiceAPI: stopListening å‡¦ç†å®Œäº†')
  }, [
    clearSilenceDetection,
    clearInitialSpeechCheckTimer,
    recognition,
    audioContext,
    stopRecording,
    sendAudioBuffer,
    isSpeechEnded,
    onChatProcessStart,
  ])

  // ----- éŸ³å£°èªè­˜é–‹å§‹å‡¦ç† -----
  const startListening = useCallback(async () => {
    const hasPermission = await checkMicrophonePermission()
    if (!hasPermission) return

    if (!recognition || !audioContext) return

    // æ—¢ã«èªè­˜ãŒé–‹å§‹ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ä¸€åº¦åœæ­¢ã—ã¦ã‹ã‚‰å†é–‹ã™ã‚‹
    if (isListeningRef.current) {
      try {
        recognition.stop()
        // åœæ­¢å®Œäº†ã‚’å¾…ã¤ãŸã‚ã®çŸ­ã„é…å»¶
        await new Promise((resolve) => setTimeout(resolve, 100))
      } catch (err) {
        console.log('Recognition was not running, proceeding to start', err)
      }
    }

    // ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
    transcriptRef.current = ''
    setUserMessage('')

    try {
      recognition.start()
      console.log('Recognition started successfully')
      // ãƒªã‚¹ãƒ‹ãƒ³ã‚°çŠ¶æ…‹ã‚’æ›´æ–°
      isListeningRef.current = true
      setIsListening(true)
    } catch (error) {
      console.error('Error starting recognition:', error)

      // InvalidStateErrorã®å ´åˆã¯ã€æ—¢ã«é–‹å§‹ã•ã‚Œã¦ã„ã‚‹ã¨ã¿ãªã™
      if (error instanceof DOMException && error.name === 'InvalidStateError') {
        console.log('Recognition is already running, skipping retry')
        // æ—¢ã«å®Ÿè¡Œä¸­ãªã®ã§ã€ãƒªã‚¹ãƒ‹ãƒ³ã‚°çŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹
        isListeningRef.current = true
        setIsListening(true)
      } else {
        // ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã®å ´åˆã®ã¿å†è©¦è¡Œ
        setTimeout(() => {
          try {
            if (recognition) {
              recognition.start()
              console.log('Recognition started on retry')
              isListeningRef.current = true
              setIsListening(true)
            }
          } catch (retryError) {
            console.error('Failed to start recognition on retry:', retryError)
            isListeningRef.current = false
            setIsListening(false)
            return
          }
        }, 300)
      }
    }

    // éŒ²éŸ³ã‚’é–‹å§‹
    const success = await startRecording({ mimeType: 'audio/webm' })
    if (!success) {
      console.error('Failed to start recording')
      toastStore.getState().addToast({
        message: t('Toasts.SpeechRecognitionError'),
        type: 'error',
        tag: 'speech-recognition-error',
      })
    }
  }, [recognition, audioContext, checkMicrophonePermission, startRecording, t])

  // ----- éŸ³å£°èªè­˜ãƒˆã‚°ãƒ«å‡¦ç† -----
  const toggleListening = useCallback(() => {
    if (isListeningRef.current) {
      stopListening()
    } else {
      keyPressStartTime.current = Date.now()
      isKeyboardTriggered.current = true
      startListening()
      // AIã®ç™ºè©±ã‚’åœæ­¢
      homeStore.setState({ isSpeaking: false })
    }
  }, [startListening, stopListening])

  // ----- ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ -----
  const handleSendMessage = useCallback(() => {
    if (userMessage.trim()) {
      // AIã®ç™ºè©±ã‚’åœæ­¢
      homeStore.setState({ isSpeaking: false })
      onChatProcessStart(userMessage)
      setUserMessage('')
    }
  }, [userMessage, onChatProcessStart])

  // ----- ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¥åŠ› -----
  const handleInputChange = useCallback(
    (e: React.ChangeEvent<HTMLInputElement | HTMLTextAreaElement>) => {
      setUserMessage(e.target.value)
    },
    []
  )

  // ----- éŸ³å£°èªè­˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®åˆæœŸåŒ–ã¨ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©è¨­å®š -----
  useEffect(() => {
    const SpeechRecognition =
      window.SpeechRecognition || window.webkitSpeechRecognition

    if (!SpeechRecognition) {
      console.error('Speech Recognition API is not supported in this browser')
      return
    }

    const newRecognition = new SpeechRecognition()
    newRecognition.lang = 'ja-JP' // æ—¥æœ¬èªè¨­å®šï¼ˆå¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ï¼‰
    newRecognition.continuous = true
    newRecognition.interimResults = true

    // ----- ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ã®è¨­å®š -----

    // éŸ³å£°èªè­˜é–‹å§‹æ™‚
    newRecognition.onstart = () => {
      console.log('Speech recognition started')

      // ç„¡éŸ³æ¤œå‡ºé–‹å§‹
      if (stopListening) {
        startSilenceDetection(stopListening)
      }
    }

    // éŸ³å£°èªè­˜çµæœãŒå¾—ã‚‰ã‚ŒãŸã¨ã
    newRecognition.onresult = (event) => {
      if (!isListeningRef.current) return

      const transcript = Array.from(event.results)
        .map((result) => result[0].transcript)
        .join('')

      updateSpeechTimestamp()
      speechDetectedRef.current = true
      transcriptRef.current = transcript
      setUserMessage(transcript)
    }

    // éŸ³å£°èªè­˜çµ‚äº†æ™‚
    newRecognition.onend = () => {
      console.log('Recognition ended')
      clearSilenceDetection()
    }

    setRecognition(newRecognition)

    // ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–¢æ•°
    return () => {
      try {
        if (newRecognition) {
          newRecognition.abort()
        }
      } catch (error) {
        console.error('Error cleaning up speech recognition:', error)
      }
      clearSilenceDetection()
    }
  }, [])

  // WebSocketã®æº–å‚™ãŒã§ãã¦ã„ã‚‹ã‹ã‚’ç¢ºèª
  const isWebSocketReady = useCallback(() => {
    const wsManager = webSocketStore.getState().wsManager
    return wsManager?.websocket?.readyState === WebSocket.OPEN
  }, [])

  return {
    userMessage,
    isListening,
    silenceTimeoutRemaining,
    handleInputChange,
    handleSendMessage,
    toggleListening,
    startListening,
    stopListening,
    isWebSocketReady,
  }
}

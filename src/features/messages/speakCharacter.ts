import homeStore from '@/features/stores/home'
import settingsStore from '@/features/stores/settings'
import { AIVoice } from '@/features/constants/settings'
import { wait } from '@/utils/wait'
import { Talk } from './messages'
import { synthesizeStyleBertVITS2Api } from './synthesizeStyleBertVITS2'
import { synthesizeVoiceKoeiromapApi } from './synthesizeVoiceKoeiromap'
import { synthesizeVoiceElevenlabsApi } from './synthesizeVoiceElevenlabs'
import { synthesizeVoiceCartesiaApi } from './synthesizeVoiceCartesia'
import { synthesizeVoiceGoogleApi } from './synthesizeVoiceGoogle'
import { synthesizeVoiceVoicevoxApi } from './synthesizeVoiceVoicevox'
import { synthesizeVoiceAivisSpeechApi } from './synthesizeVoiceAivisSpeech'
import {
  synthesizeVoiceAivisCloudApi,
  synthesizeVoiceAivisCloudApiStreaming,
} from './synthesizeVoiceAivisCloudApi'
import { synthesizeVoiceGSVIApi } from './synthesizeVoiceGSVI'
import { synthesizeVoiceOpenAIApi } from './synthesizeVoiceOpenAI'
import { synthesizeVoiceAzureOpenAIApi } from './synthesizeVoiceAzureOpenAI'
import toastStore from '@/features/stores/toast'
import i18next from 'i18next'
import { SpeakQueue } from './speakQueue'
import { synthesizeVoiceNijivoiceApi } from './synthesizeVoiceNijivoice'
import { Live2DHandler } from './live2dHandler'
import {
  asyncConvertEnglishToJapaneseReading,
  containsEnglish,
} from '@/utils/textProcessing'

const speakQueue = SpeakQueue.getInstance()

export function preprocessMessage(
  message: string,
  settings: ReturnType<typeof settingsStore.getState>
): string | null {
  // å‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤
  let processed: string | null = message.trim()
  if (!processed) return null

  // çµµæ–‡å­—ã‚’å‰Šé™¤ (ã“ã‚Œã‚’å…ˆã«è¡Œã†ã“ã¨ã§å¤‰æ›å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆé‡ã‚’æ¸›ã‚‰ã™)
  processed = processed.replace(
    /[\u{1F300}-\u{1F9FF}]|[\u{1F600}-\u{1F64F}]|[\u{1F680}-\u{1F6FF}]|[\u{2600}-\u{26FF}]|[\u{2700}-\u{27BF}]|[\u{1F900}-\u{1F9FF}]|[\u{1F1E0}-\u{1F1FF}]/gu,
    ''
  )

  // ç™ºéŸ³ã¨ã—ã¦ä¸é©åˆ‡ãªè¨˜å·ã®ã¿ã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
  // æ„Ÿå˜†ç¬¦ã€ç–‘å•ç¬¦ã€å¥èª­ç‚¹ã€æ‹¬å¼§é¡ã€å¼•ç”¨ç¬¦ã€æ•°å­¦è¨˜å·ã€ãã®ä»–ä¸€èˆ¬çš„ãªè¨˜å·ã‚’å«ã‚€
  const isOnlySymbols: boolean =
    /^[!?.,ã€‚ã€ï¼ï¼Œ'"(){}[\]<>+=\-*\/\\|;:@#$%^&*_~ï¼ï¼Ÿï¼ˆï¼‰ã€Œã€ã€ã€ã€ã€‘ã€”ã€•ï¼»ï¼½ï½›ï½ã€ˆã€‰ã€Šã€‹ï½¢ï½£ã€‚ã€ï¼ï¼Œï¼šï¼›ï¼‹ï¼ï¼Šï¼ï¼ï¼œï¼ï¼…ï¼†ï¼¾ï½œï½ï¼ ï¼ƒï¼„ï¼¿"ã€€]+$/.test(
      processed
    )

  // ç©ºæ–‡å­—åˆ—ã®å ´åˆã¯nullã‚’è¿”ã™
  if (processed === '' || isOnlySymbols) return null

  // è‹±èªã‹ã‚‰æ—¥æœ¬èªã¸ã®å¤‰æ›ã¯æ¬¡ã®æ¡ä»¶ã®ã¿å®Ÿè¡Œ
  // 1. è¨­å®šã§ã‚ªãƒ³ã«ãªã£ã¦ã„ã‚‹
  // 2. è¨€èªãŒæ—¥æœ¬èª
  // 3. ãƒ†ã‚­ã‚¹ãƒˆã«è‹±èªã®ã‚ˆã†ãªæ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿
  if (
    settings.changeEnglishToJapanese &&
    settings.selectLanguage === 'ja' &&
    containsEnglish(processed)
  ) {
    // ã“ã®æ™‚ç‚¹ã§å‡¦ç†æ¸ˆã¿ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™ï¼ˆå¾Œã§éåŒæœŸã§å¤‰æ›å‡¦ç†ã‚’å®Œäº†ã™ã‚‹ï¼‰
    return processed
  }

  // å¤‰æ›ä¸è¦ãªå ´åˆã¯ãã®ã¾ã¾è¿”ã™
  return processed
}

async function synthesizeVoice(
  talk: Talk,
  voiceType: AIVoice
): Promise<ArrayBuffer | null> {
  const ss = settingsStore.getState()

  console.log(`ğŸ¤ éŸ³å£°åˆæˆé–‹å§‹: ${voiceType}ãƒ¢ãƒ¼ãƒ‰`)

  if (ss.audioMode) {
    console.log('âš ï¸ AudioModeãŒæœ‰åŠ¹ã®ãŸã‚éŸ³å£°åˆæˆã‚’ã‚¹ã‚­ãƒƒãƒ—')
    return null
  }

  try {
    switch (voiceType) {
      case 'koeiromap':
        return await synthesizeVoiceKoeiromapApi(
          talk,
          ss.koeiromapKey,
          ss.koeiroParam
        )
      case 'voicevox':
        return await synthesizeVoiceVoicevoxApi(
          talk,
          ss.voicevoxSpeaker,
          ss.voicevoxSpeed,
          ss.voicevoxPitch,
          ss.voicevoxIntonation,
          ss.voicevoxServerUrl
        )
      case 'google':
        return await synthesizeVoiceGoogleApi(
          talk,
          ss.googleTtsType,
          ss.selectLanguage
        )
      case 'stylebertvits2':
        return await synthesizeStyleBertVITS2Api(
          talk,
          ss.stylebertvits2ServerUrl,
          ss.stylebertvits2ApiKey,
          ss.stylebertvits2ModelId,
          ss.stylebertvits2Style,
          ss.stylebertvits2SdpRatio,
          ss.stylebertvits2Length,
          ss.selectLanguage
        )
      case 'aivis_speech':
        return await synthesizeVoiceAivisSpeechApi(
          talk,
          ss.aivisSpeechSpeaker,
          ss.aivisSpeechSpeed,
          ss.aivisSpeechPitch,
          ss.aivisSpeechIntonationScale,
          ss.aivisSpeechServerUrl,
          ss.aivisSpeechTempoDynamics,
          ss.aivisSpeechPrePhonemeLength,
          ss.aivisSpeechPostPhonemeLength
        )
      case 'aivis_cloud_api':
        // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œç‰ˆã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹åˆ¤å®š
        console.log('ğŸŒŠ Aivis Cloud APIè¨­å®šç¢ºèª: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°=' + ss.aivisCloudStreamingEnabled)
        
        if (ss.aivisCloudStreamingEnabled) {
          console.log('ğŸŒŠ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã§éŸ³å£°åˆæˆå®Ÿè¡Œ')
          
          await synthesizeVoiceAivisCloudApiStreaming(
            talk,
            ss.aivisCloudApiKey,
            ss.aivisCloudModelUuid,
            ss.aivisCloudStyleId,
            ss.aivisCloudStyleName,
            ss.aivisCloudUseStyleName,
            ss.aivisCloudSpeed,
            ss.aivisCloudPitch,
            ss.aivisCloudIntonationScale,
            ss.aivisCloudTempoDynamics,
            ss.aivisCloudPrePhonemeLength,
            ss.aivisCloudPostPhonemeLength
          )
          
          // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ™‚ã¯nullã‚’è¿”ã—ã¦ã‚­ãƒ¥ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã‚’ãƒã‚¤ãƒ‘ã‚¹
          return null
        } else {
          console.log('ğŸ“¦ éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã§éŸ³å£°åˆæˆå®Ÿè¡Œ')
          return await synthesizeVoiceAivisCloudApi(
            talk,
            ss.aivisCloudApiKey,
            ss.aivisCloudModelUuid,
            ss.aivisCloudStyleId,
            ss.aivisCloudStyleName,
            ss.aivisCloudUseStyleName,
            ss.aivisCloudSpeed,
            ss.aivisCloudPitch,
            ss.aivisCloudIntonationScale,
            ss.aivisCloudTempoDynamics,
            ss.aivisCloudPrePhonemeLength,
            ss.aivisCloudPostPhonemeLength
          )
        }
      case 'gsvitts':
        return await synthesizeVoiceGSVIApi(
          talk,
          ss.gsviTtsServerUrl,
          ss.gsviTtsModelId,
          ss.gsviTtsBatchSize,
          ss.gsviTtsSpeechRate
        )
      case 'elevenlabs':
        return await synthesizeVoiceElevenlabsApi(
          talk,
          ss.elevenlabsApiKey,
          ss.elevenlabsVoiceId,
          ss.selectLanguage
        )
      case 'cartesia':
        return await synthesizeVoiceCartesiaApi(
          talk,
          ss.cartesiaApiKey,
          ss.cartesiaVoiceId,
          ss.selectLanguage
        )
      case 'openai':
        return await synthesizeVoiceOpenAIApi(
          talk,
          ss.openaiKey,
          ss.openaiTTSVoice,
          ss.openaiTTSModel,
          ss.openaiTTSSpeed
        )
      case 'azure':
        return await synthesizeVoiceAzureOpenAIApi(
          talk,
          ss.azureTTSKey || ss.azureKey,
          ss.azureTTSEndpoint || ss.azureEndpoint,
          ss.openaiTTSVoice,
          ss.openaiTTSSpeed
        )
      case 'nijivoice':
        return await synthesizeVoiceNijivoiceApi(
          talk,
          ss.nijivoiceApiKey,
          ss.nijivoiceActorId,
          ss.nijivoiceSpeed,
          ss.nijivoiceEmotionalLevel,
          ss.nijivoiceSoundDuration
        )
      default:
        return null
    }
  } catch (error) {
    handleTTSError(error, voiceType)
    return null
  }
}

const createSpeakCharacter = () => {
  let lastTime = 0
  let prevFetchPromise: Promise<unknown> = Promise.resolve()

  return (
    sessionId: string,
    talk: Talk,
    onStart?: () => void,
    onComplete?: () => void
  ) => {
    let called = false
    const ss = settingsStore.getState()
    onStart?.()

    const initialToken = SpeakQueue.currentStopToken

    speakQueue.checkSessionId(sessionId)

    // åœæ­¢å¾Œãªã‚‰å³å®Œäº†
    if (SpeakQueue.currentStopToken !== initialToken) {
      if (onComplete && !called) {
        called = true
        onComplete()
      }
      return
    }

    const processedMessage = preprocessMessage(talk.message, ss)
    if (!processedMessage && !talk.buffer) {
      if (onComplete && !called) {
        called = true
        onComplete()
      }
      return
    }

    if (processedMessage) {
      talk.message = processedMessage
    } else if (talk.buffer) {
      talk.message = ''
    }

    let isNeedDecode = true

    const processAndSynthesizePromise = prevFetchPromise.then(async () => {
      const now = Date.now()
      if (now - lastTime < 1000) {
        await wait(1000 - (now - lastTime))
      }

      // ãƒœã‚¿ãƒ³åœæ­¢ã§ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚ŒãŸå ´åˆã¯ã“ã“ã§çµ‚äº†
      if (SpeakQueue.currentStopToken !== initialToken) {
        return null
      }

      if (
        processedMessage &&
        ss.changeEnglishToJapanese &&
        ss.selectLanguage === 'ja' &&
        containsEnglish(processedMessage)
      ) {
        try {
          const convertedText =
            await asyncConvertEnglishToJapaneseReading(processedMessage)
          talk.message = convertedText
        } catch (error) {
          console.error('Error converting English to Japanese:', error)
        }
      }

      let buffer
      try {
        if (talk.message == '' && talk.buffer) {
          buffer = talk.buffer
          isNeedDecode = false
        } else if (talk.message !== '') {
          buffer = await synthesizeVoice(talk, ss.selectVoice)
        } else {
          buffer = null
        }
      } catch (error) {
        handleTTSError(error, ss.selectVoice)
        return null
      } finally {
        lastTime = Date.now()
      }

      // åˆæˆé–‹å§‹å‰ã«å–å¾—ã—ãŸ initialToken ã‚’ãã®ã¾ã¾ä¿æŒã™ã‚‹
      const tokenAtStart = initialToken
      return { buffer, isNeedDecode, tokenAtStart }
    })

    prevFetchPromise = processAndSynthesizePromise.catch((err) => {
      console.error('Speak chain error (swallowed):', err)
      // å¾Œç¶šå‡¦ç†ã‚’æ­¢ã‚ãªã„ãŸã‚ã« resolve ã§è¿”ã™
      return null
    })

    processAndSynthesizePromise
      .then((result) => {
        if (!result || !result.buffer) {
          if (onComplete && !called) {
            called = true
            onComplete()
          }
          return
        }

        // Stop ãƒœã‚¿ãƒ³å¾Œã«ç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ã§ãªã„ã‹ç¢ºèª
        if (result.tokenAtStart !== SpeakQueue.currentStopToken) {
          // ç”Ÿæˆä¸­ã« Stop ã•ã‚ŒãŸ => ç ´æ£„
          if (onComplete && !called) {
            called = true
            onComplete()
          }
          return
        }

        // Wrap the onComplete passed to speakQueue.addTask
        const guardedOnComplete = () => {
          if (onComplete && !called) {
            called = true
            onComplete()
          }
        }

        speakQueue.addTask({
          sessionId,
          audioBuffer: result.buffer,
          talk,
          isNeedDecode: result.isNeedDecode,
          onComplete: guardedOnComplete, // Pass the guarded function
        })
      })
      .catch((error) => {
        console.error('Error in processAndSynthesizePromise chain:', error)
        if (onComplete && !called) {
          called = true
          onComplete()
        }
      })
  }
}

export function handleTTSError(error: unknown, serviceName: string): void {
  let message: string
  if (error instanceof Error) {
    message = error.message
  } else if (typeof error === 'string') {
    message = error
  } else {
    message = i18next.t('Errors.UnexpectedError')
  }
  const errorMessage = i18next.t('Errors.TTSServiceError', {
    serviceName,
    message,
  })

  toastStore.getState().addToast({
    message: errorMessage,
    type: 'error',
    duration: 5000,
    tag: 'tts-error',
  })

  console.error(errorMessage)
}

export const speakCharacter = createSpeakCharacter()

export const testVoiceVox = async (customText?: string) => {
  await testVoice('voicevox', customText)
}

export const testAivisSpeech = async (customText?: string) => {
  await testVoice('aivis_speech', customText)
}

export const testVoice = async (voiceType: AIVoice, customText?: string) => {
  const ss = settingsStore.getState()

  const defaultMessages: Record<AIVoice, string> = {
    voicevox: 'ãƒœã‚¤ã‚¹ãƒœãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨ã—ã¾ã™',
    aivis_speech: 'AivisSpeechã‚’ä½¿ç”¨ã—ã¾ã™',
    aivis_cloud_api: 'Aivis Cloud APIã‚’ä½¿ç”¨ã—ã¾ã™',
    koeiromap: 'ã‚³ã‚¨ã‚¤ãƒ­ãƒãƒƒãƒ—ã‚’ä½¿ç”¨ã—ã¾ã™',
    google: 'Google Text-to-Speechã‚’ä½¿ç”¨ã—ã¾ã™',
    stylebertvits2: 'StyleBertVITS2ã‚’ä½¿ç”¨ã—ã¾ã™',
    gsvitts: 'GSVI TTSã‚’ä½¿ç”¨ã—ã¾ã™',
    elevenlabs: 'ElevenLabsã‚’ä½¿ç”¨ã—ã¾ã™',
    cartesia: 'Cartesiaã‚’ä½¿ç”¨ã—ã¾ã™',
    openai: 'OpenAI TTSã‚’ä½¿ç”¨ã—ã¾ã™',
    azure: 'Azure TTSã‚’ä½¿ç”¨ã—ã¾ã™',
    nijivoice: 'ã«ã˜ãƒœã‚¤ã‚¹ã‚’ä½¿ç”¨ã—ã¾ã™',
  }

  const message = customText || defaultMessages[voiceType]

  const talk: Talk = {
    message,
    emotion: 'neutral',
  }

  try {
    const currentVoice = ss.selectVoice
    settingsStore.setState({ selectVoice: voiceType })

    const buffer = await synthesizeVoice(talk, voiceType)

    settingsStore.setState({ selectVoice: currentVoice })

    if (buffer) {
      if (ss.modelType === 'vrm') {
        const hs = homeStore.getState()
        await hs.viewer.model?.speak(buffer, talk)
      } else if (ss.modelType === 'live2d') {
        Live2DHandler.speak(buffer, talk)
      }
    }
  } catch (error) {
    console.error(`Error testing ${voiceType} voice:`, error)
    handleTTSError(error, voiceType)
  }
}
